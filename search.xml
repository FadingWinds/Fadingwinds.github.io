<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>The Chain of Blocks</title>
      <link href="/2020/02/17/the-chain-of-blocks/"/>
      <url>/2020/02/17/the-chain-of-blocks/</url>
      
        <content type="html"><![CDATA[<hr><p><em>Banner Image Ref:</em></p><p><a href="https://cointelegraph.com/bitcoin-for-beginners/what-are-cryptocurrencies" target="_blank" rel="noopener">https://cointelegraph.com/bitcoin-for-beginners/what-are-cryptocurrencies</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Blockchain | Âå∫ÂùóÈìæ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Natural Language Processing</title>
      <link href="/2020/02/17/natural-language-processing/"/>
      <url>/2020/02/17/natural-language-processing/</url>
      
        <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Three type of models:</p><ul><li>Generative Models</li><li>Discriminative Models, e.g. neural networks</li><li>Graphical Models</li></ul><p>The fundamental goal of NLP is to have <strong>deep understanding of broad language</strong>.</p><p>End systems that we want to build:</p><ul><li><strong>Simple</strong>: spelling correction, text categorization, ‚Ä¶</li><li><strong>Complex</strong>: speech recognition, machine translation, information extraction, dialog interfaces, question answering, ‚Ä¶</li><li>Unknown: human-level comprehension (probably not just NLP)</li></ul><p><strong>Key problems</strong> in NLP:</p><ol><li><p>Ambiguity</p><ul><li><p>Syntactic ambiguity</p><p>   e.g. <em>Stolen Painting Found by Tree</em> </p><p>   In state-of-the-art ML, can reach ~95% accuracy for many languages when given many training examples</p></li><li><p>Semantic ambiguity </p><p>   e.g. <em>Siri, call me an ambulance</em></p></li></ul></li><li><p>Scale  </p></li><li><p>Sparsity</p><p><strong>Corpus</strong> is a collection of text. Often annotated in some way. </p></li></ol><h3 id="Meta-NLP"><a href="#Meta-NLP" class="headerlink" title="Meta NLP"></a>Meta NLP</h3><p>This section is about steps to conduct a NLP research.</p><h4 id="Literature-review-should-be-done-early"><a href="#Literature-review-should-be-done-early" class="headerlink" title="Literature review: should be done early"></a>Literature review: should be done early</h4><ul><li>Avoid re-invent the wheel</li><li>Learn about common tricks, resources, and libraries</li></ul><ol><li>Do a keyword search on <em>Google Scholar</em>, <em>Semantic Scholar</em>, or <em>the ACL Anthology</em>.</li><li>Download the papers that seem most relevant.</li><li>Skim the abstracts, intros, and previous work sections.</li><li>Identify papers that look relevant, appear often, or have lots of citations on Google Scholar, and download them. Then repeat.</li></ol><p>Places to find the most trustworthy papers:</p><ul><li><em>NLP</em>: Proceedings of ACL conferences (ACL, NAACL, EACL, EMNLP, CoNLL, LREC), Journal of Computational Linguistics, TACL, COLING, arXiv*</li><li><em>Machine Learning/AI</em>: Proceedings of NIPS, ICML, ICLR, AAAI, IJCAI, and arXiv*</li><li><em>Computational Linguistics</em>: Journals like Linguistic Inquiry, NLLT, Semantics and Pragmatics</li></ul><p>What to mention in literature review:</p><ul><li>General problem/task definition</li><li>Relevant methods and results</li><li>Comparisons with your work and other related work</li><li>Open issues</li></ul><h4 id="Acquiring-Datasets"><a href="#Acquiring-Datasets" class="headerlink" title="Acquiring Datasets"></a>Acquiring Datasets</h4><ul><li><p>Existing datasets</p><p>ACL anthology, Linguistic Data Consortium (LDC), Look for datasheets when available (e.g. QuAC)</p></li><li><p>Wild datasets</p><p>e.g. Ubuntu Dialogue Corpus, StackOverflow Data (warning: easy to violate copyright and terms of service)</p></li><li><p>Build own datasets</p><p>Either write detailed guidelines, and work with experts; or write simple guidelines, and crowdsource</p></li><li><p>Generate datasets</p><p>It‚Äôs super easy, but artificial data does not reflect the real world.</p></li></ul><h4 id="Quantitative-Evaluation"><a href="#Quantitative-Evaluation" class="headerlink" title="Quantitative Evaluation"></a>Quantitative Evaluation</h4><ol><li>Follow prior work and use existing metrics<ul><li>If it‚Äôs a new task, create a metric before you start testing. It must be independent of your model.</li></ul></li><li>Use ablations to study the effectiveness of your choices (and don‚Äôt adopt fancy solutions that don‚Äôt really help)<ul><li>e.g. MLP sentiment classifier with GloVe embeddings, MLP sentiment classifier with random embeddings, MaxEnt classifier with GloVe embeddings, MaxEnt classifier with random embeddings</li></ul></li><li>Consider controlled human evaluation when standard, and even when less standard<ul><li>e.g. summarization, machine translation, generation</li></ul></li><li>Test for statistical significance when differences are small and models are complex</li><li>Consider extrinsic evaluation on <em>downstream tasks</em> (what the field calls those supervised-learning tasks that utilize a pre-trained model or component)</li><li>Negative results are also important.</li></ol><h4 id="Qualitative-Evaluation-and-Error-Analysis"><a href="#Qualitative-Evaluation-and-Error-Analysis" class="headerlink" title="Qualitative Evaluation and Error Analysis"></a>Qualitative Evaluation and Error Analysis</h4><p>Goal: convince that your hypothesis is correct.</p><p>Interesting hypotheses are often hard to evaluate with<br>standard/intuitive quantitative metrics.</p><p>Start with:</p><ul><li>Look to prior work</li><li>Show examples of system output</li><li>Identify qualitative categories of system error and count them</li><li>Visualize your embedding spaces with tools like t-SNE or PCA</li><li>Visualize your hidden states with tools like LSTMVis</li><li>Plot how your model performance varies with amount of data</li><li>Build an online demo if ambitious (which I‚Äôm probably not <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">ü§î</span>)</li></ul><p><strong>Formative evaluation</strong>: guiding further investigations</p><ul><li>Typically: lightweight, automatic, intrinsic</li><li>Compare design option A to option B</li><li>Tune <em>hyperparameters</em> (parameter whose value is set before the learning process begins): smoothing, weighting, learning rate</li></ul><p><strong>Summative evaluation</strong>: reporting results</p><ul><li>Compare your approach to previous approaches</li><li>Compare different major variants of your approach</li><li>Generally only bother with human or extrinsic evaluations here</li></ul><p><em>Common mistake: Don‚Äôt save all your qualitative evaluation<br>for the summative evaluation.</em></p><h4 id="Hyperparameter-Tuning"><a href="#Hyperparameter-Tuning" class="headerlink" title="Hyperparameter Tuning"></a>Hyperparameter Tuning</h4><ul><li>Must tune the hyperparameters of your baselines just as thoroughly as you tune them for any new model you propose</li><li>Failure to do this invalidates your comparisons</li><li>As always, don‚Äôt tune on test set.</li><li>Read the fine print while you‚Äôre doing your literature review to get a sense of what hyperparameters to worry about and what values to expect.</li><li>If you‚Äôre not sure whether to tune a hyperparameter, you probably should.</li></ul><p>Methods:</p><ul><li>Grid search: Inefficient (but common)</li><li>Bayesian optimization: Optimal, but public packages aren‚Äôt great.</li><li>Good read: random search - easy, and near-optimal<ul><li>Define distributions over all your hyperparameters.</li><li>Sample N times for N experiments.</li><li>Look for patterns in your results.</li><li>Adjust the distributions and repeat until you run out of<br>resources or performance stops improving.</li></ul></li></ul><h4 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h4><p><strong>Biases</strong></p><p>Deploying biased models in the wrong places can lead to harms far worse than bad user experiences.</p><p>Model de-biasing can be complex, political, and maybe even impossible to do fully, and it may harm performance on reasonable metrics.</p><details><summary>Unfold to see a bias example</summary><p>In training data, women appear in cooking scenes 33% more<br>often than men.</p><p>In model‚Äôs labeling of similar test data, women are detected in cooking scenes 68% more often than men.</p></details><p><strong>Talk about data</strong></p><ul><li>What your data looks like, why it was collected, and what kind of information your system learn from it</li><li>Who (country, region, gender, native language, etc.) produced the text and labels in your dataset</li><li>Any known biases in your dataset (including the obvious ones)</li></ul><h2 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h2><p>Examples of classification problems: spam vs. non-spam, text genre, word sense, etc.</p><p><strong>Supervised Learning</strong>:</p><ul><li>Na√Øve Bayes</li><li>Log-linear models (Maximum Entropy Models)</li><li>Weighted linear models and the Perceptron</li><li>Neural networks</li></ul><p>Learning from annotated data problem:</p><ul><li>Annotation requires specific expertise.</li><li>Annotation is expensive.</li><li>Data is private and not accessible.</li><li>Often difficult to define and be consistent.</li></ul><p><em>Always think about the data, and how much of it your model needs (even better: think of the data first, model second).</em></p><p><strong>Training data, development data and held-out data</strong>: an important tool for estimating generalization is train on one set, evaluate during development on another, and use test data only once.</p><p><strong>Main ideas of text classification</strong>:</p><ul><li>Representation as feature vectors</li><li>Scoring by linear functions</li><li>Learning by optimization</li></ul><p><strong>Probabilistic classifiers</strong>: two broad approaches to predict a class $y$</p><p>a) Joint / Generative models (e.g. Na√Øve Bayes)</p><ul><li>Work with a joint probabilistic model of data $P(X,y)$, weights are (often) local conditional probabilities.</li><li>Often assume functional form for $P(X|y), P(y)$; represent p(y,X) as Na√Øve Bayes model, compute $y*=argmax_y p(y,X)= argmax_y p(y)p(X|y)$.</li><li>Estimate <em>probabilities</em> from data.</li><li>Advantages: learning weights is easy and well understood.</li></ul><p>b) Conditional / Discriminative models (e.g. Logistic Regression)</p><ul><li>Work with conditional probability $p(y|X)$. We can then directly compute $y* = argmax_y p(y|X)$ (estimating $p(y|X)$ directly).</li><li>Require numerical optimization methods.</li><li>Estimate <em>parameters</em> from data.</li><li>Advantages: Don‚Äôt have to model $p(X)$! Can develop feature rich models for $p(y|X)$.</li><li>Popular in NLP.</li></ul><h3 id="Generative-Approach-Naive-Bayes-Models"><a href="#Generative-Approach-Naive-Bayes-Models" class="headerlink" title="Generative Approach: Na√Øve-Bayes Models"></a>Generative Approach: Na√Øve-Bayes Models</h3><p>The generative story: <strong>pick a topic, then generate a<br>document.</strong></p><p><strong>Assumption: All words (features) are independent given the topic (label).</strong></p><p>Order invariant for tokens.</p><p>Issues: underflow; large number of topics (a lot of computing).</p><p>NB Learning: <strong>Maximum Likelihood Estimate</strong></p><p>In MLE, two parameters to estimate: 1) $ùëû(ùë¶) = \theta_y$ for each topic $ùë¶$; 2) $q(x|y) = \theta_{xy}$ for each topic $y$ and word $x$.</p><p><strong>Zero frequency problem</strong> in MLE: if there is a zero in the calculation the whole product becomes zero, no matter how many other values you got which maybe would find another solution.</p><p>Learning by <strong>count</strong>: $\theta_y = C(y)/N$, $\theta_{xy} = C(x,y)/C(y)$. The learning complexity is <strong>O(n)</strong>.</p><p><em>Word sense</em>: bag-of-words classification works ok for nouns, but verbal senses are less topical and more sensitive to structure (argument choice).</p><h3 id="Discriminative-Approach-Linear-Models"><a href="#Discriminative-Approach-Linear-Models" class="headerlink" title="Discriminative Approach: Linear Models"></a>Discriminative Approach: Linear Models</h3><p>Features are indicator functions which count the occurrences of certain patterns in the input. Initially we will have different feature values for every pair of input $X$ and class $y$.</p><p><strong>Block Feature Vectors</strong>: Input has features, which are multiplied by outputs to form the candidates.</p><p>Different candidates will often share features.</p><p>In linear models, each feature gets a weight in $w$. We compute the scores and then according to the prediction rule, we choose the highest (positive) one.</p><details><summary>Example of Linear Model</summary><p>$\Phi(X, SPORTS) = [1 0 1 0 \space 0 0 0 0 \space 0 0 0 0]$</p><p>$\Phi(X, POLITICS) = [0 0 0 0 \space 1 0 1 0 \space 0 0 0 0]$</p><p>$\Phi(X, OTHER) = [0 0 0 0 \space 0 0 0 0 \space 1 0 1 0]$</p><p>$W = [1 \space 1 \space -1 \space 2, 1 \space -1 \space 1 \space -2, -2 \space -1 \space -1 \space 1]$</p><p>Respectively $SCORE= 0;2;-3$, thus $prediction = POLITICS$</p></details><p>(Multinomial) Na√Øve-Bayes is a linear model.</p><p><strong>Picking weights</strong>:</p><ul><li>Goal: choose ‚Äúbest‚Äù vector $w$ given training data.</li><li>The best we can ask for are weights that give best training set accuracy, but it‚Äôs a hard optimization problem.</li></ul><h3 id="Discriminative-Approach-Maximum-Entropy-Models-Logistic-Regression"><a href="#Discriminative-Approach-Maximum-Entropy-Models-Logistic-Regression" class="headerlink" title="Discriminative Approach: Maximum Entropy Models (=Logistic Regression)"></a>Discriminative Approach: Maximum Entropy Models (=Logistic Regression)</h3><p>Use the scores as probabilities.</p><p>Learning: maximize the (log) conditional likelihood of training data ${(X^{(i)}, y^{(i)})}^N_{i=1}$</p><p>An equation is said to be a <strong>closed-form solution</strong> if it solves a given problem in terms of functions and mathematical operations from a given generally accepted set.</p><p>$L(w) = \Sigma^{N}_{i=1}logP(y^{(i)}|X^{(i)}; w), w^*=argmax_wL(w)$</p><p>‚Äì $w^<em>$ doesn‚Äôt have a closed-form solution. So the MaxEnt objective is an *unconstrained optimization problem</em>.</p><ul><li>Basic idea: move uphill from current guess.</li><li>Gradient ascent / descent follows the gradient incrementally.</li><li>At <strong>local optimum</strong>, derivative vector is <strong>zero</strong>.</li><li>Will converge <strong>if step sizes are small enough</strong>, but <strong>not efficient</strong>.</li><li>All we need is to be able to evaluate the function and its derivative. Once we have a function $f$, we can find a local optimum by iteratively following the gradient.</li><li>For <strong>convex</strong> (curved or rounded outward) functions, <strong>a local optimum will be global</strong>. Convexity guarantees a single, global maximum value because any higher points are greedily reachable.</li></ul><p>Basic gradient ascent isn‚Äôt very efficient, but there are simple enhancements which take into account previous gradients: conjugate gradient, L-BFGS. There are special-purpose optimization techniques for MaxEnt, like<br>iterative scaling, but they aren‚Äôt better.</p><p><strong>The optimum parameters are the ones for which each feature‚Äôs predicted expectation equals its empirical expectation.</strong></p><p>In logistic regression, instead of worrying about zero count in MLE, we worry about <em>large feature weights</em>. Use regularization (smoothing) for log-linear models (add a L2 regularization term to the likelihood to push weights to zero).</p><p>But even after regularization, MaxEnt still doesn‚Äôt have a closed-form solution. We will have to differentiate and use gradient ascent.</p><h3 id="Perceptron-Algorithm"><a href="#Perceptron-Algorithm" class="headerlink" title="Perceptron Algorithm"></a>Perceptron Algorithm</h3><p>Iteratively processes the training set, reacting to training errors. Can be thought of as trying to drive down training error.</p><ul><li>Online (or batch)</li><li>Error driven</li><li>Simple, additive updates</li></ul><h4 id="Binary-Class"><a href="#Binary-Class" class="headerlink" title="Binary Class"></a>Binary Class</h4><p><strong>Steps</strong> for the online (binary $y = \plusmn1$) perceptron algorithm:</p><ol><li>Start with <strong>zero</strong> weights</li><li>Visit training instances $(X^{(i)}, y^{(i)})$ one by one, until all correct<ul><li>Make a prediction: $y^* = sign(w ¬∑\Phi(X^{(i)}))$ (sign means whether it &gt;(=) 0)</li><li>If correct ($y^*==y^{(i)}$): no change, go to next example!</li><li>If wrong: adjust weights $w = w - y^* \Phi(X^{(i)})$</li></ul></li></ol><p>The perceptron finds a separating hyperplane.</p><p>If add bias, algorithm stays the same.</p><h4 id="Multiclass"><a href="#Multiclass" class="headerlink" title="Multiclass"></a>Multiclass</h4><p>For multiclass situation, we will have:</p><ul><li>A weight vector for each class</li><li>Score (activation) of a class $y$: $w_y¬∑\Phi(x)$</li><li>Compare all possible outputs, prediction highest score wins</li></ul><p><strong>Perceptron learning traits</strong>:</p><ul><li>No counting or computing probabilities on training set</li><li>Separability: some parameters get the training set perfectly correct</li><li>Convergence: if the training is separable (e.g. could divide into two classes with 100% accuracy for binary case), perceptron will<br>eventually converge</li><li>Mistake Bound: the maximum number of mistakes (binary case)<br>related to the margin or degree of separability</li></ul><p><strong>Perceptron problems</strong>:</p><ul><li>Noise: if the data isn‚Äôt separable, weights might<br>thrash<ul><li>Averaging weight vectors over time can help (averaged perceptron)</li></ul></li><li>Mediocre generalization: finds a ‚Äúbarely‚Äù separating solution</li><li>Overtraining: test / held-out accuracy usually<br>rises, then falls (overtraining is a kind of overfitting)</li></ul><h3 id="A-Note-for-Features-TF-IDF"><a href="#A-Note-for-Features-TF-IDF" class="headerlink" title="A Note for Features: TF/IDF"></a>A Note for Features: TF/IDF</h3><ul><li>More frequent terms in a document are more important.</li><li>May want to normalize term frequency (TF) by dividing by the frequency of the most common term in the document.</li><li>Terms that appear in many different documents are less indicative (thus inverse document frequency) </li></ul><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p><strong><em>Representation Learning</em></strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning | Êú∫Âô®Â≠¶‰π† </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Computer Vision</title>
      <link href="/2020/02/12/notes-for-computer-vision/"/>
      <url>/2020/02/12/notes-for-computer-vision/</url>
      
        <content type="html"><![CDATA[<h3 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h3><p><strong>Image</strong>: a grid(matrix) of intensity values</p><p><strong>Kernel</strong>: the filter/mask.</p><p><strong>Gradient</strong>(of an image): $‚ñΩf = [df/dx, df/dy]$(partial derivative), points in the direction of the most rapid increase in intensity.</p><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>Form a new image whose pixels are a combination of the original ones.</p><p>Use filter to get useful information from images or enhance the image.</p><h4 id="Linear-Filter"><a href="#Linear-Filter" class="headerlink" title="Linear Filter"></a>Linear Filter</h4><p>Replace each pixel by a linear combination (weighted sums) of its neighbors.</p><ol><li><p>Cross-correlation</p><p> Like a dot product - sum of the neighbor matrix multiplied by kernel for every exactly matching position. e.g. (5, 3) * (5, 3)</p></li></ol><ol start="2"><li><p>Convolution</p><p> Like an outer product - compared with cross correlation, the kernel is flipped both horizontally and vertically.</p><p> Convolution is commutative and associative. </p><p> Why convolution: consider the situation between two identical images except one of them is flipped/turned/etc.</p></li><li><p>Blurring / Sharpening</p><p> Mean filter, box filter</p></li></ol><h4 id="Gaussian-filter"><a href="#Gaussian-filter" class="headerlink" title="Gaussian filter"></a>Gaussian filter</h4><p>Apply a Gaussian function. Recall that the weights should always be normalized to sum=1.</p><p>Removes ‚Äúhigh-frequency‚Äù components (low-pass filter).</p><p>Convolution with itself will get another Gaussian filter.</p><h3 id="Edge-Detection"><a href="#Edge-Detection" class="headerlink" title="Edge Detection"></a>Edge Detection</h3><p>Convert a 2D image into a set of curves.</p><p>An edge is a place of rapid change in the image intensity function.</p><p>Edges are caused by a variety of factors:</p><ul><li>surface normal discontinuity</li><li>depth discontinuity</li><li>surface color discontinuity</li><li>illumination discontinuity</li></ul><p>To differentiate a digital image:</p><ul><li>reconstruct a continuous image, then compute the derivative</li><li>take discrete derivative(find difference)</li></ul><p>For noisy input images, smooth them first with Gaussian filter(?).</p><p><strong>Sobel Operator</strong>: Common approximation of derivative of Gaussian (need the 1/8 to get the right gradient magnitude)</p><p>Thresholding edges - 2 thresholds, 3 cases (strong edge)</p><p>Connecting edges - Weak edges are edges if they are connected to strong edges. Look in some neighborhood (usually 8 closest).</p><h4 id="Canny-Edge-Detector"><a href="#Canny-Edge-Detector" class="headerlink" title="Canny Edge Detector"></a>Canny Edge Detector</h4><ol><li>Filter image with derivative of Gaussian</li><li>Find magnitude and orientation of gradient</li><li>Non-maximum suppression</li><li>Linking and thresholding (hysteresis): one low and one high threshold; use the high one to start edge curves and the low threshold to continue them</li></ol><p>Parameters: high threshold, low threshold, sigma(width of the Gaussian blur, large sigma detects large-scale edges)</p><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><h4 id="Down-sampling"><a href="#Down-sampling" class="headerlink" title="Down-sampling"></a>Down-sampling</h4><p>Making smaller images.</p><p>e.g. throw away every other row and column to create a 1/2 size image.</p><p><strong>Aliasing</strong>: Get a wrong image by sub-sampling (this problem occurs especially for synthetic images).</p><p>Aliasing occurs when your sampling rate is not high enough to capture the amount of detail in your image.</p><p>To avoid aliasing: sampling rate ‚â• 2 * max frequency in the image</p><p><strong>Nyquist Rate</strong>: minimum sampling rate</p><p>Wagon-wheel effect: in video wheel appear to rotate backwards</p><p>Fix aliasing: <strong>Gaussian pre-filtering</strong></p><p><strong>Gaussian pyramid</strong>: Represent N<em>N image as a pyramid of $1</em>1, 2<em>2, 4</em>4, 2^k*2^k$ (assuming $N=2^k$)</p><h4 id="Up-sampling-Image-Interpolation"><a href="#Up-sampling-Image-Interpolation" class="headerlink" title="Up-sampling / Image Interpolation"></a>Up-sampling / Image Interpolation</h4><p>Correspondingly, making bigger images.</p><ol><li>Nearest-neighbor interpolation (use neighbor value to replace)</li><li>Linear interpolation (use a line to fit the gap)</li><li>Gaussian reconstruction</li><li>Bicubic interpolation</li></ol><h3 id="Feature-Detection"><a href="#Feature-Detection" class="headerlink" title="Feature Detection"></a>Feature Detection</h3><p>Can be used to conduct automatic panoramas.</p><p>Combine two images:</p><ol><li>Extract features</li><li>Match features</li><li>Align images</li></ol><p>Find features that are invariant to transformations:</p><ul><li>Geometric invariance: translation, rotation, scale</li><li>Photometric invariance: brightness, exposure, etc.</li></ul><p><strong>Local Features</strong> refer to a pattern or distinct structure found in an image, such as a point, edge, or small image patch.</p><p>Advantages of local features:</p><ul><li>Locality: robust to occlusion (means some sort of blocking of an object[not sure]) and clutter (lots of objects in the image).</li><li>Quantity: hundreds or thousands in a single image</li><li>Distinctiveness: differentiate a large database of objects</li><li>Efficiency: real-time performance achievable</li></ul><p>Good features - look for image regions that are ‚Äúunusual‚Äù.</p><h4 id="Harris-Corner-Detection"><a href="#Harris-Corner-Detection" class="headerlink" title="Harris Corner Detection"></a>Harris Corner Detection</h4><p>Consider shifting the window $W$ by $(u,v)$, compute the squared differences (SSD). Look for high SSDs. </p><p>$E(u,v) \approx [ u \space v ] \begin{bmatrix}A &amp; B \ B &amp; C\end{bmatrix} \begin{bmatrix} u \ v \end{bmatrix}$</p><p>$A = \Sigma I_x^2(dI/dx), B = \Sigma I_xI_y, C = \Sigma I_y^2(dI/dy)$</p><p>$E(u,v)$ is locally approximated as a quadratic error function.</p><p>Corner detection summary:</p><ol><li>Compute the gradient at each point in the image.</li><li>Compute the ‚ÄúABBC‚Äù matrix from the entries in the gradient.</li><li>Compute the eigenvalues.</li><li>Find points with large response ($\lambda_{max}&gt;threshold$).</li><li>Choose these points where $\lambda_{min}$ is a local maximum as features.</li></ol><p>In practice, using a simple window $W$ doesn‚Äôt work too well. Instead, we‚Äôll weight each derivative value based on its distance from the center pixel.</p><h4 id="Image-Transformations"><a href="#Image-Transformations" class="headerlink" title="Image Transformations"></a>Image Transformations</h4><p>Geometric: Rotation, Scale</p><p>Photometric: Intensity</p><p>We want corner locations to be invariant to photometric transformations and covariant to geometric transformations.</p><p><strong>Invariance</strong>: Image is transformed and corner locations do not change.</p><p><strong>Covariance</strong>:  If we have two transformed versions of the same image, features should be detected in corresponding locations.</p><p>In <em>Harris Detector</em>:</p><ul><li>Corner location is covariant w.r.t. translation.</li><li>Corner location is covariant w.r.t. rotation.</li><li>Partially invariant to affine intensity change.</li><li>Not invariant to scaling.</li></ul><p><strong>Laplacian of Gaussian</strong> (LoG): Use a Gaussian filter first then Laplacian.</p><p><strong>Difference of Gaussian</strong> (DoG): A Gaussian minus a slightly smaller Gaussian.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning | Êú∫Âô®Â≠¶‰π† </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Behavioral Economics</title>
      <link href="/2020/01/26/notes-for-behavioral-economics/"/>
      <url>/2020/01/26/notes-for-behavioral-economics/</url>
      
        <content type="html"><![CDATA[<h4 id="Making-a-resolution"><a href="#Making-a-resolution" class="headerlink" title="Making a resolution?"></a>Making a resolution?</h4><p>A resolution may be wrong for one of three main reasons:</p><ul><li>It‚Äôs a resolution created based on what someone else (or society) is telling you to change.</li><li>It‚Äôs too vague.</li><li>You don‚Äôt have a realistic plan for achieving your resolution.</li></ul><p>Make <strong>SMART</strong> goals - Specific, Measurable, Achievable, Relevant and Time-bound.</p><p>To figure out how to change a habit, try breaking it down into three parts: a cue, a routine and a reward. It‚Äôs important to know why you do (or can‚Äôt do) something so you could control it.</p><details><summary>Example here</summary>Bad Habit: I don't get enough sleep at night.<p>Cue: I feel like I need time to myself in the evening.</p><p>Routine: I stay up too late watching TV.</p><p>Reward: I‚Äôm entertained.</p><p>Way to change the behavior: Instead of staying up late to watch TV, carve out special time each day to spend by yourself, even if that may mean asking for help with your children or taking a break from work each day.</p></details><p>A plan should allow some inevitable situations (slip days, or I would like to call ‚Äúfault tolerance‚Äù) and prepare for them before they actually show up. Some useful technique include:</p><ul><li>Focus on the small one between what is done and what left.</li><li><strong>Don‚Äôt be too positive</strong> because who needs a real achievement if already enjoying a fantastic daydream? Try W.O.O.P.: Wish, Outcome, Obstacle, Plan.</li></ul><p>To keep up the resolution, other things might help are:</p><ul><li>Tell someone(or post on public platform even if nobody reads it) about your plan.</li><li>Make yourself lose something if you fail. Money is a good choice.</li><li>Find a group to get some good peer pressure.</li><li><strong>Do a ‚Äústress test‚Äù before it start to know how feasible it is.</strong></li></ul><h4 id="Cognitive-Biases"><a href="#Cognitive-Biases" class="headerlink" title="Cognitive Biases"></a>Cognitive Biases</h4><p>The picture below has done a great work summarizing this topic. </p><br><div align="center"><img src="https://miro.medium.com/max/3072/1*71TzKnr7bzXU_l_pU6DCNA.jpeg" title="Cognitive Bias Codex"></div><br><p>Some thoughts:</p><ul><li>Will the biases cancel each other under certain circumstances?</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Psychological Fun | ÂøÉÁêÜ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Making Aesthetic Interfaces</title>
      <link href="/2019/11/25/making-aesthetic-interfaces/"/>
      <url>/2019/11/25/making-aesthetic-interfaces/</url>
      
        <content type="html"><![CDATA[<p>Expert designers usually do not solve every problem, instead, they <strong>reuse</strong> solutions that worked before, aka. design patterns. </p><p><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> is always a good reference resource.</p><h3 id="Hierarchy"><a href="#Hierarchy" class="headerlink" title="Hierarchy"></a>Hierarchy</h3><p>Decide the content to present, level of importance for each element and organize them.</p><p><strong>Steps</strong> to implement visual hierarchy: collect -&gt; group -&gt; prioritize.</p><p><strong>Aspects</strong>: scale, color, contrast, alignment, proximity.</p><p><strong>Components</strong>: buttons, cards, lists, tabs, menus, etc.</p><h3 id="Typography"><a href="#Typography" class="headerlink" title="Typography"></a>Typography</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Two easily mixed up terms:</p><ul><li>A <strong>typeface</strong> is a style of type design which includes a complete scope of characters in all sizes and weight. e.g. Noto Sans SC. Usually could be divided into <strong>Sans-serif</strong> and <strong>Serif</strong>.</li><li>A <strong>font</strong> is a graphical representation of text character usually introduced in one particular typeface, size, and weight. e.g. Multiple files of Noto Sans SC with different suffixes when you download it from Google Fonts.</li></ul><h4 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h4><p>Consider the common small notebooks with four lines for each row used in elementary schools‚Ä¶it will help understand some of the concepts below.</p><ul><li><p>Mean line and baseline</p><p>The two lines in the middle.</p></li><li><p>X-height (x)</p><p>The height between the two lines, which equals to the height of a lowercased <em>x</em>.</p></li><li><p>Ascender and Descender</p><p>The part above the mean line is ascender. Correspondingly, descender is the part below the baseline.</p></li><li><p>Alignment</p><p>Besides left, right and center, there‚Äôs a <strong>justified</strong> alignment for English which could be seen used in newspaper. </p></li><li><p>Line length</p><p>A perfect line length is like row numbers when writing codes.</p></li><li><p>Tracking and Kerning</p><p>Space between letters. The former refers to general ones while the latter refers to specific ones.</p></li><li><p>Leading</p><p>The spacing between the baselines of copy.</p></li><li><p>Indent and Line Space</p><p>Paragraph-wise concepts.</p></li></ul><h4 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h4><ul><li>One typeface for headline and one for body text is usually enough.</li><li>Never use both indent and line space.</li><li>In design, the standard leading is 120% the point size of the font.</li></ul><h3 id="Color-Palettes"><a href="#Color-Palettes" class="headerlink" title="Color Palettes"></a>Color Palettes</h3><h4 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h4><ul><li><strong>Additive / RGB</strong>: use for digital mediums</li><li><strong>Subtractive / CMYK</strong>: use for paint and print</li></ul><h4 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h4><ul><li><p>Chroma</p><p>Purity of a color. A hue with high chroma has no black, white or gray added.</p><p>Use hues with chromas that are either exactly the same or a few steps away from each other on the color wheel.</p></li><li><p>Hue</p><p>Just color, like purple, blue, etc.</p></li><li><p>Saturation</p><p>How a hue appears under particular lighting conditions.</p></li><li><p>Value</p><p>How light or dark a color is.</p></li></ul><h4 id="Schemes"><a href="#Schemes" class="headerlink" title="Schemes"></a>Schemes</h4><ul><li><p>Monochromatic</p><p>Different shades of a specific hue.</p><p>Hard to make mistakes.</p><p>Adding a strong neutral (e.g. black or white) could avoid boredom.</p></li><li><p>Analogous</p><p>Use colors located right next to each other on the color wheel.</p></li><li><p>Complementary</p><p>Combine colors from the opposite site of the color wheel.</p><p>Provides high contrast.</p></li><li><p>Triadic</p><p>Based on three separate colors which are equidistant on the color wheel.</p></li></ul><br><br><p><em>Ref:</em></p><p><a href="https://designshack.net/articles/ux-design/google-material-design-everything-you-need-to-know/" target="_blank" rel="noopener">https://designshack.net/articles/ux-design/google-material-design-everything-you-need-to-know/</a></p><p><a href="https://uxplanet.org/typography-in-ui-guide-for-beginners-7ee9bdbc4833" target="_blank" rel="noopener">https://uxplanet.org/typography-in-ui-guide-for-beginners-7ee9bdbc4833</a></p><p><a href="https://uxplanet.org/color-theory-brief-guide-for-designers-76e11c57eaa" target="_blank" rel="noopener">https://uxplanet.org/color-theory-brief-guide-for-designers-76e11c57eaa</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | ËÆæËÆ° </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> UI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms and Data Structures</title>
      <link href="/2019/11/11/algorithms-and-data-structures/"/>
      <url>/2019/11/11/algorithms-and-data-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="Theories-and-Methods"><a href="#Theories-and-Methods" class="headerlink" title="Theories and Methods"></a>Theories and Methods</h2><h3 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h3><h4 id="Situation"><a href="#Situation" class="headerlink" title="Situation"></a>Situation</h4><p>A duplicate process, overlapping subproblems</p><p>Recursion in brute force, optimal substructure</p><h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul><li>Memorization</li><li>Table filling</li></ul><h3 id="Other-Frequently-Encountered-Skills"><a href="#Other-Frequently-Encountered-Skills" class="headerlink" title="Other Frequently Encountered Skills"></a>Other Frequently Encountered Skills</h3><h4 id="‚ÄúWalker‚Äù-and-‚Äúrunner‚Äù-Solution"><a href="#‚ÄúWalker‚Äù-and-‚Äúrunner‚Äù-Solution" class="headerlink" title="‚ÄúWalker‚Äù and ‚Äúrunner‚Äù Solution"></a>‚ÄúWalker‚Äù and ‚Äúrunner‚Äù Solution</h4><p>The former move one step every time while the latter move two. Use to detect cycles.</p><h4 id="Prefix-Sums"><a href="#Prefix-Sums" class="headerlink" title="Prefix Sums"></a>Prefix Sums</h4><p>A new array in which <code>new[i] = sum(old[:i])</code>. Take O(n) to construct, but can make it constant time for the following operations.</p><p>e.g. Find subarrays of $k$ elements that has average value more than $t$.</p><h2 id="Examples-of-Problem-Solution"><a href="#Examples-of-Problem-Solution" class="headerlink" title="Examples of Problem/Solution"></a>Examples of Problem/Solution</h2><ul><li>Find the $N^{th}$ fibonacci number. [Dynamic Programming]</li><li>Given two strings s and t, find the longest subsequence (same order but can skip some letters) common to both strings. [Dynamic Programming]</li><li>Given array $a$ containing integers $[x_1, ‚Ä¶, x_n]$, find integers $i, j$ such that $1 ‚â§ i ‚â§ j ‚â§ n$ and $\Sigma^{j}_{i}a$ is maximal. [Dynamic Programming]</li></ul><h2 id="Application-Reflex"><a href="#Application-Reflex" class="headerlink" title="Application Reflex"></a>Application Reflex</h2><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li>Usually recursion takes more space and runs faster, while the ‚Äúfor‚Äù loop is the opposite.</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding | ÁºñÁ®ã </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prototypes</title>
      <link href="/2019/11/10/prototypes/"/>
      <url>/2019/11/10/prototypes/</url>
      
        <content type="html"><![CDATA[<p>There are several things people can do to test out a product. In this post I combine prototypes together with personas and storytelling, but in fact they are NOT really part of prototypes.</p><h3 id="Designers-Oriented"><a href="#Designers-Oriented" class="headerlink" title="Designers Oriented"></a>Designers Oriented</h3><p>Methods that don‚Äôt necessarily involve users.</p><h4 id="Personas"><a href="#Personas" class="headerlink" title="Personas"></a>Personas</h4><p>A fictional character that is meant to represent a group of users that share common goals, attitudes and behaviors when interacting with a particular product or service.</p><p>Steps:</p><ol><li>Collect data of users.</li><li>Segment the users.</li><li>Create personas.</li></ol><p>Personas can help designers have <strong>empathy</strong>.</p><p>Question from myself: how do you decide whether a persona is logical or realistic?</p><p>It‚Äôs easy to find online tools to help you design a persona.</p><h4 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h4><p>Written accounts and narratives of an experience. Imagining a particular situation personas encounter.</p><p>Components: motivation, context, distractions, goal</p><h4 id="Storyboarding"><a href="#Storyboarding" class="headerlink" title="Storyboarding"></a>Storyboarding</h4><p>Illustrations(drawings) that represent a story. Here‚Äôs a storyboard I drew in class:</p><br><div align="center"><img src="https://od.lk/s/MzBfMTgwMDQxMDFf/storyboard.jpg" width="50%" height="50%" title="My Drawing of Storyboard"></div><p>It‚Äôs about a situation where people have issues when using smart home devices.</p><p>Compared with actually ‚Äútelling‚Äù a story, it‚Äôs more direct and could convey more information with less content (save paper <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">üòâ</span>).</p><p>Some rules:</p><ul><li>Only include necessary details. </li><li>Be proud to be a terrible painter.</li><li>The number of frames should be around six.</li></ul><h3 id="Users-Oriented"><a href="#Users-Oriented" class="headerlink" title="Users Oriented"></a>Users Oriented</h3><p>Requires the engagement of users.</p><h4 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h4><p>Simulate the design with low cost. Could be done iteratively during design.</p><p>Two ways of prototyping:</p><ul><li>Vertical: show only part of the interface, but very detailed.</li><li>Horizontal: show many aspects of the interface in a shallow manner.</li></ul><h4 id="Low-Fidelity-Paper-Prototype"><a href="#Low-Fidelity-Paper-Prototype" class="headerlink" title="Low Fidelity: Paper Prototype"></a>Low Fidelity: Paper Prototype</h4><p>Although it sounds more complicated, in fact it‚Äôs <strong>much easier and faster</strong> to implement compared with prototyping with fancy tools, even if you‚Äôre a terrible painter.</p><p>It helps designers and users <strong>focus on big things</strong>, e.g. interface logics, instead of spending hours trying to choose a perfect font (that‚Äôs what I usually did‚Ä¶)</p><p>You are more likely to get ‚Äúhonest‚Äù feedback with paper prototype because, people will hesitate to say something negative if they realize you have put a large amount of work into it.</p><p><em>*Generally it kind of looks like the expensive and fancy cards that were popular among primary school students.</em></p><h4 id="High-Fidelity-Digital-Prototype"><a href="#High-Fidelity-Digital-Prototype" class="headerlink" title="High Fidelity: Digital Prototype"></a>High Fidelity: Digital Prototype</h4><p>Tools: Figma, Invision, Sketch, and my favorite - <strong>Proto.io</strong>.</p><p>A website that helps with choosing tools: <a href="http://www.prototypr.io/prototyping-tools/" target="_blank" rel="noopener">http://www.prototypr.io/prototyping-tools/</a></p><p>For details in digital prototyping like hierarchy, typography, colors, etc. please refer to another post.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | ËÆæËÆ° </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> UX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thoughts from Contextual Inquiry</title>
      <link href="/2019/11/09/thoughts-from-contextual-inquiry/"/>
      <url>/2019/11/09/thoughts-from-contextual-inquiry/</url>
      
        <content type="html"><![CDATA[<h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Contextual inquiry is a qualitative research method. The researcher will give participants a topic to do something, then observe and ask probing questions.</p><h4 id="More-Explanation"><a href="#More-Explanation" class="headerlink" title="More Explanation"></a>More Explanation</h4><ul><li>It takes place in the context of use, e.g. where (home/work)  people will use your app product.</li><li>The purpose is to understand why people do certain things and enhance user experience.</li><li>It‚Äôs a very ‚Äúfree‚Äù research method. </li></ul><h4 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Use ‚Äúparticipant‚Äù instead of ‚Äúuser‚Äù or ‚Äúsubject‚Äù in the research to connote an active role.</li><li>Introduce the research method to participants and make sure they know that it‚Äôs the product that is being tested.</li><li>Explain your conclusions and interpretations to participants through out the interview to avoid any misunderstanding.</li></ul><h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul><li>Start data analysis ASAP.</li><li>Contextual inquiry can be conducted in any stage of design.</li></ul><h4 id="My-Thoughts"><a href="#My-Thoughts" class="headerlink" title="My Thoughts"></a>My Thoughts</h4><p>When you ask the participants why they do something in a contextual inquiry, they might realize things that they never noticed before. So does <strong>psychology</strong>. </p><p>Just have a friend keep asking questions when describing experiences or stating points can be really helpful. When people thinking by themselves, it‚Äôs easy for them to ‚Äúfall into routines‚Äù and take something for granted. Even if they try to remind themselves from time to time, it‚Äôs still not as good as a different perspective.</p><p>It can also be applied to <strong>science</strong>, at least in the past, when no one had been curious enough about the reasons behind falling apples. In general, ‚Äúwhy‚Äù questions can be annoying, but lose the ability to wonder‚Ä¶it seems way more worse to me.</p><br><br><p><em>Ref:</em></p><p><a href="https://www.interaction-design.org/literature/article/contextual-interviews-and-how-to-handle-them" target="_blank" rel="noopener">https://www.interaction-design.org/literature/article/contextual-interviews-and-how-to-handle-them</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | ËÆæËÆ° </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Âµå‰∫éÁ™óÈó¥ÁöÑÊ°•Ëæâ</title>
      <link href="/2019/10/30/qian-yu-chuang-jian-de-qiao-hui/"/>
      <url>/2019/10/30/qian-yu-chuang-jian-de-qiao-hui/</url>
      
        <content type="html"><![CDATA[<p>ÂáåÊô®‰∫îÁÇπÈíüÊòØQueensboroughÂ§ßÊ°•Ê°•Êû∂‰∏äÁöÑÁÅØÂÖâÂáÜÊó∂ÁÜÑÁÅ≠ÁöÑÊó∂Èó¥„ÄÇ</p><p>‚Äî‚ÄîËøôÊòØÊàëÂú®‰ªäÂ§©Áü•ÈÅìÁöÑÊñ∞‰ø°ÊÅØ„ÄÇÊàëÁîöËá≥Ëøò‰∏∫Ê≠§ÂèëÂ∏É‰∫Ü‰∏ÄÊù°ÊúãÂèãÂúàÔºå‰ªø‰Ωõ‰∏Ä‰∏™Âõ∞Êâ∞ËÆ∏‰πÖÁöÑË∞úÈ¢òÂæóÂà∞‰∫ÜËß£Á≠îÔºåËÄåÊàëÂú®ÊääÂÆÉÂÖ¨ËØ∏‰∫é‰∏ñ„ÄÇ</p><p>ÂÆûÈôÖ‰∏ä‰πüÂπ∂‰∏çÊòØÊ≤°ÊúâËøô‰πàËØ¥ÁöÑÈÅìÁêÜ„ÄÇÂú®ËÆ∏Â§ö‰∏™Â§úÊôöÔºåÊàëÂú®ÈªëÊöóÁöÑÊàøÈó¥ÈáåÂ±°Ê¨°ÁùÅÂºÄÂèåÁúºÔºåÊúâÊó∂‰ºöÁúãÂà∞Êò†Âú®Á™ó‰∏äÁöÑÈÇ£ÊòüÂ∫ß‰∏ÄËà¨ÁöÑÂÖâÁÇπÈõÜÂêàÔºåÊúâÊó∂ÂàôÂè™ÊúâÂ∑ùÊµÅ‰∏çÊÅØÁöÑËΩ¶„ÄÇÁÑ∂ÂêéÊàë‰æøÂ∏¶ÁùÄÂØπÊó∂Èó¥ÁöÑÂ•ΩÂ•áËæóËΩ¨Âèç‰æßÁõ¥Âà∞ÂÜçÊ¨°Áù°Âéª„ÄÇ</p><p>ÂáåÊô®‰∫îÁÇπÈíü„ÄÇÊâÄË∞ìÁöÑ‚ÄúÊòéÂ§©‚ÄùÁúüÊ≠£Âà∞Êù•ÁöÑÊó∂Âàª„ÄÇ</p><p>ÂÅ∂Â∞îÊàë‰ºöÊÉ≥Ë¶ÅÂêëÂà´‰∫∫ËÆ≤Ëµ∑Á±ª‰ººËøôÊ†∑ÊØ´Êó†ÊÑè‰πâÁöÑÊïÖ‰∫ãÔºåÂÖ≥‰∫éÊàëÊàøÈó¥ÁöÑÁ™óÊà∑ÊòØÂ§ö‰πàÂÉèÈÇ£ÁßçÂ±ïËßà‰ºöÂ∞∫ÂØ∏Â§ßÂ∞èÁöÑÁîªÊ°ÜÔºåËÄåÊàëËÉΩÁúãÂà∞ÁöÑÈ£éÊôØÊûÑÊàê‰∫ÜÂ¶ÇÂêåÂìàÂà©¬∑Ê≥¢ÁâπÁöÑÈ≠îÊ≥ï‰∏ñÁïåÈáå‰ºöÂàäÁôªÂú®Êä•Á∫∏‰∏äÁöÑÂä®ÊÄÅÂõæÁâáÔºõÂÖ≥‰∫éÊ°•‰∏äÁöÑËΩ¶ÊµÅÊòØÊÄéÊ†∑‰ªé‰∏çÂÅúÊ≠á„ÄÅÊØèÊó∂ÊØèÂàªÈÉΩÊòØÈÇ£‰πàÂøôÁ¢åÔºåÂç≥‰ΩøÊòØÂú®ÁîµËßÜËäÇÁõÆÈÉΩ‰ºöÂÅúÊí≠ÁöÑÊó∂ÊÆµ„ÄÇ‰ΩÜÊòØÂ§úËâ≤Âú®‰øùÊä§ÁùÄÊàëÁöÑÂêåÊó∂‰πüÈöîÁªùÁùÄÊàëÔºå‰∫éÊòØÊàëÈÄêÊ∏êÂ≠¶‰ºöÈÄÇÊó∂Âú∞‰øùÊåÅÊ≤âÈªò„ÄÇ</p><p>Â¶ÇÊûúÊòüËæ∞ÊòØÁÅØÂ°î‚Ä¶‚Ä¶ÈÇ£Ëøô‰∫õ‰∏ÄÂõ¢Âõ¢ÊöñËâ≤ÁöÑÂÖâËæâÔºå‰πüÂèØ‰ª•ÊòØÂ∞èÂ∞èÁöÑÁÅØÂ°îÂêóÔºü</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Âè™Ë®ÄÁâáËØ≠ </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Become Pythonic</title>
      <link href="/2019/10/29/notes-for-python-3/"/>
      <url>/2019/10/29/notes-for-python-3/</url>
      
        <content type="html"><![CDATA[<p><em>Note: This post doesn‚Äôt include detailed theories. For those, please refer to other posts.</em></p><h3 id="Built-in"><a href="#Built-in" class="headerlink" title="Built-in"></a>Built-in</h3><h4 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h4><ul><li><code>int()</code> returns the lower bound.</li><li>Pay attention to the difference between <code>extend()</code> and <code>append()</code>, e.g. [1,2,4] vs. [[1,2],[4]].</li></ul><h4 id="Easy-Ways"><a href="#Easy-Ways" class="headerlink" title="Easy Ways"></a>Easy Ways</h4><ul><li>Remember to use lambda for simple functions and use <code>[x for x in statement if statement]</code> for iterating.</li><li>Python has a dictionary <code>defaultdict()</code>, it provides default value given by the user. <code>collections.counter()</code> could be a substitute if the default value is $0$. </li></ul><h4 id="Possible-Mistakes"><a href="#Possible-Mistakes" class="headerlink" title="Possible Mistakes"></a>Possible Mistakes</h4><ul><li><code>list.append()</code> will only return <strong>None</strong>, which indicates it has successfully complete the process. Use <code>list</code> to get to the changes.</li><li>Things like a <code>range(0,n)</code> do NOT include n but do include 0.</li><li>If let <code>a = b</code> where a,b are both lists, when you change a it will affect b too. Try <code>copy.deepcopy</code>(needs import) or <code>new = old[:]</code>.</li></ul><h4 id="Other-in-Doc"><a href="#Other-in-Doc" class="headerlink" title="Other in Doc"></a>Other in Doc</h4><ul><li>The time complexity for <code>sort()</code> is O(nlogn).</li></ul><hr><h3 id="Useful-Packages"><a href="#Useful-Packages" class="headerlink" title="Useful Packages"></a>Useful Packages</h3><h4 id="Package-List"><a href="#Package-List" class="headerlink" title="Package List"></a>Package List</h4><ul><li>*‚Äùnltk‚Äù* is a useful package for natural language processing. The functions include tokenize (divide a sentence into words), strip stop words and punctuations, lemmatize (convert different forms of a word into one, e.g. run, ran, running, runs ‚Üí run).</li><li>The famous *‚Äùsklearn‚Äù*.</li></ul><h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><ul><li>Do not iterate through rows of a <em>pandas</em> dataframe unless really necessary. Usually, just define a function and use <code>.apply(func)</code> instead. No parameter needed when calling.</li><li>If want to make changes to an existing <em>pandas</em> dataframe, remember to write <code>inplace = True</code>.</li><li>Combine <em>pandas</em> dataframes: <code>pandas.concat()</code></li><li><code>pandas.read_csv()</code> is good for loading a data file. The ‚Äúcsv‚Äù could be replaced with other type of file.</li><li>A <em>pandas</em> object could be converted to a numpy array using <code>to_numpy()</code>. </li></ul><h4 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h4><ul><li>Use <code>np.zeros()</code> to create a n-d array with zeros as initial values.</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding | ÁºñÁ®ã </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> updating </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Common Research Methods</title>
      <link href="/2019/10/15/common-research-methods/"/>
      <url>/2019/10/15/common-research-methods/</url>
      
        <content type="html"><![CDATA[<h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a><strong>Surveys</strong></h3><p>Sort of quantitative methods.</p><h4 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Identify topics needed to be covered.</li><li>Pretest drafts.</li><li>Cross-sectional: different people in same population at multiple points in time.</li><li>Panel / longitudinal: same people over time.</li><li>Use open-ended questions in pilot tests and then use the answers to form close-ended ones.</li></ul><h4 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h4><ul><li>Use the same question wording and be careful about where the question is asked.</li><li>The number of choices should be a small number ‚Äì 4 or 5 usually.</li><li>Randomize the order of options to reduce potential bias, except for ordinal ones (e.g. from ‚Äúmost‚Äù to ‚Äúleast‚Äù).</li><li>Ask <strong>clear</strong> and <strong>specific</strong> questions. Use simple and concrete language.</li><li>Only ask <strong>1</strong> question at a time. Don‚Äôt indicate two or more things.</li><li>Be careful not to use biased or potentially offensive to certain respondents. (e.g. biological sex)</li><li>Questions should be grouped and asked in a logical order.</li></ul><h4 id="Fun-Facts"><a href="#Fun-Facts" class="headerlink" title="Fun Facts"></a>Fun Facts</h4><ul><li>People tend to choose the options they heard later in a list (recency effect).</li><li>Compared with the better educated and better informed, less educated and less informed respondents have a greater tendency to agree with certain statements.</li><li>People have a natural tendency to want to be accepted and liked, and this may lead people to provide inaccurate answers to questions that deal with sensitive subjects. </li></ul><h3 id="Interviews"><a href="#Interviews" class="headerlink" title="Interviews"></a><strong>Interviews</strong></h3><p>Qualitative research method.</p><h4 id="Strategy-1"><a href="#Strategy-1" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Write a general interview guide.</li><li>Modify questions if necessary after each interview.</li><li>Explain the purpose, format, length etc. of the interview. Let the participants ask questions before start the interview.</li><li><strong>Never</strong> count on memories to recall the answers. </li><li>Encourage the participants with nodding, etc.</li><li>Careful with reactions (act surprised or take notes suddenly may discourage the participants), tolerate pauses, show you are listening.</li><li>Stay focused on the topic, don‚Äôt lose control of the interview.</li><li>Begin with a warm-up question ‚Äì simple and easy to answer.</li><li>The last question should provide some closure for the interview.</li></ul><h4 id="Rules-1"><a href="#Rules-1" class="headerlink" title="Rules"></a>Rules</h4><ul><li>When there‚Äôre multiple interviewers, make sure to use the same wordings.</li><li>Be careful to ask ‚Äúwhy‚Äù questions. Could be hard to answer or, just, annoying. Ask ‚Äúhow‚Äù instead.</li><li>Never answer a question for the respondent.</li><li>Ask questions that could elicit long answers.</li><li>Don‚Äôt ask about general things and let the respondent answer on behalf of a group. </li><li>Complete the scripts right after the interview. Don‚Äôt wait for a long time.</li></ul><br><br><p><em>Ref:</em></p><p><em><a href="https://www.pewresearch.org/methods/u-s-survey-research/questionnaire-design/" target="_blank" rel="noopener">https://www.pewresearch.org/methods/u-s-survey-research/questionnaire-design/</a></em></p><p><em><a href="http://nixdell.com/classes/HCI-and-Design-Spring-2017/Qualitative-Interview-Design.pdf" target="_blank" rel="noopener">http://nixdell.com/classes/HCI-and-Design-Spring-2017/Qualitative-Interview-Design.pdf</a></em></p><p><em><a href="http://nixdell.com/classes/HCI-and-Design-Spring-2017/interview-strategies.pdf" target="_blank" rel="noopener">http://nixdell.com/classes/HCI-and-Design-Spring-2017/interview-strategies.pdf</a></em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | ËÆæËÆ° </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> notes </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presentation in Freestyle</title>
      <link href="/2019/09/27/presentation-in-freestyle/"/>
      <url>/2019/09/27/presentation-in-freestyle/</url>
      
        <content type="html"><![CDATA[<p>  Today I took part in a workshop related to presentation skills as a Studio session. Personally, when giving presentations, there‚Äôs one thing that has always been difficult for me. Although I could remember all the content (let‚Äôs say I could<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8">üòê</span>) and present in front of people with some kind of confidence, somehow it feels like I‚Äôm just doing a perfect job in reciting or remembering all the gestures instead of really trying to convey or explain something I know about, even if it was created completely by myself. What I want to be is a presenter in freestyle ‚Äì well, like dancing to the music without choreography<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">üòâ</span>.</p><p>  Ambitions:</p><ul><li><p>Actually pay attention to the audience. Don‚Äôt see them as a group (background image).</p></li><li><p>Walk around and use body language naturally, in contrast to imitating a robot. </p></li><li><p>Humorous, at least 80% of what I will show in daily life.</p></li><li><p>Handle different kinds of situations without too much struggle. Like a comedian.</p><p>So, with these ambitions, and also a bit of sleepiness<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">üòí</span>, I went to the workshop and learned:</p></li><li><p><strong>Eye contact</strong> is very important. Also make sure to look at people that not in the center.</p></li><li><p>Must have a strong and clear <strong>ending</strong> (not the kind of ‚Äòthat‚Äôs all‚Äô).</p><p>Ummm‚Ä¶okay, it was a really good workshop, and I definitely learned more than 2 things. The rest is just all combined with my own thoughts, such as:</p></li><li><p>Personally: keep my hands away from my hair. Just try to hold something, like a chair or a pen. (Otherwise I will need a shave to fix this habit<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8">üòè</span>)</p></li><li><p>Don‚Äôt let the audience lose focus for a long period of time(30 seconds perhaps?). Example: write something on the board, deal with slides.</p></li><li><p>Suppose that you could memorize all the content and you just want to practice your presentation style. An interesting way I came up with: present with another language that your audience don‚Äôt understand at all, or just ramble words that make no sense, or remain silence, while <strong>you still have a topic to convey</strong>. Kind of like the game to guess a word. I think this will help you focus on the interaction and connection.</p></li><li><p>Sometimes be completely honest with the audience will help a lot. I mean, suppose you made a mistake during a lecture, if you don‚Äôt have the ability to fix it or act as nothing happened immediately yet, just show them how you are speaking to yourself ‚Äúuh-oh.‚Äù Maybe some non-related explanation: I played video games until 4 this morning so don‚Äôt expect me to know what I‚Äôm talking about. But definitely not too much so you will still be on the content.</p><p>Okay, that‚Äôs pretty much it. I‚Äôm running to do my AML homework now.</p><p>Not a bad article for my first post‚Ä¶I guess?</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Soft Skills | ÁªºÂêàËÉΩÂäõ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> presentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>References and Copyrights</title>
      <link href="/2019/09/24/references-and-copyrights/"/>
      <url>/2019/09/24/references-and-copyrights/</url>
      
        <content type="html"><![CDATA[<h3 id="REGULATIONS"><a href="#REGULATIONS" class="headerlink" title=" REGULATIONS "></a><font color="#7cd175"> REGULATIONS </font></h3><p>Similar to academic integrity, <font color="#b39ddb"><strong><em>DoveCode</em></strong></font> respects and protects all kinds of intellectual property rights. Most content in this website follows <em>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</em>, and any exception will be stated. </p><p>Reprint/reproduction is welcomed as long as under proper citation. If you would like to ‚Äúclone‚Äù something but couldn‚Äôt get the access to it, please contact <a href="https://FadingWinds.me/about" target="_blank" rel="noopener">@FadingWinds</a> and I would love to help. Also, it is very encouraged to point out any mistakes or missing points you notice. </p><h3 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title=" REFERENCES "></a><font color="#7cd175"> REFERENCES </font></h3><ol><li><p><font color="#b39ddb"><strong><em>DoveCode</em></strong></font> is based on <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> and uses <a href="https://github.com/blinkfox/hexo-theme-matery/" target="_blank" rel="noopener">hexo-theme-matery</a> as the theme. Currently all the functions and plugins are derived from them, so references like these won‚Äôt be listed here right now. Please visit their link to see the details.</p></li><li><p>The banner picture comes from <strong>Cytus II</strong> (A game produce by <a href="https://www.rayark.com/" target="_blank" rel="noopener">Rayark</a>). </p> <br> <div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzdf/Banner.png" width="50%" height="50%" title="Banner Picture"></div></li><li><p>Sorry it was unable for me to trace the resource of the website‚Äôs logo (which is also my avatar image on Github). I would be grateful if anyone happens to know and could provide it to me. </p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzhf/logo.png" width="10%" title="Logo"></div></li><li><p>The icon of <strong>*<font color="#b39ddb">DoveCode</font>*</strong> comes from <a href="https://icons8.com/icons" target="_blank" rel="noopener">Icons8</a>.</p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzlf/favicon.png" title="Icon"></div></li></ol><h3 id="COPYRIGHTS"><a href="#COPYRIGHTS" class="headerlink" title=" COPYRIGHTS "></a><font color="#7cd175"> COPYRIGHTS </font></h3><ol><li><p>All the posts are original on <strong>*<font color="#b39ddb">DoveCode</font>*</strong> and any quotes/references included will be listed in or at the end of the article.</p></li><li><p>There are a set of cover images that will be randomly assigned to an article if it doesn‚Äôt have a specific one. Anyone is welcomed to submit their own photographs. The current set is as follows.</p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxNjM1NTdf/photos.png" title="Photographed by FadingWinds"></div><br><div align="center"><img src="https://od.lk/s/MzBfMTc0NzM0OTZf/Em..png" title="Photographed by Emmanuel Li"></div></li><li><p>The avatar image on the ‚ÄúAbout‚Äù page is drawn by <strong><em>FadingWinds</em></strong>. </p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwODFf/NeverAnAngel.jpg" width="30%" title="Drawn by FadingWinds"></div></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
