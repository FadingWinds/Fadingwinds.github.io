<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark: Now It&#39;s Real Big Data</title>
      <link href="/2021/03/09/data-science-shu-ju-ke-xue/spark-now-it-s-real-big-data/"/>
      <url>/2021/03/09/data-science-shu-ju-ke-xue/spark-now-it-s-real-big-data/</url>
      
        <content type="html"><![CDATA[<p>Ever since I started to study information systems / information science, the term "big data" has long been used to explain what my major is doing, but the actual entity it represents is hardly ever reachable, at least to me.</p><p>Now I'm learning Spark, and hopefully after this I don't have to feel guilty for saying "big data" as if it's something I know more than "that's what Excel couldn't store" (a professor once said this during a class) any more.</p><p><em>Most notes are based on Udemy course: <img src="https://www.udemy.com/course/apache-spark-with-scala-hands-on-with-big-data/" alt="Apache Spark with Scala: Hands on with Big Data"></em></p><h3 id="overview">Overview</h3><ul><li>Spark is 100x faster in memory and 10x faster on disk compared with Hadoop MapReduce because of its DAG (directed acyclic graph) engine. Still, Spark and Hadoop can co-exist and not necessarily a substitute for each other.</li><li>Using Spark Datasets and DataFrames is similar to using SQL.</li><li>Why Scala?<ul><li>Spark is written in Scala.</li><li>Scala forces you to use functional programming (a good fit for distributed processing).</li><li>(Now only a little) faster than Python, more simple than Java.</li></ul></li><li>Spark 3 can integrate with TensorFlow to do deep learning.</li><li>Scala runs on JVM.</li></ul><h3 id="scala-basics">Scala Basics</h3><h4 id="syntax">Syntax</h4><p><em>Just refer to the file <code>LearningScala*.sc</code>(</em> = {1 ,2 ,3, 4}) and you should be fine.*</p><ul><li><code>val</code>s are immutable, in contrast to <code>var</code>s. Using immutable constants as much as possible to achieve thread safety and avoid race conditions.</li><li>Different prefix (e.g. "s") can allow you to e.g. use variables and expressions in the <code>println("")</code> string.</li><li>Regular expression: use triple parentheses and add <code>.r</code> at the end.</li><li>No ";" in Scala!</li><li><code>_</code> is a catch-all symbol in "match-case" expressions.</li><li>For loop in Scala is not like Java or Python...or even a mix of them.</li><li><strong>Having an expression in Scala will give an output of the result of the last thing.</strong></li><li>Functions syntax in Scala is similar to Python (sort of), with curly brackets and an extra <code>=</code>.</li><li><strong>Indexing</strong><ul><li><strong>Tuple index starts with 1, writen as <code>tuple._1</code> for getting the first element.</strong></li><li><strong>List index starts with 0: <code>list(0)</code></strong></li></ul></li><li><code>list.head</code> gives the first element, but <strong><code>list.tail</code> gives all the remaining elements except the first one</strong>.</li><li>Concatenate list: <code>listA ++ listB</code>.</li><li>Add element to list: <code>x :: list</code>.</li><li><strong>If there's no <code>return</code> in the function, the last expression will be taken as the return value.</strong></li><li><code>object</code> is like a <code>class</code>, but there's also a <code>case class</code> which used in DataSets to define a schema first.</li></ul><h4 id="functional-programming">Functional Programming</h4><p>Similar to what I learned on <em>Machine Learning Engineering</em> class, functional programming in Scala looks like this:</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def transformInt(x: Int, f: Int =&gt; Int) Int = {f(x)}</span><br></pre></td></tr></tbody></table></figure><h4 id="lambda-expression">Lambda Expression</h4><p>Similar to that in Java and Python:</p><p><code>transformInt(3, x =&gt; x * x * x)</code></p><p>Or:</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val ex1 = (x:Int) =&gt; x + 2</span><br><span class="line">println(ex1(7)) \\ output: 9</span><br></pre></td></tr></tbody></table></figure><h4 id="data-structures">Data Structures</h4><ul><li>Tuples can have different data typles, but lists can't.</li><li>Functions like Map, Reduce, Filter are good for being parallelizable. (Spark has its own Map, Reduce Filter other than Scala's)</li><li>Maps: dictionaries in Scala. A special function is <code>utils.Try(map("x")) getOrElse "Unknown"</code></li><li>In Java terms, Scala's <code>Seq</code> would be Java's <code>List</code>, and Scala's <code>List</code> would be Java's <code>LinkedList</code>. (Ref: <img src="https://stackoverflow.com/questions/10866639/difference-between-a-seq-and-a-list-in-scala" alt="Stack Overflow">)</li></ul><h4 id="built-in-functions">Built-in Functions</h4><ul><li>String related functions, e.g. <code>string.toUpperCase</code>, <code>string.reverse</code></li><li>Reduce: <code>val sum = numberList.reduce( (x: Int, y: Int) =&gt; x + y)</code></li><li>Filter: <code>val iHateThrees = numberList.filter(_ != 3)</code></li><li>List related functions: <code>list.reverse</code>, <code>list.sorted</code>, <code>list.max</code>, <code>list.sum</code>, <code>list.distinct</code>(producing a set), <code>list.contains()</code>(returns a boolean)</li></ul><h3 id="rdd-resilient-distributed-dataset">RDD (Resilient Distributed Dataset)</h3><p>RDD is a lower level API. Seems like it's getting less popular.</p><p>Rows of data can be divided and sent to different computer to process in parallel.</p><p>RDDs can be created from text file, hadoop(hdfs), hive context, sql commands, etc.</p><p>Transforming RDDs:</p><ul><li>map and flatMap (1 <strong>row</strong> to multiple <strong>rows</strong>)</li><li>filter</li><li>distinct</li><li>sample</li><li>union, intersection, subtract, cartesian</li></ul><p>RDD common actions:</p><ul><li>Collect</li><li>Count</li><li>countByValue</li><li>take</li><li>top</li><li>reduce</li></ul><p><strong>Nothing actually happens in your driver program until an action is called.</strong> <del>Which can lead to tricky debuggings</del>:raised_eyebrow:.</p><p>SparkContext(from Doc): Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.</p><p>Only one SparkContext should be active per JVM.</p><p>Process: Job -&gt; Stages (reorganized data will be a new stage) -&gt; Tasks (distributed across cluster, scheduled and executed)</p><p><strong>Key/Value RDD Specific Functions</strong></p><ul><li>reduceByKey(): the (x, y) in example "FriendsByAge" is actually representing two rows (entries).</li><li>groupByKey()</li><li>sortByKey()</li><li>keys(), values()</li><li>SQL style joins</li><li>mapValues() / flatMapValues()</li></ul><h3 id="sparksql">SparkSQL</h3><p>DataFrames:</p><ul><li>Contains raw objects</li><li>Can run SQL queries</li><li>Has a schema</li><li>R/W to JSON, Hive, etc.</li><li>Communicates with JDBC/ODBC, Tableau</li></ul><p>Technically, a DataFrame is just a DataSet with raw objects. DataSets wraps a given struct or type. DataSets will know its schema at compile time (Python can't do this, only have DataFrames).</p><p>RDD can be converted to DataSets and vice versa.</p><p>When using SparkSQL, create SparkSession instead of SparkContext, and need to stop the session when finish.</p><p>DataSets have similar functions to SQL queries in which it doesn't need a database view to execute. (Personally I think it's easier to use than passing SQL commands, ~~definitely not because I've forgotten so much about SQL, ~~ except for the weird syntax <code>=!=</code> and <code>===</code> in <code>.filter()</code>)</p><p>DataSets work best with structured data.</p><p>For loading unstructured data, the default column name should be set to "value" for the schema to be inferred.</p><p>*Default <code>show()</code> will display 20 rows and truncate. Use <code>show(NUM OF ROWS, false)</code>.</p><p>For files with no headers, schema can't be inferred so a new schema need to be defined first: <code>val schema = new StructType.add("COLUMN NAME", TYPE)</code></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Data Science · 数据科学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> Python </tag>
            
            <tag> Udemy </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Little Abigail</title>
      <link href="/2021/03/07/zhi-yan-pian-yu/little-abigail/"/>
      <url>/2021/03/07/zhi-yan-pian-yu/little-abigail/</url>
      
        <content type="html"><![CDATA[<p>Little Abigail, just a bit younger</p><p>Silently floating without a feather</p><p>How did you even let her get in your space</p><p>Don't you know she belongs to another place</p><p><br></p><p>You always stare, into those eyes</p><p>But they won't give away</p><p>What's on her mind</p><p>Little Abigail, is she doomed to fail</p><p>To let her slow pace make you left her behind</p><p><br></p><p>One piece of existence</p><p>Expected as assistance</p><p>Present for distance</p><p>Excuse for disappearance</p><p>Sent to your hand now</p><p>Abigail's flower</p><p>If somehow</p><p>This world still needs her little power</p><p><br></p><p>She will send away the darkness</p><p>On a fireless night</p><p>Hounds and spiders are no longer your threat</p><p>She'd take a deerclops if it dares to attack</p><p>Please do not fret</p><p>If she's out of your sight</p><p>It's just she always gets another battle to fight</p><p>Soon before you know, she will come back</p><p>Never miss a day to protect, not in this life</p><p><br></p><p>Little Abigail, dear sister</p><p>Ongoing time heals or generates sorrow</p><p>Vermilion petals preserve her smile no matter</p><p>Everywhere and every tomorrow</p><p><br> <br></p><p><em>Banner Ref: "Don't Starve: Wendy" by Bielek on DevianArt</em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 只言片语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Poem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Chain of Blocks</title>
      <link href="/2020/08/17/blockchain-qu-kuai-lian/the-chain-of-blocks/"/>
      <url>/2020/08/17/blockchain-qu-kuai-lian/the-chain-of-blocks/</url>
      
        <content type="html"><![CDATA[<p>**A slightly different writing style for notes. Somehow it just feels right for Blockchain.*</p><h3 id="chronicle">Chronicle</h3><p>The year was 2009, when BitCoin appears, also known as 1 A.B.</p><p>But the story didn't just start then. It could actually trace all the way back to 32 B.B. - 1977.</p><p>RSA =&gt; Byzantine Generals problem =&gt; Proof of Work</p><p>In an economic story, there are two flavors of <em>financial instrument</em> existing in the world.</p><ol type="1"><li><p><strong>Bearer Instrument</strong></p><p>Physical holder of object (or secret) possesses asset, e.g. cash, bearer bond.</p><p>Looser definition includes holder of secret, e.g. Swiss bank account (well, once upon a time).</p><p>Pros: Easy to transfer and anonymous; Cons: Easy to lose</p></li><li><p><strong>Registered instrument</strong></p><p>Asset with centralized record of ownership, e.g. most types of shares or bonds.</p><p>Pros: Can’t (physically) lose it; Cons: Not anonymous; hard to transfer, unless register is digital</p></li></ol><p>Both types of instrument have a venerable history.</p><p>What is money? <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f4b5.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4b5.png?v8">💵</span></p><p>Four key ingredients for bearer version, mechanisms for:</p><ol type="1"><li>Creation</li><li>Forgery prevention</li><li>Verification of validity / ownership</li><li>Transfer between parties</li></ol><h4 id="a-little-history-about-money">A little history about money</h4><p><em>The Lost Sheep Problem</em></p><p>Suppose you deposited some sheep in the communal herd...how to ensure no one forgets your sheep, or falsely claims you didn’t deposit them?</p><p>A solution: To keep track of goods, clay accountancy tokens were used, could be kept in a safe place to keep track of ownership</p><p>More cleverness: How to protect against tampering with or theft of tokens?</p><p>Tokens sealed in clay envelopes. If in doubt, envelope could be broken open, but only once (<em>Security Protocol</em>).</p><p>Some challenging security goals:</p><ul><li>Only trusted authority should be able to create tokens.</li><li>Authenticity should be verifiable by anyone.</li></ul><p>For <em>forgery prevention</em>, coinage usually relied on three things:</p><ul><li>Tokens made from scarce resource</li><li>Sign / signature that was hard to duplicate</li><li>(Death penalty for forgers didn’t hurt)</li></ul><h4 id="with-regard-to-bitcoin...-moneybag">With regard to Bitcoin... <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f4b0.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4b0.png?v8">💰</span></h4><p>Scarce resource: <strong>computation</strong></p><p>Hard-to-forge data: <strong>cryptography</strong></p><p>It's a <strong>combination</strong> of bearer instruments and registered instruments.</p><ul><li>Public record of asset ownership called the <strong>blockchain</strong></li><li><strong>Secret keys</strong> for control of individual's assets</li><li>Quasi-anonymous (pseudonymous) (like Swiss Bank)</li><li>Loose comparisons to Yap currency (stones), etc., but no perfect physical analog</li></ul><p>Where did Bitcoin / blockchains come from?</p><p>Created by "Satoshi Nakamoto", but the real identity remains a secret.</p><h4 id="blockchain-abstraction">Blockchain Abstraction</h4><ol type="1"><li>Strict ordering of messages</li><li>Rule-based write, global read</li><li>No message modification</li></ol><p><strong>Compare: Execution, clearing, and settlement</strong></p><p>Traditional:</p><ul><li>For transfer of financial instruments</li><li>Up to three days to complete (T+3)</li><li>Many middlemen</li><li>Fragmented records</li><li>Difficult to audit</li></ul><p>Blockchains are much faster and transparent.</p><h3 id="bitcoin-basics">Bitcoin Basics</h3><h4 id="transaction-authentication">Transaction authentication</h4><p>Passwords represent tradeoff</p><ul><li>Convenient but vulnerable</li><li>Require a central to manage them</li></ul><p>Alternative: Digital signatures</p><ul><li>Much stronger security</li><li>The central can verify that I authorized a transaction without knowing my secrets</li></ul><h4 id="digital-signature">Digital Signature</h4><p>Key Gen =&gt; Private key (SK) / Public key (PK)</p><p>(M, sig) + PK =&gt; Verify</p><p>Anyone can run software that executes <strong>KeyGen, Sign, or Verify</strong>, so:</p><ul><li>Any entity X can generate unique keypair (<span class="math inline">SK_X</span>, <span class="math inline">PK_X</span>)</li><li><span class="math inline">X</span> can sign using private key <span class="math inline">SK_X</span></li><li>Anyone can verify <span class="math inline">X</span>’s signatures against <span class="math inline">PK_X</span></li><li>But PKX does not contain <span class="math inline">X</span>’s real-world identity</li></ul><p>Bitcoin uses <strong>ECDSA</strong> (Elliptic-Curve Digital Signature Algorithm). Private key SK is 256 bits (32 bytes); (compressed) public key PK is 33 bytes.</p><p>(More details in the later section.)</p><h4 id="cryptocurrency-case">Cryptocurrency Case</h4><p>For every account X, sign transactions using private key <span class="math inline">SK_X</span>; Assume all public keys <span class="math inline">PK_X</span> known to the world.</p><p>Pros:</p><ul><li>No passwords to steal</li><li>Universal access</li><li>Fast transactions</li><li>Central can’t falsify transactions</li></ul><p>Cons:</p><ul><li>Users need to manage private keys (Can’t store in head like password)</li><li>Central needs to manage public keys</li><li>Partial privacy: Pseudonymity only</li><li>Central can still cheat<ul><li>Can fail to process / post transactions</li><li>Can go back and erase transactions</li><li>Can show different subsets of transactions to different users</li></ul></li></ul><p>Private key SK = ownership leakage = theft</p><p>Improvement:</p><ul><li>Central periodically (every 10 minutes) digitally signs batches of transactions (linked to old batches)</li><li>Central publishes batch for users to see and record</li></ul><p>In this way,</p><ul><li>Batches of transactions signed with <span class="math inline">SK_{Central}</span></li><li>Present different batches/delete transactions will let central be caught</li></ul><p>Ledger now constructed as <strong>a chain of blocks</strong>.</p><p>A block is <strong>a batch of transactions + a digital signature</strong>.</p><p>Whole chain contains all transactions over time.</p><p>Specifies complete system history.</p><p>BUT central can still <strong>suppress transactions</strong>:</p><ul><li>Refuse to process / post transactions</li><li>If central cheats, there's no recourse (You catch it cheating, not much you can do)</li><li>Trust resides in a single entity</li></ul><p>Why not trust a central authority? (e.g. banks) -- Hyperinflation, Frozen assets</p><h4 id="consensus">Consensus</h4><p>For Bitcoin, similarly,</p><ul><li>Pseudonymous: User identities correspond to (SK, PK) key pairs</li><li>Users digitally sign transactions to authorize movement of money</li><li>Transactions recorded in blockchain</li></ul><p>And differently,</p><ul><li>The blockchain is fully decentralized</li><li>Instead of one trusted entity, we rely on whole community</li></ul><p>the problem of <strong>consensus</strong>: How does community agree on what transactions in ledger?</p><p>Naïve consensus idea: Everyone broadcasts to everyone else</p><p><em>Problems</em>: Won’t work with thousands or millions of participants (not always online); And even if it did, messages delivery times may vary -- ordering could affect validity</p><p>Improve: Vote</p><p><em>Problems</em>: vote early, vote often (Sybil Problem)</p><p>Solution: <strong>Bitcoin Mining</strong></p><ul><li>In Bitcoin, blocks of transactions made authoritative by <strong>mining</strong></li><li>Anyone in community can be a miner</li><li>Miners all race to find community signature on current, ordered batch of transactions</li><li>Community signature discovered via <strong>computationally costly process</strong> (mining)</li><li>(No private key for this signature type)</li><li>Block then published with signature</li></ul><h4 id="mining">Mining</h4><p>Hash function (details in the next section)</p><p>To mine block, miner keeps trying different values of <span class="math inline">N</span> <strong>until <span class="math inline">H(B||N)</span> has <span class="math inline">k</span> leading zeros</strong>.</p><ul><li><span class="math inline">B</span> block data including fresh transactions</li><li><span class="math inline">N</span> is a nonce</li><li><span class="math inline">k</span> is set by system</li><li>Lots of 0s = lots of work!</li><li>The faster your machine, the higher the probability that you get lucky and are the first to mine a block</li></ul><p>To achieve decentralized / community signature (without identities or trusted hardware), this is <strong>only known, fully battle-tested procedure</strong>.</p><p>aka. <strong>Proof of Work</strong></p><h4 id="acquire-and-spend">Acquire and Spend</h4><ul><li>Anyone can create her own "address" / account X<ul><li>Digital wallet (app) can do this for you</li><li>Creates cryptographic "key pair" (<span class="math inline">SK_X</span>, <span class="math inline">PK_X</span>)</li></ul></li><li>Secret key <span class="math inline">SK_X</span>: to authorize use of your Bitcoin</li><li>Public key <span class="math inline">PK_X</span>: public identifier and to validate transactions</li><li>Address comes from public key</li></ul><p>Transaction lifecycle:</p><ol type="1"><li>You scan QR code -- get address A of receiver.</li><li>You choose transfer amount.</li><li>Your wallet uses your private key (SK) to digitally sign transaction.</li><li>Your wallet broadcasts transaction to cryptocurrency network / miners (I'm sending X coins to A).</li><li>Miners pick up your transaction.</li><li>A successful miner includes your transaction in a block.</li><li>Your transaction is on the blockchain!</li><li>The whole world knows you sent X coins to address A.</li></ol><h3 id="hash-function">Hash Function</h3><p>A deterministic cryptographic function.</p><p><span class="math inline">H</span> takes as input any desired bitstring / text B, outputs a random(-looking) fixed-size (256-bit) value digest <span class="math inline">H(B)</span>.</p><p><em>SHA-256^2 used in Bitcoin.</em></p><p>Same input <span class="math inline">B</span> always produces same output, and any <span class="math inline">B' ≠ B</span> produces completely different, random-looking output <span class="math inline">H(B')</span>.</p><p>Think of it as: A unique "fingerprint" of message B, and a very lossy compression of message B.</p><p>Common cryptographic hash function: MD5, SHA-1, SHA-256 (n = 256)</p><p>Two <strong>key security properties</strong> for a cryptographic hash function <span class="math inline">H</span>:</p><ol type="1"><li><strong>preimage resistance</strong></li></ol><ul><li>Image is any n-bit value <span class="math inline">y</span></li><li>Given image <span class="math inline">y</span>, a preimage is any <span class="math inline">x</span> s.t. <span class="math inline">H(x) = y</span></li><li><strong>Given random <span class="math inline">y</span> (uniform over <span class="math inline">\{0,1\}^n</span>), it's infeasible to find image <span class="math inline">x</span></strong>.</li></ul><p>Note, though, that for at least one image, there are infinitely many preimages. (?)</p><ol start="2" type="1"><li><strong>Collision-resistance</strong></li></ol><p>It is <strong>hard</strong> to find any pair of inputs <span class="math inline">(w,x)</span> such that <span class="math inline">H(w) = H(x) = y</span>.</p><h4 id="random-oracle-model">Random Oracle Model</h4><p>An ideal hash function case for conceptual simplicity, mathematically rigorous but simple proofs of security and understanding how to use hash functions.</p><p><em>Question: Birthday Paradox</em> <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f382.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f382.png?v8">🎂</span></p><p>There are <span class="math inline">N</span> (=365) days in an (ordinary) year. Suppose there are <span class="math inline">k</span> people in the room. Assume (uniformly) randomly distributed birthdays. How large must <span class="math inline">k</span> be for it to be likely (prob. ≥1/2) that two people share a birthday?</p><details><p></p><summary>Click to see the answer</summary><p></p><p>Prob ≈ 50.7% for k = 23.</p><p>For k = 120 ≈ 99.9999999+% chance.</p>For k = 120 ≈ 83% chance of three-way collision.</details><p>Birthday paradox illuminates collision-resistance.</p><p><strong>General rule of thumb in cryptography</strong>: 128-bit security, meaning <span class="math inline">2^{128}</span> work for attacker, is "strong"</p><h4 id="blockchain-applications-in-hashing">Blockchain Applications in Hashing</h4><ol type="1"><li><p>Addresses</p><p>Public key (PK) in Bitcoin: 256-bit value</p><p>Address in Bitcoin: base-58 string</p><p>Address is hash of PK (plus other stuff)</p></li><li><p>Script hashing (?)</p><p>P2SH-type transaction (Pay-to-Script-Hash)</p><p>Special transaction: payment sent to "script hash" ('3' address), H(S) for "redeem script" S</p><p>Redeem script S included in spending transaction</p></li><li><p>Transaction IDs</p><p>Hash of transaction data is transaction identifier (TXID)</p></li><li><p><strong>Chaining Blockchain Blocks</strong></p><p>Block header includes hash of previous block</p></li><li><p>Mining</p><p>Special-purpose hardware is blazingly fast! ASIC (Application-Specific Integrated Circuit)</p><p>Instead: <strong>Memory-hard hash functions</strong> (Make computation of H gobble up lots of fast memory)</p><p>e.g. scrypt, Argon2, Balloon Hashing</p><p>Meant to be ASIC-resistant (but with mixed success)</p></li><li><p>Commitment schemes</p><p><strong>Commitment</strong>:</p><p>Computational hiding.</p><p>Suppose Alice (welcome our first character) chooses short message <span class="math inline">m</span>, e.g. <span class="math inline">m \in {0,1}</span>,Alice gives us <span class="math inline">C = H(m)</span>.</p><p>Easy to compute m from <span class="math inline">C = H(m)</span>, like brute-force password cracking -- <span class="math inline">C = H(0) or H(1)</span>?</p><p>Can Alice somehow use <span class="math inline">H</span> to hide <span class="math inline">m</span>?</p><p>Answer: Alice chooses random, secret key <span class="math inline">r</span> and gives us <span class="math inline">C = H(m||r)</span>, without <span class="math inline">r</span></p><p>Collision resistance forbids Alice to cheats (change <span class="math inline">m</span>)</p><p>A good commitment scheme is:</p><ul><li>Efficient: easy to compute <span class="math inline">C</span></li><li>Hiding: hard to compute <span class="math inline">m</span> from <span class="math inline">C</span></li><li>Binding: hard to change <span class="math inline">m</span> for given commitment <span class="math inline">C</span></li></ul></li><li><p>Blockchain inclusion proofs</p></li></ol><p>Use short “Root” to prove existence of a transaction: <strong>Merkle Trees</strong></p><h4 id="coin-flip-protocol">Coin-flip Protocol</h4><p>In many blockchain protocols, "committee" must choose random number.</p><p>Goal: Alice, Bob, and Charlie together generate a truly random bit <span class="math inline">b</span>, assume at least one of them is honest</p><ul><li>Generate respective random bits <span class="math inline">b_A</span>, <span class="math inline">b_B</span>, <span class="math inline">b_C</span>.</li><li>Generate commitments <span class="math inline">c_A</span>, <span class="math inline">c_B</span>, and <span class="math inline">c_C</span> to their respective bits</li><li>Post their commitments to the blockchain</li><li>Compute <span class="math inline">b</span> = <span class="math inline">b_A</span> XOR <span class="math inline">b_B</span> XOR <span class="math inline">b_C</span></li></ul><p>Deadline problem: the last person waits and decides whether to decommit</p><h3 id="basic-crypto">Basic Crypto</h3><h4 id="public-key-cryptography">Public Key Cryptography</h4><p>Symmetric-key encryption</p><p>Symmetric = Classical (intuitive) encryption</p><p>Alice =&gt; Bob with <span class="math inline">C=enc_K[M]</span>, where <span class="math inline">K</span> is a secret</p><p>Main challenge: How to share K safely to begin with?</p><p>Unfortunately, meeting under bridge <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f608.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f608.png?v8">😈</span> <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f309.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f309.png?v8">🌉</span> at night not scalable for Internet</p><p>When logging on website, we have a secure channel to send things -- User and website somehow manage to choose and share a random, secret key K, despite:</p><ul><li>No prior communication about K</li><li>Eavesdropper or malicious entity intercepting their communication</li></ul><p><strong>Merkle puzzles</strong></p><p>Randomly selected secret keys and indices.</p><p>Alice sends =&gt; <span class="math inline">P_1, P_2, ..., P_n</span> where <span class="math inline">P_j = (k_j, i_j)</span></p><p>Bob returns =&gt; <span class="math inline">i_r</span></p><ul><li><span class="math inline">(k_j, i_j)</span> are embedded in puzzle</li><li>Constructing puzzle is easy (O(1) work)</li><li>Solving puzzle is made moderately hard: brute force decryption requires O(m) work</li><li>The evil Eve doesn’t know <strong>which puzzle</strong> Bob chose -- so she has to keep solving until she finds it.</li></ul><p>Observe that:</p><ul><li>Alice does work O(n)</li><li>Bob does work O(m)</li><li>Eve must do work O(n × m) (much harder than Alice and Bob)</li></ul><h4 id="diffie-hellman-key-agreement">Diffie-Hellman key agreement</h4><p>First practical public-key cryptosystem, and the simplest.</p><p>Scenario:</p><p>Alice and Bob want to share a secret key <span class="math inline">K</span>, but:</p><ul><li>They've never met.</li><li>They don’t want Eve, who’s eavesdropping, to learn <span class="math inline">K</span>.</li></ul><p><strong>Discrete log problem</strong></p><p>Given a group <span class="math inline">G</span> of order <span class="math inline">q</span> and the pair <span class="math inline">(g,y)</span>, where <span class="math inline">g</span> is a generator of G and <span class="math inline">y = g^x</span> for random <span class="math inline">x \in [0,q-1]</span>, compute <span class="math inline">x = log_gy</span> .</p><p>Typical choices of G:</p><ul><li><span class="math inline">p = 2q+1</span>, for primes <span class="math inline">p</span> and <span class="math inline">q</span> (or <span class="math inline">q ∣ p - 1</span>)</li><li>Computation is performed mod <span class="math inline">p</span></li><li><span class="math inline">g</span> generates cyclic subgroup <span class="math inline">G</span> of order <span class="math inline">q</span></li><li>So Alice’s public-key is <span class="math inline">A = g^a \space mod \space p</span>, for <span class="math inline">a \in R [0,q-1]</span></li><li>Typical parameter choices: <span class="math inline">p</span> is a 2048-bit prime, <span class="math inline">q</span> is a 256-bit value</li></ul><p>Random values in exponent space are hidden, e.g. <span class="math inline">x</span> is hidden in <span class="math inline">y=g^x</span>. So we can "compute secretly" in the exponent space. (now omit mod p for visual clarity)</p><p>(Simplified) DH Key Agreement:</p><ol type="1"><li><p>Key Generation</p><p>Alice - Random private key <span class="math inline">a</span> and public key <span class="math inline">A=g^a</span></p><p>Bob - Random private key <span class="math inline">b</span> and public key <span class="math inline">B=g^b</span></p></li><li><p>Public Key Exchange</p></li><li><p>Compute Secret, Shared Key</p><p>Alice - <span class="math inline">K' = B^a = (g^b)^a = g^{ba} = g^{ab}</span></p><p>Bob - <span class="math inline">K' = A^b = (g^a)^b = g^{ab}</span></p></li></ol><p>Why can't Eve learn about <span class="math inline">K</span>?</p><ul><li>Values <span class="math inline">a</span> and <span class="math inline">b</span> are in exponent space, so they remain hidden.</li><li>Alice can multiply hidden value <span class="math inline">b</span> by known value <span class="math inline">a</span>; vice versa for Bob.</li><li>Eve doesn’t know <span class="math inline">a</span> or <span class="math inline">b</span>, can't do secret multiplication (DH assumption).</li><li>Eve can only compute, e.g. AB = <span class="math inline">g^ag^b</span> = <span class="math inline">g^{a+b}</span>.</li><li><span class="math inline">g^{ab}</span> is hashed to obtain symmetric key, e.g., AES key</li></ul><p><strong>RSA Encryption</strong>: Uses modular exponentiation (like D-H); Security related to hardness of factoring -- Given <span class="math inline">pq</span> for large primes <span class="math inline">p</span> and <span class="math inline">q</span>, compute <span class="math inline">p</span> and <span class="math inline">q</span></p><p>How hard it is to break RSA?</p><p>• Best known general attack involves factoring <span class="math inline">N = pq</span> • Difficulty of best classical factoring algorithm (general number field sieve) grows super-polynomially (but sub-exponentially)</p><p>In fact...encryption uses <em>nowhere</em> in Bitcoin or basic Ethereum <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f643.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f643.png?v8">🙃</span>. But it's important to know it because:</p><ul><li>Important background for digital signatures</li><li>Important <strong>contrast</strong> with digital signatures</li><li>Used in advanced blockchain protocols</li></ul><h4 id="digital-signatures">Digital Signatures</h4><p>A metaphor story:</p><p>You have a special signet ring (rubber stamp ring? <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">😉</span>), everyone knows what your signature looks like (PK) but only the ring can produce it (SK).</p><p>Security property for digital signatures: Should <em>only</em> be possible to sign using SK, even though Alice's public key PK is published, i.e., known to world and anyone can verify using PK.</p><p>In encryption, public key used to <strong>initiate algorithm</strong> ( Encrypt under public key), with digital signatures, private key used to initiate algorithm (Sign using private key) -- <strong>"Opposite" workflows</strong></p><h4 id="schnorr-identification">Schnorr Identification</h4><p>A <strong>interactive signature scheme</strong>:</p><p>Goal: Alice identifies herself to Bob by proving knowledge of her private key.</p><p>Assume Alice's SK/PK: <span class="math inline">(a,A=g^a)</span> and Bob has <span class="math inline">A</span></p><ul><li>Bob sends Alice <span class="math inline">c</span></li><li>Alice replies with <span class="math inline">s=ca+r</span></li><li>Bob checks <span class="math inline">g^{s} == RA^c</span></li></ul><p>Intuition:</p><ul><li>Bob can verify that <span class="math inline">a</span> is properly "mixed in" -- so it’s really Alice.</li><li>Alice removes <span class="math inline">a</span> from exponent space, reveals it in <span class="math inline">s</span>, but, <span class="math inline">r</span> is a one-time value that blinds, i.e., conceals <span class="math inline">a</span>.</li></ul><p>Problems:</p><ul><li>Requires interaction</li><li>Bob can't prove to another person that Alice "signed" <span class="math inline">c</span></li></ul><p><strong>Schnorr signature scheme</strong>:</p><p>In addition to above,</p><ul><li>Alice has a <strong>one-time</strong> (if not, look for "Sony PS3 Break" accident) private key <span class="math inline">r</span> and public key <span class="math inline">R = g^r</span>. <span class="math inline">C=H(R,m)</span></li><li>Signature: <span class="math inline">(R, s = ca + r)</span></li><li>Verify: <span class="math inline">g^s == RA^c</span></li><li>Key point: generate "challenge" c from m</li></ul><h4 id="ecdsa">ECDSA</h4><p>Operates over subgroup on <strong>elliptic curve</strong></p><ul><li>Subgroup of order <span class="math inline">n</span> generated by base point <span class="math inline">g</span></li><li>Public / private key pair: <span class="math inline">(g^a, a)</span></li><li>To sign <span class="math inline">m</span>, compute <span class="math inline">e = H(m)</span></li><li>Truncate <span class="math inline">e</span> to <span class="math inline">⌈log2 n⌉</span> bits (bit length of subgroup order)</li><li>k ⇐$ [1,n-1]</li><li><span class="math inline">(x,y) = g^k; x' = x</span> mod <span class="math inline">n</span> • <span class="math inline">s = k^1(e + x'a)</span> mod <span class="math inline">n</span> • Signature is <span class="math inline">(r,s)</span></li></ul><h3 id="utxos">UTXOs</h3><p>Unspent Transaction Outputs.</p><p>The (intuitive) account model (used in Ethereum):</p><p><em>Block n</em>: Starting balances + TX (transactions) =&gt; <em>Block n+1</em>: New balances</p><p>*Addresses are actually for one-time use (more about this later)</p><p>But in Bitcoin... <strong>No explicit balances, only a set of transactions.</strong> Circulating money consists of Unspent Transaction Outputs (UTXOs).</p><p>A transaction contains <strong>IN</strong> and <strong>OUT</strong>, IN = OUT</p><p>Alice recovers unspent value by giving herself a partial refund.</p><p>What needs to be shown in [in] to prove legitimate use of [out]? [in] must: - Include "unlocking code" - With valid signature <span class="math inline">sig</span> for Transaction under <span class="math inline">SK_{Alice}</span></p><p>Complicated scripts are usually P2SH (Pay to Script Hash).</p><p>Bitcoin scripting language not Turing complete; but still many possible scripts, e.g.</p><ol type="1"><li><p>Multisig</p><p>k-out-of-N sig -- Why not N-out-of-N?</p><ul><li>Ensure no one steals money</li><li>Ensure collective agreement on which music videos to fund (or anything else in other cases)</li><li>And protect against loss of one key</li></ul><p>Application: Joint control, Escrow,</p><p>Case: Alice sells Bob a Lambo. Suppose Carol is a trusted third party, and can verify delivery of Lambo. To ensure that</p><ul><li>If Alice and Bob agree, Carol isn’t bothered.</li><li>If there’s a dispute, Carol can make sure money goes to right person.</li></ul><details><p></p><summary>Click to see solution</summary><p></p><p>Bob pays into 2-out-of-3 multisig, if Lambo not delivered or Bob cheats, Carol and honest player direct money.</p></details></li><li><p>Payment channels</p><p>Problem: On-chain Bitcoin transactions are expensive and slow</p><p>Solution: Make Bitcoin payments (mostly) without using blockchain (:raised_eyebrow:) -- Mechanism called payment channel</p><p>Main implementation: Lightning network</p><p>Case: Alice wants to make multiple small payments to Bob (e.g. subscription) and prefunds channel</p></li><li><p>Cross-chain atomic swaps</p></li><li><p>Zero-Knowledge Contingent Payments</p></li></ol><h3 id="byzantine-agreement">Byzantine Agreement</h3><p>The third person can't tell who's cheating (he said, she said).</p><h4 id="synchronous-ba">Synchronous BA</h4><p>There does not exist an n-party <span class="math inline">(n &gt; 3)</span> Synchronous Byzantine Agreement protocol <span class="math inline">\Pi</span> that is <span class="math inline">(t;\epsilon)</span>-secure for <span class="math inline">t ≥ ⌈n/3⌉</span> and <span class="math inline">\epsilon &lt; 1/3</span>. (That is, secure w.p &gt; 2/3 with 1 faulty player).</p><p>Idea: convert into 3-player protocol</p><h4 id="asynchronous-ba">Asynchronous BA</h4><p>FLP (Fischer, Lynch, Patterson):</p><p><br> <br></p><hr><p><em>Banner Image Ref:</em></p><p>https://cointelegraph.com/bitcoin-for-beginners/what-are-cryptocurrencies</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Blockchain · 区块链 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Natural Language Processing</title>
      <link href="/2020/04/16/data-science-shu-ju-ke-xue/natural-language-processing/"/>
      <url>/2020/04/16/data-science-shu-ju-ke-xue/natural-language-processing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="overview">Overview</h2><p>Three type of models:</p><ul><li>Generative Models</li><li>Discriminative Models, e.g. neural networks</li><li>Graphical Models</li></ul><p>The fundamental goal of NLP is to have <strong>deep understanding of broad language</strong>.</p><p>End systems that we want to build:</p><ul><li><strong>Simple</strong>: spelling correction, text categorization, ...</li><li><strong>Complex</strong>: speech recognition, machine translation, information extraction, dialog interfaces, question answering, ...</li><li>Unknown: human-level comprehension (probably not just NLP)</li></ul><p><strong>Key problems</strong> in NLP:</p><ol type="1"><li><p>Ambiguity</p><ul><li><p>Syntactic ambiguity</p><p>e.g. <em>Stolen Painting Found by Tree</em></p><p>In state-of-the-art ML, can reach ~95% accuracy for many languages when given many training examples</p></li><li><p>Semantic ambiguity</p><p>e.g. <em>Siri, call me an ambulance</em></p></li></ul></li><li><p>Scale</p></li><li><p>Sparsity</p><p><strong>Corpus</strong> is a collection of text. Often annotated in some way.</p></li></ol><h3 id="meta-nlp">Meta NLP</h3><details><p></p><summary> This section is about steps to conduct a NLP research. Unfold to see details. </summary><p></p><h4 id="literature-review-should-be-done-early">Literature review: should be done early</h4><ul><li>Avoid re-invent the wheel</li><li>Learn about common tricks, resources, and libraries</li></ul><ol type="1"><li>Do a keyword search on <em>Google Scholar</em>, <em>Semantic Scholar</em>, or <em>the ACL Anthology</em>.</li><li>Download the papers that seem most relevant.</li><li>Skim the abstracts, intros, and previous work sections.</li><li>Identify papers that look relevant, appear often, or have lots of citations on Google Scholar, and download them. Then repeat.</li></ol><p>Places to find the most trustworthy papers:</p><ul><li><em>NLP</em>: Proceedings of ACL conferences (ACL, NAACL, EACL, EMNLP, CoNLL, LREC), Journal of Computational Linguistics, TACL, COLING, arXiv*</li><li><em>Machine Learning/AI</em>: Proceedings of NIPS, ICML, ICLR, AAAI, IJCAI, and arXiv*</li><li><em>Computational Linguistics</em>: Journals like Linguistic Inquiry, NLLT, Semantics and Pragmatics</li></ul><p>What to mention in literature review:</p><ul><li>General problem/task definition</li><li>Relevant methods and results</li><li>Comparisons with your work and other related work</li><li>Open issues</li></ul><h4 id="acquiring-datasets">Acquiring Datasets</h4><ul><li><p>Existing datasets</p><p>ACL anthology, Linguistic Data Consortium (LDC), Look for datasheets when available (e.g. QuAC)</p></li><li><p>Wild datasets</p><p>e.g. Ubuntu Dialogue Corpus, StackOverflow Data (warning: easy to violate copyright and terms of service)</p></li><li><p>Build own datasets</p><p>Either write detailed guidelines, and work with experts; or write simple guidelines, and crowdsource</p></li><li><p>Generate datasets</p><p>It's super easy, but artificial data does not reflect the real world.</p></li></ul><h4 id="quantitative-evaluation">Quantitative Evaluation</h4><ol type="1"><li><p>Follow prior work and use existing metrics</p><ul><li>If it's a new task, create a metric before you start testing. It must be independent of your model.</li></ul></li><li><p>Use ablations to study the effectiveness of your choices (and don’t adopt fancy solutions that don’t really help)</p><ul><li>e.g. MLP sentiment classifier with GloVe embeddings, MLP sentiment classifier with random embeddings, MaxEnt classifier with GloVe embeddings, MaxEnt classifier with random embeddings</li></ul></li><li><p>Consider controlled human evaluation when standard, and even when less standard</p><ul><li>e.g. summarization, machine translation, generation</li></ul></li><li><p>Test for statistical significance when differences are small and models are complex</p></li><li><p>Consider extrinsic evaluation on <em>downstream tasks</em> (what the field calls those supervised-learning tasks that utilize a pre-trained model or component)</p></li><li><p>Negative results are also important.</p></li></ol><h4 id="qualitative-evaluation-and-error-analysis">Qualitative Evaluation and Error Analysis</h4><p>Goal: convince that your hypothesis is correct.</p><p>Interesting hypotheses are often hard to evaluate with standard/intuitive quantitative metrics.</p><p>Start with:</p><ul><li>Look to prior work</li><li>Show examples of system output</li><li>Identify qualitative categories of system error and count them</li><li>Visualize your embedding spaces with tools like t-SNE or PCA</li><li>Visualize your hidden states with tools like LSTMVis</li><li>Plot how your model performance varies with amount of data</li><li>Build an online demo if ambitious (which I'm probably not <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">🤔</span>)</li></ul><p><strong>Formative evaluation</strong>: guiding further investigations - Typically: lightweight, automatic, intrinsic - Compare design option A to option B - Tune <em>hyperparameters</em> (parameter whose value is set before the learning process begins): smoothing, weighting, learning rate</p><p><strong>Summative evaluation</strong>: reporting results - Compare your approach to previous approaches - Compare different major variants of your approach - Generally only bother with human or extrinsic evaluations here</p><p><em>Common mistake: Don’t save all your qualitative evaluation for the summative evaluation.</em></p><h4 id="hyperparameter-tuning">Hyperparameter Tuning</h4><ul><li>Must tune the hyperparameters of your baselines just as thoroughly as you tune them for any new model you propose</li><li>Failure to do this invalidates your comparisons</li><li>As always, don't tune on test set.</li><li>Read the fine print while you’re doing your literature review to get a sense of what hyperparameters to worry about and what values to expect.</li><li>If you’re not sure whether to tune a hyperparameter, you probably should.</li></ul><p>Methods:</p><ul><li>Grid search: Inefficient (but common)</li><li>Bayesian optimization: Optimal, but public packages aren’t great.</li><li>Good read: random search - easy, and near-optimal<ul><li>Define distributions over all your hyperparameters.</li><li>Sample N times for N experiments.</li><li>Look for patterns in your results.</li><li>Adjust the distributions and repeat until you run out of resources or performance stops improving.</li></ul></li></ul><h4 id="other">Other</h4><p><strong>Biases</strong></p><p>Deploying biased models in the wrong places can lead to harms far worse than bad user experiences.</p><p>Model de-biasing can be complex, political, and maybe even impossible to do fully, and it may harm performance on reasonable metrics.</p><details><p></p><summary>Unfold to see a bias example</summary><p></p><p>In training data, women appear in cooking scenes 33% more often than men.</p>In model’s labeling of similar test data, women are detected in cooking scenes 68% more often than men.</details><p><strong>Talk about data</strong></p><ul><li>What your data looks like, why it was collected, and what kind of information your system learn from it</li><li>Who (country, region, gender, native language, etc.) produced the text and labels in your dataset</li><li>Any known biases in your dataset (including the obvious ones)</li></ul></details><h3 id="computation-graphs">Computation Graphs</h3><p>Computation graphs are the descriptive language of deep learning models.</p><p>Functional description of the required computation.</p><p>Can be instantiated to do two types of computation: forward and backward computation.</p><h4 id="structure">Structure</h4><ul><li>A <em>node</em> is a {tensor, matrix, vector, scalar} value</li><li>An <em>edge</em> represents a function argument (and also data dependency).</li><li>A node with an incoming edge is a <em>function</em> of that edge’s tail node.</li><li>A node knows how to compute its value and the value of its derivative w.r.t each argument (edge) times a derivative of an arbitrary input <span class="math inline">\frac{dF}{df(u)}</span>.</li></ul><h4 id="algorithms">Algorithms</h4><p><strong>Forward propagation</strong></p><ul><li>Loop over nodes in topological order</li><li>Compute the value of the node given its inputs</li><li>Given my inputs, make a prediction (or compute an "error" with respect to a "target output")</li></ul><p><strong>Backward propagation</strong></p><ul><li>Loop over the nodes in reverse topological order starting with a final goal node</li><li>Compute derivatives of final goal node value with respect to each edge’s tail node</li><li>How does the output change if I make a small change to the inputs?</li></ul><p><strong>Static declaration</strong></p><ul><li>Phase 1: define an architecture (maybe with some primitive flow control like loops and conditionals)</li><li>Phase 2: run a bunch of data through it to train the model and/or make predictions</li></ul><p><strong>Dynamic declaration</strong></p><ul><li>Graph is defined implicitly (e.g., using operator overloading) as the forward computation is executed</li></ul><p><strong>Batching</strong></p><ul><li><p>Packing a few examples together has significant computational benefits</p></li><li><p>CPU is helpful, but with GPU you get to use all the GPU cores, which is world changing.</p></li><li><p>Easy with simple networks, but gets harder as the architecture becomes more complex</p><ul><li>Complex networks may include different parts with varying length</li><li>It is very hard to batch complete examples this way</li><li>But you can still batch sub-parts across examples, so you alternate between batched and non-batched computations</li></ul></li></ul><h2 id="text-classification">Text Classification</h2><p>Examples of classification problems: spam vs. non-spam, text genre, word sense, etc.</p><p><strong>Supervised Learning</strong>:</p><ul><li>Naïve Bayes</li><li>Log-linear models (Maximum Entropy Models)</li><li>Weighted linear models and the Perceptron</li><li>Neural networks</li></ul><p>Learning from annotated data problem:</p><ul><li>Annotation requires specific expertise.</li><li>Annotation is expensive.</li><li>Data is private and not accessible.</li><li>Often difficult to define and be consistent.</li></ul><p><em>Always think about the data, and how much of it your model needs (even better: think of the data first, model second).</em></p><p><strong>Training data, development data and held-out data</strong>: an important tool for estimating generalization is train on one set, evaluate during development on another, and use test data only once.</p><p>* My understanding of dev vs. test: dev data is the one that you have correct labels to, while test data is like real new data coming in.</p><p><strong>Main ideas of text classification</strong>:</p><ul><li>Representation as feature vectors</li><li>Scoring by linear functions</li><li>Learning by optimization</li></ul><p><strong>Probabilistic classifiers</strong>: two broad approaches to predict a class <span class="math inline">y</span></p><ol type="a"><li>Joint / Generative models (e.g. Naïve Bayes)</li></ol><ul><li>Work with a joint probabilistic model of data <span class="math inline">P(X,y)</span>, weights are (often) local conditional probabilities.</li><li>Often assume functional form for <span class="math inline">P(X|y), P(y)</span>; represent p(y,X) as Naïve Bayes model, compute <span class="math inline">y*=argmax_y p(y,X)= argmax_y p(y)p(X|y)</span>.</li><li>Estimate <em>probabilities</em> from data.</li><li>Advantages: learning weights is easy and well understood.</li></ul><ol start="2" type="a"><li>Conditional / Discriminative models (e.g. Logistic Regression)</li></ol><ul><li>Work with conditional probability <span class="math inline">p(y|X)</span>. We can then directly compute <span class="math inline">y* = argmax_y p(y|X)</span> (estimating <span class="math inline">p(y|X)</span> directly).</li><li>Require numerical optimization methods.</li><li>Estimate <em>parameters</em> from data.</li><li>Advantages: Don’t have to model <span class="math inline">p(X)</span>! Can develop feature rich models for <span class="math inline">p(y|X)</span>.</li><li>Popular in NLP.</li></ul><h3 id="generative-approach-naïve-bayes-models">Generative Approach: Naïve-Bayes Models</h3><p>The generative story: <strong>pick a topic, then generate a document.</strong></p><p><strong>Assumption: All words (features) are independent given the topic (label).</strong></p><p>Order invariant for tokens.</p><p>Issues: underflow; large number of topics (a lot of computing).</p><p>NB Learning: <strong>Maximum Likelihood Estimate</strong></p><p>In MLE, two parameters to estimate: 1) <span class="math inline">𝑞(𝑦) = \theta_y</span> for each topic <span class="math inline">𝑦</span>; 2) <span class="math inline">q(x|y) = \theta_{xy}</span> for each topic <span class="math inline">y</span> and word <span class="math inline">x</span>.</p><p><strong>Zero frequency problem</strong> in MLE: if there is a zero in the calculation the whole product becomes zero, no matter how many other values you got which maybe would find another solution.</p><p>Learning by <strong>count</strong>: <span class="math inline">\theta_y = C(y)/N</span>, <span class="math inline">\theta_{xy} = C(x,y)/C(y)</span>. The learning complexity is <strong>O(n)</strong>.</p><p><em>Word sense</em>: bag-of-words classification works ok for nouns, but verbal senses are less topical and more sensitive to structure (argument choice).</p><h3 id="discriminative-approach-linear-models">Discriminative Approach: Linear Models</h3><p>Features are indicator functions which count the occurrences of certain patterns in the input. Initially we will have different feature values for every pair of input <span class="math inline">X</span> and class <span class="math inline">y</span>.</p><p><strong>Block Feature Vectors</strong>: Input has features, which are multiplied by outputs to form the candidates.</p><p>Different candidates will often share features.</p><p>In linear models, each feature gets a weight in <span class="math inline">w</span>. We compute the scores and then according to the prediction rule, we choose the highest (positive) one.</p><details><p></p><summary>Example of Linear Model</summary><p></p><p><span class="math inline">\Phi(X, SPORTS) = [1 0 1 0 \space 0 0 0 0 \space 0 0 0 0]</span></p><p><span class="math inline">\Phi(X, POLITICS) = [0 0 0 0 \space 1 0 1 0 \space 0 0 0 0]</span></p><p><span class="math inline">\Phi(X, OTHER) = [0 0 0 0 \space 0 0 0 0 \space 1 0 1 0]</span></p><p><span class="math inline">W = [1 \space 1 \space -1 \space 2, 1 \space -1 \space 1 \space -2, -2 \space -1 \space -1 \space 1]</span></p>Respectively <span class="math inline">SCORE= 0;2;-3</span>, thus <span class="math inline">prediction = POLITICS</span></details><p>(Multinomial) Naïve-Bayes is a linear model.</p><p><strong>Picking weights</strong>:</p><ul><li>Goal: choose "best" vector <span class="math inline">w</span> given training data.</li><li>The best we can ask for are weights that give best training set accuracy, but it's a hard optimization problem.</li></ul><h3 id="discriminative-approach-maximum-entropy-models-logistic-regression">Discriminative Approach: Maximum Entropy Models (=Logistic Regression)</h3><p>Use the scores as probabilities.</p><p>Learning: maximize the (log) conditional likelihood of training data <span class="math inline">\{(X^{(i)}, y^{(i)})\}^N_{i=1}</span></p><p>An equation is said to be a <strong>closed-form solution</strong> if it solves a given problem in terms of functions and mathematical operations from a given generally accepted set.</p><p><span class="math inline">L(w) = \Sigma^{N}_{i=1}logP(y^{(i)}|X^{(i)}; w), w^*=argmax_wL(w)</span></p><p>-- <span class="math inline">w^*</span> doesn't have a closed-form solution. So the MaxEnt objective is an <em>unconstrained optimization problem</em>.</p><ul><li>Basic idea: move uphill from current guess.</li><li>Gradient ascent / descent follows the gradient incrementally.</li><li>At <strong>local optimum</strong>, derivative vector is <strong>zero</strong>.</li><li>Will converge <strong>if step sizes are small enough</strong>, but <strong>not efficient</strong>.</li><li>All we need is to be able to evaluate the function and its derivative. Once we have a function <span class="math inline">f</span>, we can find a local optimum by iteratively following the gradient.</li><li>For <strong>convex</strong> (curved or rounded outward) functions, <strong>a local optimum will be global</strong>. Convexity guarantees a single, global maximum value because any higher points are greedily reachable.</li></ul><p>Basic gradient ascent isn’t very efficient, but there are simple enhancements which take into account previous gradients: conjugate gradient, L-BFGS. There are special-purpose optimization techniques for MaxEnt, like iterative scaling, but they aren’t better.</p><p><strong>The optimum parameters are the ones for which each feature’s predicted expectation equals its empirical expectation.</strong></p><p>In logistic regression, instead of worrying about zero count in MLE, we worry about <em>large feature weights</em>. Use regularization (smoothing) for log-linear models (add a L2 regularization term to the likelihood to push weights to zero).</p><p>But even after regularization, MaxEnt still doesn't have a closed-form solution. We will have to differentiate and use gradient ascent.</p><h3 id="perceptron-algorithm">Perceptron Algorithm</h3><p>Iteratively processes the training set, reacting to training errors. Can be thought of as trying to drive down training error.</p><ul><li>Online (or batch)</li><li>Error driven</li><li>Simple, additive updates</li></ul><h4 id="binary-class">Binary Class</h4><p><strong>Steps</strong> for the online (binary <span class="math inline">y = \plusmn 1</span>) perceptron algorithm:</p><ol type="1"><li>Start with <strong>zero</strong> weights</li><li>Visit training instances <span class="math inline">(X^{(i)}, y^{(i)})</span> one by one, until all correct<ul><li>Make a prediction: <span class="math inline">y^* = sign(w ·\Phi(X^{(i)}))</span> (sign means whether it &gt;(=) 0)</li><li>If correct (<span class="math inline">y^*==y^{(i)}</span>): no change, go to next example!</li><li>If wrong: adjust weights <span class="math inline">w = w - y^* \Phi(X^{(i)})</span></li></ul></li></ol><p>The perceptron finds a separating hyperplane.</p><p>If add bias, algorithm stays the same.</p><h4 id="multiclass">Multiclass</h4><p>For multiclass situation, we will have:</p><ul><li>A weight vector for each class</li><li>Score (activation) of a class <span class="math inline">y</span>: <span class="math inline">w_y·\Phi(x)</span></li><li>Compare all possible outputs, prediction highest score wins</li></ul><p><strong>Perceptron learning traits</strong>:</p><ul><li>No counting or computing probabilities on training set</li><li>Separability: some parameters get the training set perfectly correct</li><li>Convergence: if the training is separable (e.g. could divide into two classes with 100% accuracy for binary case), perceptron will eventually converge</li><li>Mistake Bound: the maximum number of mistakes (binary case) related to the margin or degree of separability</li></ul><p><strong>Perceptron problems</strong>:</p><ul><li>Noise: if the data isn’t separable, weights might thrash<ul><li>Averaging weight vectors over time can help (averaged perceptron)</li></ul></li><li>Mediocre generalization: finds a “barely” separating solution</li><li>Overtraining: test / held-out accuracy usually rises, then falls (overtraining is a kind of overfitting)</li></ul><h3 id="a-note-for-features-tfidf">A Note for Features: TF/IDF</h3><ul><li>More frequent terms in a document are more important.</li><li>May want to normalize term frequency (TF) by dividing by the frequency of the most common term in the document.</li><li>Terms that appear in many different documents are less indicative (thus inverse document frequency)</li></ul><h3 id="neural-networks">Neural Networks</h3><h4 id="two-types-of-machine-learning">Two types of machine learning</h4><p><strong><em>Representation Learning</em></strong> attempts to automatically learn good features and representations.</p><p><strong><em>Deep Learning</em></strong> attempts to learn multiple levels of representation of increasing complexity/abstraction.</p><h4 id="parameters">Parameters</h4><ul><li>Weights: <span class="math inline">w</span> and <span class="math inline">b</span></li><li>Activation function (If dropped and single neuron, becomes perceptron)</li><li>Hidden layer numbers and neuron numbers</li></ul><h4 id="process">Process</h4><p>Neural net -&gt; several MaxEnt models</p><p>Hidden layer figures out what to do by <em>learning</em>.</p><p>Training:</p><ul><li>Without hidden layer: supervised, just like MaxEnt</li><li>With hidden layer: latent units -&gt; not convex; back propagate the gradient, about the same but no guarantee</li></ul><h4 id="probabilistic-output-from-neural-nets">Probabilistic Output from Neural Nets</h4><p>Normalize the output activations with <strong><em>softmax</em></strong>:</p><p><span class="math display">y = softmax(o) \\softmax(o_i) = \frac{exp(o_i)}{\Sigma^k_{j=1}exp(o_j)}</span></p><p>* <span class="math inline">o</span> is the output layer.</p><p>Usually no non-linearity before softmax.</p><h4 id="word-embeddings">Word Embeddings</h4><p>Representing words with <strong>one-hot</strong> vectors.</p><details><p></p><summary>Dimensionality (unfold)</summary><p></p><ul><li>Size of vocabulary</li><li>20K for speech</li><li>500K for broad-converage domains</li><li>13M for Google corpora</li></ul></details><p>The word embeddings should represent each word with a <em>dense low-dimensional vector</em>.</p><p>Low-dimensional &lt;&lt; vocabulary size</p><p>If trained well, similar words will have similar vectors.</p><p>Word embeddings as features, e.g. sentiment classification.</p><p>Feature based models: bag of words</p><details><summary>Practical Tips (unfold)</summary> - Select network structure appropriate for the problem, e.g. window vs. recurrent vs. recursive - Gradient checks to identify bugs - Parameter initialization - If model isn't powerful enough, make it larger; else regularize to avoid overfitting - Know your non-linearity function and its gradient</details><p><strong>Debugging</strong></p><ul><li><p>Verify value of initial loss when using softmax.</p></li><li><p>Perfectly fit a single mini-batch.</p></li><li><p>If learning fails completely, maybe gradients stuck.</p><ul><li>Check learning rate</li><li>Verify parameter initialization</li><li>Change non-linearity functions</li></ul></li></ul><p><strong>Avoid overfitting</strong></p><ul><li>Reduce model size a little</li><li>L1 and L2 regularization</li><li>Early stopping</li><li>Dropout</li></ul><p>Word embeddings vs. sparse vectors:</p><ul><li>Count vectors: sparse and large</li><li>Embedded vectors: small dense</li><li>More contested advantage other than dimensionality: better generalization</li></ul><h2 id="lexical-semantics">Lexical Semantics</h2><h3 id="word-sense-disambiguation-wsd">Word Sense Disambiguation (WSD)</h3><p><strong>Lemma (citation form)</strong>: Basic part of the word, same stem, rough semantics. One lemma can have many meanings.</p><p><strong>Wordform</strong>: The "inflected" word as it appears in text.</p><details><p></p><summary>Examples (click to unfold)</summary><p></p><p>Wordform: banks, sung</p><p>Lemma: bank, sing</p></details><p><strong>Sense (word sense)</strong>: A discrete representation of an aspect of a word’s meaning.</p><p><strong>Homonyms</strong>: Words that share a form but have unrelated, distinct meanings.</p><ul><li>Homographs: bank/bank</li><li>Homophones: write/right</li></ul><p>Homonyms in NLP: Information retrieval, machine translation, text-to-speech</p><p><strong>Synonyms</strong>: Different words that have same meanings in some or all contexts.</p><ul><li>Very few examples for perfect synonyms</li></ul><p><strong>Antonyms</strong>: Senses that are opposites with respect to one feature of meaning.</p><p><strong>Hyponymy and Hypernymy</strong>: One sense is a hyponym/subordinate of another if the first sense is more specific, denoting a subclass of the other; On the contrary, the other will be hypernym/superordinate.</p><details><summary>Examples (click to unfold)</summary> - Car is a hyponym of vehicle - Fruit is a hypernym of mango</details><h4 id="wordnet">Wordnet</h4><p>A hierarchically organized lexical database.</p><ul><li>Each word in WordNet has at least one sense</li><li>Each sense has a gloss (textual description)</li><li>The synset (synonym set), the set of near-synonyms, is a set of senses with a shared gloss</li></ul><details><p></p><summary>Example (click to unfold)</summary><p></p><p>Chump as a noun with the gloss: "a person who is gullible and easy to take advantage of"</p><p>This sense of "chump" is shared with 9 words: chump1, fool2, gull1, mark9, patsy1, fall guy1, sucker1, soft touch1, mug2</p>All these senses have the same gloss -&gt; they form a synset</details><h4 id="supervised-wsd">Supervised WSD</h4><p>Given a lexicon (e.g., WordNet) and a word in a sentence, the goal is to classify the sense of the word.</p><p>Linear model:</p><p><span class="math display">p(sense | word, context) \propto e^{\theta *\phi(sense, word, context)}\\p(sense | word, context) \frac{e^{\theta *\phi(sense, word, context)}}{\Sigma_{s'}e^{\theta *\phi(s', word, context)}} \\</span></p><h4 id="unsupervised-wsd">Unsupervised WSD</h4><p>Goal: induce the senses of each word and classify in context</p><ol type="1"><li>For each word in context, compute some features</li><li>Cluster each instance using a clustering algorithm</li><li>Cluster labels are word senses</li></ol><h3 id="semantic-role-labeling-srl">Semantic role labeling (SRL)</h3><ul><li>Some word senses (a.k.a. predicates) represent events</li><li>Events have participants that have specific roles (as arguments)</li><li>Predicate-argument structure at the type level can be stored in a lexicon</li></ul><h4 id="propbank">PropBank</h4><p>A semantic role lexicon.</p><p>run.01 (operate) ---- Frame</p><ul><li>ARG0 (operator)</li><li>ARG1 (machine/operation)</li><li>ARG2 (employer)</li><li>ARG3 (co-worker)</li><li>ARG4 (instrument) ----Semantic roles</li></ul><p>*FrameNet, an alternative role lexicon</p><p>Task for semantic role labeling: given a sentence, disambiguate predicate frames and annotate semantic roles</p><h4 id="role-identification">Role Identification</h4><p>Classification models similar to WSD</p><p>Sentence spans -&gt; potential roles</p><p>Score can come from any classifier (linear, SVM, NN)</p><h4 id="word-similarity">Word Similarity</h4><p>Given two words, predict how similar they are.</p><p>The Distributional Hypothesis: You shall know a word by the company it keeps.</p><p>Given a vocabulary of <span class="math inline">n</span> words, represent a word <span class="math inline">w</span> as: <span class="math inline">w =(f_1, f_2, f_3, ..., f_n)</span></p><p><span class="math inline">f_i</span>: Binary (or count) features indicating the presence of the <span class="math inline">i^{th}</span> word in the vocabulary in the word’s context</p><p>Similarity can be measured using vector distance metrics. e.g. cosine similarity, gives values between -1 (completely different), 0 (orthogonal), and 1 (the same).</p><p><strong>Vector-space Models</strong></p><ul><li>Words represented by vectors</li><li>In contrast to the discrete class representation of word senses</li><li>Common methods (and packages): Word2Vec, GloVe</li></ul><h4 id="word2vec">Word2Vec</h4><ul><li>Method (and open-source package) for learning word vectors from raw text</li><li>Widely used across academia/industry</li><li>Goal: good word embeddings, aka. embeddings are vectors in a low dimensional space; similar words should be close to one another</li><li>Two models: skip-gram, CBOW</li></ul><h4 id="skip-gram-model">Skip-Gram Model</h4><p>Given: corpus <span class="math inline">D</span> of pairs <span class="math inline">(w, c)</span> where <span class="math inline">w</span> is a word and <span class="math inline">c</span> is context.</p><p>Context may be a single neighboring word (in window of size <span class="math inline">k</span>). Consider the parameterized probability <span class="math inline">p(c|w;\theta)</span>.</p><p>Goal: maximize the corpus probability:</p><p><span class="math inline">argmax_{\theta} \Pi_{(w,c)\in D}p(c|w;\theta)</span></p><p>The important thing is <strong>how we parametrize the probability distribution</strong>.</p><p>If <span class="math inline">d</span> is the dimensionality of the vectors, we have <span class="math inline">d \times |V| + d \times |C|</span> parameters.</p><p>The log of the objective sums over all context is <strong>not tractable in practice</strong>. It can be approximated with <strong><em>negative sampling</em></strong>.</p><p>Negative sampling for skip-gram:</p><ul><li>Efficient way of deriving word embeddings</li><li>Consider a word-context pair <span class="math inline">(w,c)</span>, the probability that it was not observed is <span class="math inline">1-p(D=1|w,c)</span>.</li><li>Parameterization: <span class="math inline">p(D=1|w,c)=\frac{1}{1+e^{-v_cv_w}}</span></li><li>New learning objective： <span class="math inline">argmax_\theta\Pi_{(w,c)\in D}p(D=1|w,c)\Pi_{(w,c)\in D'}p(D=0|w,c)</span></li><li>For a given <span class="math inline">k</span>, the size of <span class="math inline">D'</span> is <span class="math inline">k</span> times bigger than <span class="math inline">D</span></li><li>Each context c is a word</li><li>For each observed word-context pair, <span class="math inline">k</span> samples are generated based on unigram distribution</li></ul><p>Intuition for skip-gram model: <strong>words that share many contexts will be similar</strong>.</p><h2 id="language-model">Language Model</h2><h3 id="problem-overview">Problem Overview</h3><p><strong>Setup</strong>: assume a (finite) vocabulary of words <span class="math inline">V=\{ the, a, man, telescope, Beckham, two, Madrid,...\}</span>, We can construct an (infinite) set of strings</p><p><strong>Data</strong>: given a training set of example sentences</p><p><strong>Problem</strong>: estimate a probability distribution over sentences</p><p><em>Why would we ever want to do this?</em></p><p>Answer: (Automatic) Speech Recognition (ASR): audio in, text out</p><details><p></p><summary>Click to unfold and see some funny examples</summary> Wreck a nice beach? -- Recognize speech<p></p>Eye eight uh Jerry? -- I ate a cherry</details><h4 id="learning-language-models">Learning language models</h4><p><strong>Goal</strong>: Assign useful probabilities <span class="math inline">P(X)</span> to sentences <span class="math inline">X</span></p><p><em>Input</em>: many observations of training sentences <span class="math inline">X</span></p><p><em>Output</em>: system that can compute <span class="math inline">P(X)</span></p><p>Probabilities should broadly indicate plausibility of sentences, e.g. P("I saw a van") &gt;&gt; P("eyes awe of an")</p><p>This is not only about grammar, e.g. P("infected curly chair") =(almost) 0 (yes, I made this one up)</p><h3 id="models">Models</h3><p>Empirical distribution over training sentences...</p><ul><li>Doesn't generalize at all</li><li>Need to assign non-zero probability to previously unseen sentences</li></ul><p>Decompose Probability...</p><ul><li>Assume word choice depends on previous words only</li><li>Not really better because last word still represents complete event</li></ul><p>Markov assumption</p><ul><li>P(english | this is written in)</li><li>P(english | is written in)</li><li>P(english | written in)</li><li>P(english | in)</li><li>P(english)</li></ul><h4 id="the-noisy-channel-model">The Noisy Channel Model</h4><p>Goal: predict sentence given acoustics</p><p><span class="math display">w^* = argmax_XP(X|a)= argmax_XP(a|X)P(X)</span></p><ul><li>Language model: Distributions over sequences of words (sentences) -- <span class="math inline">P(X)</span></li><li>Acoustic model: Distributions over acoustic waves given a sentence -- <span class="math inline">P(a|X)</span></li></ul><p>ASR Noisy Channel System:</p><p>Language Model: source <span class="math inline">P(X)</span> -&gt; <span class="math inline">X</span> -&gt; Acoustic Model: channel <span class="math inline">P(a|X)</span> -&gt; <span class="math inline">a</span></p><p>observed <span class="math inline">a</span> -&gt; decoder (LM&amp;AM) -&gt; best <span class="math inline">X</span></p><p>Other systems with similar structure: <em>MT Noisy Channel System (translation)</em>, <em>Caption Generation Noisy Channel System</em></p><h4 id="n-gram-models">N-gram Models</h4><p><strong>Unigram</strong></p><ul><li>Generative process: pick a word, pick a word, repeat...until you pick STOP</li><li>Big <strong>problem</strong>: P(the the the the) &gt;&gt; P(a real sentence)</li></ul><p><strong>Bigram (and more)</strong></p><ul><li>Generative process: pick start, pick a word conditioned on previous one, repeat until to pick STOP</li><li>k-gram: conditioning on k-1 previous words</li><li>Learning: estimate the distribution</li></ul><p><strong><em>Well-defined Distributions - proof for unigrams</em></strong></p><p>For all string <span class="math inline">X</span> (of any length), <span class="math inline">P(X) \geq 0</span>.</p><p>Claim: the sum over string of all length is 1: <span class="math inline">\Sigma_XP(X)=1</span></p><p>RNN language models are surprisingly not necessarily well defined distributions.</p><p><strong><em>Parameters for n-gram models</em></strong></p><p>Maximum likelihood estimate - relative frequency</p><p><span class="math display">q_{ML}(w) = \frac{c(w)}{c()}, q_{ML}(w|v) = \frac{c(v,w)}{c(v)}, q_{ML}(w|u,v) = \frac{c(u,v,w)}{c(u,v)}, ...</span></p><p>where <span class="math inline">c</span> is the empirical counts on a training set.</p><p>General approach:</p><ul><li>Take a training set <span class="math inline">D</span></li></ul><h3 id="continuous-representations">Continuous representations</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Data Science · 数据科学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anaconda+VS Code+多版本Python的&quot;Unable to Import&quot;问题解决方案</title>
      <link href="/2020/02/27/bugs-in-the-wild/anaconda-vs-code-duo-ban-ben-python-unable-to-import-wen-ti/"/>
      <url>/2020/02/27/bugs-in-the-wild/anaconda-vs-code-duo-ban-ben-python-unable-to-import-wen-ti/</url>
      
        <content type="html"><![CDATA[<h4 id="问题描述">问题描述</h4><p>Win10系统，借助Anaconda2安装了Python（2.7版本），同时又通过<code>conda create --name py3 python=3.6</code>命令安装了Python 3.6版本。</p><p>这样可以在两个版本之间任意切换。以VS Code为例，只需使用<code>Ctrl + Shift + P</code> -&gt; <code>Python: Select Interpreter</code>即可。</p><p>但在使用过程中遇到了问题，尽管Anaconda已经安装了绝大部分常用库，如果在VS Code使用Python 3.6版本，pylint会对诸如<code>import numpy</code>的命令进行报错，提示没有这个库（具体提示信息记不清了）。如果这时进行<code>pip install numpy</code>，又会提示已经有这个库了。切换回Python 2.7则一切正常。</p><h4 id="解决办法">解决办法</h4><p>这个的原因在于pylint只搜索默认的路径。解决办法是打开<code>settings.json</code>（或在设置里搜索<code>python.autoComplete.extraPaths</code>），如果已经有<code>python.autoComplete.extraPaths</code>这一项，则在其中加入库所在的路径，如果没有则加入类似以下代码： </p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">"python.autoComplete.extraPaths": [</span><br><span class="line">        "D:\\Special\\Anaconda2\\Lib",</span><br><span class="line">        "D:\\Special\\Anaconda2\\Library",</span><br><span class="line">        "D:\\Special\\Anaconda2\\libs",</span><br><span class="line">        "D:\\special\\Anaconda2\\Lib\\site-packages\\numpy"</span><br><span class="line">    ],</span><br></pre></td></tr></tbody></table></figure><p></p><p>把其中的具体路径替换成自己的本地路径即可。实际应该只加入Lib文件夹路径就够了。如果是其他外部库，途径类似（Ref中的例子是google-cloud-sdk的库）。</p><p><strong>注意</strong>：更改完毕设置后，需要重启电脑才能生效，只重启VS Code似乎不行。</p><hr><p>更新：</p><p>在某一次装了另一个版本的Python后不久，这个方法莫名失效了……<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">😒</span><span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">😒</span><span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">😒</span></p><hr><p><em>Ref</em>:</p><p>https://stackoverflow.com/questions/43574995/visual-studio-code-pylint-unable-to-import-protorpc</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Bugs In The Wild </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> VS Code </tag>
            
            <tag> Anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Computer Vision</title>
      <link href="/2020/02/12/data-science-shu-ju-ke-xue/notes-for-computer-vision/"/>
      <url>/2020/02/12/data-science-shu-ju-ke-xue/notes-for-computer-vision/</url>
      
        <content type="html"><![CDATA[<h3 id="basic-concepts">Basic Concepts</h3><p><strong>Image</strong>: a grid(matrix) of intensity values</p><p><strong>Kernel</strong>: the filter/mask.</p><p><strong>Gradient</strong>(of an image): <span class="math inline">▽f = [df/dx, df/dy]</span>(partial derivative), points in the direction of the most rapid increase in intensity.</p><h3 id="filter">Filter</h3><p>Form a new image whose pixels are a combination of the original ones.</p><p>Use filter to get useful information from images or enhance the image.</p><h4 id="linear-filter">Linear Filter</h4><p>Replace each pixel by a linear combination (weighted sums) of its neighbors.</p><ol type="1"><li><p>Cross-correlation</p><p>Like a dot product - sum of the neighbor matrix multiplied by kernel for every exactly matching position. e.g. (5, 3) * (5, 3)</p></li><li><p>Convolution</p><p>Like an outer product - compared with cross correlation, the kernel is flipped both horizontally and vertically.</p><p>Convolution is commutative and associative.</p><p>Why convolution: consider the situation between two identical images except one of them is flipped/turned/etc.</p></li><li><p>Blurring / Sharpening</p><p>Mean filter, box filter</p></li></ol><h4 id="gaussian-filter">Gaussian filter</h4><p>Apply a Gaussian function. Recall that the weights should always be normalized to sum=1.</p><p>Removes "high-frequency" components (low-pass filter).</p><p>Convolution with itself will get another Gaussian filter.</p><h3 id="edge-detection">Edge Detection</h3><p>Convert a 2D image into a set of curves.</p><p>An edge is a place of rapid change in the image intensity function.</p><p>Edges are caused by a variety of factors:</p><ul><li>surface normal discontinuity</li><li>depth discontinuity</li><li>surface color discontinuity</li><li>illumination discontinuity</li></ul><p>To differentiate a digital image:</p><ul><li>reconstruct a continuous image, then compute the derivative</li><li>take discrete derivative(find difference)</li></ul><p>For noisy input images, smooth them first with Gaussian filter(?).</p><p><strong>Sobel Operator</strong>: Common approximation of derivative of Gaussian (need the 1/8 to get the right gradient magnitude)</p><p>Thresholding edges - 2 thresholds, 3 cases (strong edge)</p><p>Connecting edges - Weak edges are edges if they are connected to strong edges. Look in some neighborhood (usually 8 closest).</p><h4 id="canny-edge-detector">Canny Edge Detector</h4><ol type="1"><li>Filter image with derivative of Gaussian</li><li>Find magnitude and orientation of gradient</li><li>Non-maximum suppression</li><li>Linking and thresholding (hysteresis): one low and one high threshold; use the high one to start edge curves and the low threshold to continue them</li></ol><p>Parameters: high threshold, low threshold, sigma(width of the Gaussian blur, large sigma detects large-scale edges)</p><h3 id="sampling">Sampling</h3><h4 id="down-sampling">Down-sampling</h4><p>Making smaller images.</p><p>e.g. throw away every other row and column to create a 1/2 size image.</p><p><strong>Aliasing</strong>: Get a wrong image by sub-sampling (this problem occurs especially for synthetic images).</p><p>Aliasing occurs when your sampling rate is not high enough to capture the amount of detail in your image.</p><p>To avoid aliasing: sampling rate ≥ 2 * max frequency in the image</p><p><strong>Nyquist Rate</strong>: minimum sampling rate</p><p>Wagon-wheel effect: in video wheel appear to rotate backwards</p><p>Fix aliasing: <strong>Gaussian pre-filtering</strong></p><p><strong>Gaussian pyramid</strong>: Represent N*N image as a pyramid of <span class="math inline">1*1, 2*2, 4*4, 2^k*2^k</span> (assuming <span class="math inline">N=2^k</span>)</p><h4 id="up-sampling-image-interpolation">Up-sampling / Image Interpolation</h4><p>Correspondingly, making bigger images.</p><ol type="1"><li>Nearest-neighbor interpolation (use neighbor value to replace)</li><li>Linear interpolation (use a line to fit the gap)</li><li>Gaussian reconstruction</li><li>Bicubic interpolation</li></ol><h3 id="feature-detection">Feature Detection</h3><p>Can be used to conduct automatic panoramas.</p><p>Combine two images:</p><ol type="1"><li>Extract features</li><li>Match features</li><li>Align images</li></ol><p>Find features that are invariant to transformations:</p><ul><li>Geometric invariance: translation, rotation, scale</li><li>Photometric invariance: brightness, exposure, etc.</li></ul><p><strong>Local Features</strong> refer to a pattern or distinct structure found in an image, such as a point, edge, or small image patch.</p><p>Advantages of local features:</p><ul><li>Locality: robust to occlusion (means some sort of blocking of an object[not sure]) and clutter (lots of objects in the image).</li><li>Quantity: hundreds or thousands in a single image</li><li>Distinctiveness: differentiate a large database of objects</li><li>Efficiency: real-time performance achievable</li></ul><p>Good features - look for image regions that are "unusual".</p><h4 id="harris-corner-detection">Harris Corner Detection</h4><p>Consider shifting the window <span class="math inline">W</span> by <span class="math inline">(u,v)</span>, compute the squared differences (SSD). Look for high SSDs.</p><p><span class="math inline">E(u,v) \approx [ u \space v ] \begin{bmatrix}A &amp; B \\ B &amp; C\end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix}</span></p><p><span class="math inline">A = \Sigma I_x^2(dI/dx), B = \Sigma I_xI_y, C = \Sigma I_y^2(dI/dy)</span></p><p><span class="math inline">E(u,v)</span> is locally approximated as a quadratic error function.</p><p>Corner detection summary:</p><ol type="1"><li>Compute the gradient at each point in the image.</li><li>Compute the "ABBC" matrix from the entries in the gradient.</li><li>Compute the eigenvalues.</li><li>Find points with large response (<span class="math inline">\lambda_{max}&gt;threshold</span>).</li><li>Choose these points where <span class="math inline">\lambda_{min}</span> is a local maximum as features.</li></ol><p>In practice, using a simple window <span class="math inline">W</span> doesn't work too well. Instead, we'll weight each derivative value based on its distance from the center pixel.</p><h4 id="image-transformations">Image Transformations</h4><p>Geometric: Rotation, Scale</p><p>Photometric: Intensity</p><p>We want corner locations to be invariant to photometric transformations and covariant to geometric transformations.</p><p><strong>Invariance</strong>: Image is transformed and corner locations do not change.</p><p><strong>Covariance</strong>: If we have two transformed versions of the same image, features should be detected in corresponding locations.</p><p>In <em>Harris Detector</em>:</p><ul><li>Corner location is covariant w.r.t. translation.</li><li>Corner location is covariant w.r.t. rotation.</li><li>Partially invariant to affine intensity change.</li><li>Not invariant to scaling.</li></ul><p><strong>Laplacian of Gaussian</strong> (LoG): Use a Gaussian filter first then Laplacian.</p><p><strong>Difference of Gaussian</strong> (DoG): A Gaussian minus a slightly smaller Gaussian.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Data Science · 数据科学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Behavioral Economics</title>
      <link href="/2020/01/26/economics-jing-ji-xue/notes-for-behavioral-economics/"/>
      <url>/2020/01/26/economics-jing-ji-xue/notes-for-behavioral-economics/</url>
      
        <content type="html"><![CDATA[<h3 id="cognitive-biases">Cognitive Biases</h3><p>The picture below has done a great work summarizing this topic.</p><br><div data-align="center"><img src="https://miro.medium.com/max/3072/1*71TzKnr7bzXU_l_pU6DCNA.jpeg" title="Cognitive Bias Codex"></div><p><br></p><p>[TODO]</p><p>Some thoughts:</p><ul><li>Will the biases cancel each other under certain circumstances?</li></ul><h4 id="making-a-resolution">Making a resolution?</h4><p>A resolution may be wrong for one of three main reasons:</p><ul><li>It’s a resolution created based on what someone else (or society) is telling you to change.</li><li>It’s too vague.</li><li>You don’t have a realistic plan for achieving your resolution.</li></ul><p>Make <strong>SMART</strong> goals - Specific, Measurable, Achievable, Relevant and Time-bound.</p><p>To figure out how to change a habit, try breaking it down into three parts: a cue, a routine and a reward. It's important to know why you do (or can't do) something so you could control it.</p><details><p></p><summary>Example here</summary> Bad Habit: I don't get enough sleep at night.<p></p><p>Cue: I feel like I need time to myself in the evening.</p><p>Routine: I stay up too late watching TV.</p><p>Reward: I'm entertained.</p>Way to change the behavior: Instead of staying up late to watch TV, carve out special time each day to spend by yourself, even if that may mean asking for help with your children or taking a break from work each day.</details><p>A plan should allow some inevitable situations (slip days, or I would like to call "fault tolerance") and prepare for them before they actually show up. Some useful technique include:</p><ul><li>Focus on the small one between what is done and what left.</li><li><strong>Don't be too positive</strong> because who needs a real achievement if already enjoying a fantastic daydream? Try W.O.O.P.: Wish, Outcome, Obstacle, Plan.</li></ul><p>To keep up the resolution, other things might help are:</p><ul><li>Tell someone(or post on public platform even if nobody reads it) about your plan.</li><li>Make yourself lose something if you fail. Money is a good choice.</li><li>Find a group to get some good peer pressure.</li><li><strong>Do a "stress test" before it start to know how feasible it is.</strong></li></ul><h3 id="mental-accounting">Mental Accounting</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Economics · 经济学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Making Aesthetic Interfaces</title>
      <link href="/2019/11/25/design-she-ji/making-aesthetic-interfaces/"/>
      <url>/2019/11/25/design-she-ji/making-aesthetic-interfaces/</url>
      
        <content type="html"><![CDATA[<p>Expert designers usually do not solve every problem, instead, they <strong>reuse</strong> solutions that worked before, aka. design patterns.</p><p><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> is always a good reference resource.</p><h3 id="hierarchy">Hierarchy</h3><p>Decide the content to present, level of importance for each element and organize them.</p><p><strong>Steps</strong> to implement visual hierarchy: collect -&gt; group -&gt; prioritize.</p><p><strong>Aspects</strong>: scale, color, contrast, alignment, proximity.</p><p><strong>Components</strong>: buttons, cards, lists, tabs, menus, etc.</p><h3 id="typography">Typography</h3><h4 id="definition">Definition</h4><p>Two easily mixed up terms:</p><ul><li>A <strong>typeface</strong> is a style of type design which includes a complete scope of characters in all sizes and weight. e.g. Noto Sans SC. Usually could be divided into <strong>Sans-serif</strong> and <strong>Serif</strong>.</li><li>A <strong>font</strong> is a graphical representation of text character usually introduced in one particular typeface, size, and weight. e.g. Multiple files of Noto Sans SC with different suffixes when you download it from Google Fonts.</li></ul><h4 id="parameters">Parameters</h4><p>Consider the common small notebooks with four lines for each row used in elementary schools...it will help understand some of the concepts below.</p><ul><li><p>Mean line and baseline</p><p>The two lines in the middle.</p></li><li><p>X-height (x)</p><p>The height between the two lines, which equals to the height of a lowercased <em>x</em>.</p></li><li><p>Ascender and Descender</p><p>The part above the mean line is ascender. Correspondingly, descender is the part below the baseline.</p></li><li><p>Alignment</p><p>Besides left, right and center, there's a <strong>justified</strong> alignment for English which could be seen used in newspaper.</p></li><li><p>Line length</p><p>A perfect line length is like row numbers when writing codes.</p></li><li><p>Tracking and Kerning</p><p>Space between letters. The former refers to general ones while the latter refers to specific ones.</p></li><li><p>Leading</p><p>The spacing between the baselines of copy.</p></li><li><p>Indent and Line Space</p><p>Paragraph-wise concepts.</p></li></ul><h4 id="rules">Rules</h4><ul><li>One typeface for headline and one for body text is usually enough.</li><li>Never use both indent and line space.</li><li>In design, the standard leading is 120% the point size of the font.</li></ul><h3 id="color-palettes">Color Palettes</h3><h4 id="models">Models</h4><ul><li><strong>Additive / RGB</strong>: use for digital mediums</li><li><strong>Subtractive / CMYK</strong>: use for paint and print</li></ul><h4 id="concepts">Concepts</h4><ul><li><p>Chroma</p><p>Purity of a color. A hue with high chroma has no black, white or gray added.</p><p>Use hues with chromas that are either exactly the same or a few steps away from each other on the color wheel.</p></li><li><p>Hue</p><p>Just color, like purple, blue, etc.</p></li><li><p>Saturation</p><p>How a hue appears under particular lighting conditions.</p></li><li><p>Value</p><p>How light or dark a color is.</p></li></ul><h4 id="schemes">Schemes</h4><ul><li><p>Monochromatic</p><p>Different shades of a specific hue.</p><p>Hard to make mistakes.</p><p>Adding a strong neutral (e.g. black or white) could avoid boredom.</p></li><li><p>Analogous</p><p>Use colors located right next to each other on the color wheel.</p></li><li><p>Complementary</p><p>Combine colors from the opposite site of the color wheel.</p><p>Provides high contrast.</p></li><li><p>Triadic</p><p>Based on three separate colors which are equidistant on the color wheel.</p></li></ul><p><br> <br></p><p><em>Ref:</em></p><p>https://designshack.net/articles/ux-design/google-material-design-everything-you-need-to-know/</p><p>https://uxplanet.org/typography-in-ui-guide-for-beginners-7ee9bdbc4833</p><p>https://uxplanet.org/color-theory-brief-guide-for-designers-76e11c57eaa</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design · 设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> UI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms and Data Structures</title>
      <link href="/2019/11/11/coding-bian-cheng/algorithms-and-data-structures/"/>
      <url>/2019/11/11/coding-bian-cheng/algorithms-and-data-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="theories-and-methods">Theories and Methods</h2><h3 id="dynamic-programming">Dynamic Programming</h3><h4 id="situation">Situation</h4><p>A duplicate process, overlapping subproblems</p><p>Recursion in brute force, optimal substructure</p><h4 id="methods">Methods</h4><ul><li>Memorization</li><li>Table filling</li></ul><h3 id="other-frequently-encountered-skills">Other Frequently Encountered Skills</h3><h4 id="walker-and-runner-solution">"Walker" and "runner" Solution</h4><p>The former move one step every time while the latter move two. Use to detect cycles.</p><h4 id="prefix-sums">Prefix Sums</h4><p>A new array in which <code>new[i] = sum(old[:i])</code>. Take O(n) to construct, but can make it constant time for the following operations.</p><p>e.g. Find subarrays of <span class="math inline">k</span> elements that has average value more than <span class="math inline">t</span>.</p><h2 id="examples-of-problemsolution">Examples of Problem/Solution</h2><ul><li>Find the <span class="math inline">N^{th}</span> fibonacci number. <a href="#dynamic-programming">Dynamic Programming</a></li><li>Given two strings s and t, find the longest subsequence (same order but can skip some letters) common to both strings. <a href="#dynamic-programming">Dynamic Programming</a></li><li>Given array <span class="math inline">a</span> containing integers <span class="math inline">[x_1, …, x_n]</span>, find integers <span class="math inline">i, j</span> such that <span class="math inline">1 ≤ i ≤ j ≤ n</span> and <span class="math inline">\Sigma^{j}_{i}a</span> is maximal. <a href="#dynamic-programming">Dynamic Programming</a></li></ul><h2 id="application-reflex">Application Reflex</h2><h2 id="others">Others</h2><h3 id="tips">Tips</h3><ul><li>Usually recursion takes more space and runs faster, while the "for" loop is the opposite.</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding · 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prototypes</title>
      <link href="/2019/11/10/design-she-ji/prototypes/"/>
      <url>/2019/11/10/design-she-ji/prototypes/</url>
      
        <content type="html"><![CDATA[<p>There are several things people can do to test out a product. In this post I combine prototypes together with personas and storytelling, but in fact they are NOT really part of prototypes.</p><h3 id="designers-oriented">Designers Oriented</h3><p>Methods that don't necessarily involve users.</p><h4 id="personas">Personas</h4><p>A fictional character that is meant to represent a group of users that share common goals, attitudes and behaviors when interacting with a particular product or service.</p><p>Steps:</p><ol type="1"><li>Collect data of users.</li><li>Segment the users.</li><li>Create personas.</li></ol><p>Personas can help designers have <strong>empathy</strong>.</p><p>Question from myself: how do you decide whether a persona is logical or realistic?</p><p>It's easy to find online tools to help you design a persona.</p><h4 id="scenarios">Scenarios</h4><p>Written accounts and narratives of an experience. Imagining a particular situation personas encounter.</p><p>Components: motivation, context, distractions, goal</p><h4 id="storyboarding">Storyboarding</h4><p>Illustrations(drawings) that represent a story. Here's a storyboard I drew in class:</p><br><div data-align="center"><img src="https://od.lk/s/MzBfMTgwMDQxMDFf/storyboard.jpg" width="50%" height="50%" title="My Drawing of Storyboard"></div><p>It's about a situation where people have issues when using smart home devices.</p><p>Compared with actually "telling" a story, it's more direct and could convey more information with less content (save paper <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">😉</span>).</p><p>Some rules:</p><ul><li>Only include necessary details.</li><li>Be proud to be a terrible painter.</li><li>The number of frames should be around six.</li></ul><h3 id="users-oriented">Users Oriented</h3><p>Requires the engagement of users.</p><h4 id="prototype">Prototype</h4><p>Simulate the design with low cost. Could be done iteratively during design.</p><p>Two ways of prototyping:</p><ul><li>Vertical: show only part of the interface, but very detailed.</li><li>Horizontal: show many aspects of the interface in a shallow manner.</li></ul><h4 id="low-fidelity-paper-prototype">Low Fidelity: Paper Prototype</h4><p>Although it sounds more complicated, in fact it's <strong>much easier and faster</strong> to implement compared with prototyping with fancy tools, even if you're a terrible painter.</p><p>It helps designers and users <strong>focus on big things</strong>, e.g. interface logics, instead of spending hours trying to choose a perfect font (that's what I usually did...)</p><p>You are more likely to get "honest" feedback with paper prototype because, people will hesitate to say something negative if they realize you have put a large amount of work into it.</p><p>**Generally it kind of looks like the expensive and fancy cards that were popular among primary school students.*</p><h4 id="high-fidelity-digital-prototype">High Fidelity: Digital Prototype</h4><p>Tools: Figma, Invision, Sketch, and my favorite - <strong>Proto.io</strong>.</p><p>A website that helps with choosing tools: http://www.prototypr.io/prototyping-tools/</p><p>For details in digital prototyping like hierarchy, typography, colors, etc. please refer to another post.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design · 设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> UX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thoughts from Contextual Inquiry</title>
      <link href="/2019/11/09/design-she-ji/thoughts-from-contextual-inquiry/"/>
      <url>/2019/11/09/design-she-ji/thoughts-from-contextual-inquiry/</url>
      
        <content type="html"><![CDATA[<h4 id="definition">Definition</h4><p>Contextual inquiry is a qualitative research method. The researcher will give participants a topic to do something, then observe and ask probing questions.</p><h4 id="more-explanation">More Explanation</h4><ul><li>It takes place in the context of use, e.g. where (home/work) people will use your app product.</li><li>The purpose is to understand why people do certain things and enhance user experience.</li><li>It's a very "free" research method.</li></ul><h4 id="strategy">Strategy</h4><ul><li>Use "participant" instead of "user" or "subject" in the research to connote an active role.</li><li>Introduce the research method to participants and make sure they know that it's the product that is being tested.</li><li>Explain your conclusions and interpretations to participants through out the interview to avoid any misunderstanding.</li></ul><h4 id="notes">Notes</h4><ul><li>Start data analysis ASAP.</li><li>Contextual inquiry can be conducted in any stage of design.</li></ul><h4 id="my-thoughts">My Thoughts</h4><p>When you ask the participants why they do something in a contextual inquiry, they might realize things that they never noticed before. So does <strong>psychology</strong>.</p><p>Just have a friend keep asking questions when describing experiences or stating points can be really helpful. When people thinking by themselves, it's easy for them to "fall into routines" and take something for granted. Even if they try to remind themselves from time to time, it's still not as good as a different perspective.</p><p>It can also be applied to <strong>science</strong>, at least in the past, when no one had been curious enough about the reasons behind falling apples. In general, "why" questions can be annoying, but lose the ability to wonder...it seems way more worse to me.</p><p><br> <br></p><p><em>Ref:</em></p><p>https://www.interaction-design.org/literature/article/contextual-interviews-and-how-to-handle-them</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design · 设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>嵌于窗间的桥辉</title>
      <link href="/2019/10/30/zhi-yan-pian-yu/qian-yu-chuang-jian-de-qiao-hui/"/>
      <url>/2019/10/30/zhi-yan-pian-yu/qian-yu-chuang-jian-de-qiao-hui/</url>
      
        <content type="html"><![CDATA[<p>凌晨五点钟是Queensborough大桥桥架上的灯光准时熄灭的时间。</p><p>——这是我在今天知道的新信息。我甚至还为此发布了一条朋友圈，仿佛一个困扰许久的谜题得到了解答，而我在把它公诸于世。</p><p>实际上也并不是没有这么说的道理。在许多个夜晚，我在黑暗的房间里屡次睁开双眼，有时会看到映在窗上的那星座一般的光点集合，有时则只有川流不息的车。然后我便带着对时间的好奇辗转反侧直到再次睡去。</p><p>凌晨五点钟。所谓的“明天”真正到来的时刻。</p><p>偶尔我会想要向别人讲起类似这样毫无意义的故事，关于我房间的窗户是多么像那种展览会尺寸大小的画框，而我能看到的风景构成了如同哈利·波特的魔法世界里会刊登在报纸上的动态图片；关于桥上的车流是怎样从不停歇、每时每刻都是那么忙碌，即使是在电视节目都会停播的时段。但是夜色在保护着我的同时也隔绝着我，于是我逐渐学会适时地保持沉默。</p><p>如果星辰是灯塔……那这些一团团暖色的光辉，也可以是小小的灯塔吗？</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 只言片语 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Becoming Pythonic</title>
      <link href="/2019/10/29/coding-bian-cheng/becoming-pythonic/"/>
      <url>/2019/10/29/coding-bian-cheng/becoming-pythonic/</url>
      
        <content type="html"><![CDATA[<p><em>Note: This post doesn't include detailed theories. For those, please refer to other posts.</em></p><h3 id="built-in">Built-in</h3><h4 id="grammar">Grammar</h4><ul><li><code>int()</code> returns the lower bound.</li><li>Pay attention to the difference between <code>extend()</code> and <code>append()</code>, e.g. [1,2,4] vs. [[1,2],[4]].</li></ul><h4 id="easy-ways">Easy Ways</h4><ul><li>Remember to use lambda for simple functions and use <code>[x for x in statement if statement]</code> for iterating. And this is called <em>list comprehension</em>.</li><li>Python has a dictionary <code>defaultdict()</code>, it provides default value given by the user. <code>collections.Counter()</code> could be a substitute if the default value is <span class="math inline">0</span>.</li><li>Use <code>zip()</code> to make two lists into one with tuple: a, b -&gt; (a,b) with len(a)/len(b). <strong>Note</strong>: need to convert to list manually in Python 3.</li><li>Use <code>del</code> and <code>gc.collect()</code> to free RAM storage.</li></ul><h4 id="possible-mistakes">Possible Mistakes</h4><ul><li><code>list.append()</code> will only return <strong>None</strong>, which indicates it has successfully complete the process. Use <code>list</code> to get to the changes.</li><li>Things like a <code>range(0,n)</code> do NOT include n but do include 0.</li><li>If let <code>a = b</code> where a,b are both lists, when you change a it will affect b too. Try <code>copy.deepcopy</code>(needs import) or <code>new = old[:]</code>.</li></ul><h4 id="other-in-doc">Other in Doc</h4><ul><li>The time complexity for <code>sort()</code> is O(nlogn).</li></ul><h4 id="differences-between-python-2-and-3">Differences Between Python 2 and 3</h4><ul><li>Calculation of integer is different. In python 2, sometimes need to use "2.0" instead of "2" to get correct answers.</li><li>Print, of course.</li></ul><hr><h3 id="useful-packages">Useful Packages</h3><h4 id="overall">Overall</h4><ul><li>Default packages: csv, math, re, ...</li><li><strong>tqdm</strong>: progress bar package. For iPython Notebook, remember to use <code>tqdm_notebook</code> instead.</li></ul><h4 id="pandas">Pandas</h4><ul><li>Do not iterate through rows of a <em>pandas</em> dataframe unless really necessary. Usually, just define a function and use <code>.apply(func)</code> instead. No parameter needed when calling. It will pass either the selected column or the entire row (select column inside the function instead).</li><li>If want to make changes to an existing <em>pandas</em> dataframe, remember to write <code>inplace = True</code>.</li><li>Combine <em>pandas</em> dataframes: <code>pandas.concat()</code></li><li><code>pandas.read_csv()</code> is good for loading a data file. The "csv" could be replaced with other type of file.</li><li>A <em>pandas</em> object could be converted to a numpy array using <code>to_numpy()</code>.</li><li>Sometimes when reloading an output DataFrame file, by default <em>pandas</em> will <strong>convert lists to strings</strong>. Solution: add parameters: <code>converters={'column_name': eval}</code> in <code>read_csv()</code> function.</li></ul><h4 id="numpy">Numpy</h4><ul><li>Use <code>np.zeros()</code> to create a n-d array with zeros as initial values.</li><li><code>np.nan != np.nan</code>! Which means if you assign <code>np.nan</code> to some variables, you can't use <code>==np.nan</code> to check it. Use <code>isna()</code> instead.</li></ul><h4 id="nltk">NLTK</h4><ul><li>For natural language processing.</li><li>Tokenizer to split sentences into words.</li><li>Lemmatizer to eliminate the effects between different word senses (convert different forms of a word into one, e.g. run, ran, running, runs → run).</li></ul><h4 id="pytorch">PyTorch</h4><ul><li>If loss is <code>NaN</code>, the most possible reason is something's wrong with the data.</li><li><code>cuda()</code> needs GPU.</li><li>To convert <code>cuda()</code> object to numpy arrays, need to add <code>.detach().cpu().clone().numpy()</code></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding · 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Common Research Methods</title>
      <link href="/2019/10/15/design-she-ji/common-research-methods/"/>
      <url>/2019/10/15/design-she-ji/common-research-methods/</url>
      
        <content type="html"><![CDATA[<h3 id="surveys"><strong>Surveys</strong></h3><p>Sort of quantitative methods.</p><h4 id="strategy">Strategy</h4><ul><li>Identify topics needed to be covered.</li><li>Pretest drafts.</li><li>Cross-sectional: different people in same population at multiple points in time.</li><li>Panel / longitudinal: same people over time.</li><li>Use open-ended questions in pilot tests and then use the answers to form close-ended ones.</li></ul><h4 id="rules">Rules</h4><ul><li>Use the same question wording and be careful about where the question is asked.</li><li>The number of choices should be a small number -- 4 or 5 usually.</li><li>Randomize the order of options to reduce potential bias, except for ordinal ones (e.g. from "most" to "least").</li><li>Ask <strong>clear</strong> and <strong>specific</strong> questions. Use simple and concrete language.</li><li>Only ask <strong>1</strong> question at a time. Don't indicate two or more things.</li><li>Be careful not to use biased or potentially offensive to certain respondents. (e.g. biological sex)</li><li>Questions should be grouped and asked in a logical order.</li></ul><h4 id="fun-facts">Fun Facts</h4><ul><li>People tend to choose the options they heard later in a list (recency effect).</li><li>Compared with the better educated and better informed, less educated and less informed respondents have a greater tendency to agree with certain statements.</li><li>People have a natural tendency to want to be accepted and liked, and this may lead people to provide inaccurate answers to questions that deal with sensitive subjects.</li></ul><h3 id="interviews"><strong>Interviews</strong></h3><p>Qualitative research method.</p><h4 id="strategy-1">Strategy</h4><ul><li>Write a general interview guide.</li><li>Modify questions if necessary after each interview.</li><li>Explain the purpose, format, length etc. of the interview. Let the participants ask questions before start the interview.</li><li><strong>Never</strong> count on memories to recall the answers.</li><li>Encourage the participants with nodding, etc.</li><li>Careful with reactions (act surprised or take notes suddenly may discourage the participants), tolerate pauses, show you are listening.</li><li>Stay focused on the topic, don't lose control of the interview.</li><li>Begin with a warm-up question -- simple and easy to answer.</li><li>The last question should provide some closure for the interview.</li></ul><h4 id="rules-1">Rules</h4><ul><li>When there're multiple interviewers, make sure to use the same wordings.</li><li>Be careful to ask "why" questions. Could be hard to answer or, just, annoying. Ask "how" instead.</li><li>Never answer a question for the respondent.</li><li>Ask questions that could elicit long answers.</li><li>Don't ask about general things and let the respondent answer on behalf of a group.</li><li>Complete the scripts right after the interview. Don't wait for a long time.</li></ul><p><br> <br></p><p><em>Ref:</em></p><p><em>https://www.pewresearch.org/methods/u-s-survey-research/questionnaire-design/</em></p><p><em>http://nixdell.com/classes/HCI-and-Design-Spring-2017/Qualitative-Interview-Design.pdf</em></p><p><em>http://nixdell.com/classes/HCI-and-Design-Spring-2017/interview-strategies.pdf</em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design · 设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presentation in Freestyle</title>
      <link href="/2019/09/27/soft-skills-zong-he-neng-li/presentation-in-freestyle/"/>
      <url>/2019/09/27/soft-skills-zong-he-neng-li/presentation-in-freestyle/</url>
      
        <content type="html"><![CDATA[<p>Today I took part in a workshop related to presentation skills as a Studio session. Personally, when giving presentations, there's one thing that has always been difficult for me. Although I could remember all the content (let's say I could<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8">😐</span>) and present in front of people with some kind of confidence, somehow it feels like I'm just doing a perfect job in reciting or remembering all the gestures instead of really trying to convey or explain something I know about, even if it was created completely by myself. What I want to be is a presenter in freestyle -- well, like dancing to the music without choreography<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">😉</span>.</p><p>Ambitions: - Actually pay attention to the audience. Don't see them as a group (background image). - Walk around and use body language naturally, in contrast to imitating a robot. - Humorous, at least 80% of what I will show in daily life. - Handle different kinds of situations without too much struggle. Like a comedian.</p><p>So, with these ambitions, and also a bit of sleepiness<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">😒</span>, I went to the workshop and learned: - <strong>Eye contact</strong> is very important. Also make sure to look at people that not in the center. - Must have a strong and clear <strong>ending</strong> (not the kind of 'that's all').</p><p>Ummm...okay, it was a really good workshop, and I definitely learned more than 2 things. The rest is just all combined with my own thoughts, such as: - Personally: keep my hands away from my hair. Just try to hold something, like a chair or a pen. (Otherwise I will need a shave to fix this habit<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8">😏</span>) - Don't let the audience lose focus for a long period of time(30 seconds perhaps?). Example: write something on the board, deal with slides. - Suppose that you could memorize all the content and you just want to practice your presentation style. An interesting way I came up with: present with another language that your audience don't understand at all, or just ramble words that make no sense, or remain silence, while <strong>you still have a topic to convey</strong>. Kind of like the game to guess a word. I think this will help you focus on the interaction and connection. - Sometimes be completely honest with the audience will help a lot. I mean, suppose you made a mistake during a lecture, if you don't have the ability to fix it or act as nothing happened immediately yet, just show them how you are speaking to yourself "uh-oh." Maybe some non-related explanation: I played video games until 4 this morning so don't expect me to know what I'm talking about. But definitely not too much so you will still be on the content.</p><p>Okay, that's pretty much it. I'm running to do my AML homework now.</p><p>Not a bad article for my first post...I guess?</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Soft Skills · 综合能力 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> presentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>References and Copyrights</title>
      <link href="/2019/09/24/references-and-copyrights/"/>
      <url>/2019/09/24/references-and-copyrights/</url>
      
        <content type="html"><![CDATA[<h3 id="regulations"><font color="#7cd175"> REGULATIONS </font></h3><p>Similar to academic integrity, <font color="#b39ddb"><strong><em>DoveCode</em></strong></font> respects and protects all kinds of intellectual property rights. Most content in this website follows <em>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</em>, and any exception will be stated.</p><p>Reprint/reproduction is welcomed as long as under proper citation. If you would like to "clone" something but couldn't get the access to it, please contact <span class="citation" data-cites="FadingWinds">[@FadingWinds]</span>(https://FadingWinds.me/about) and I would love to help. Also, it is very encouraged to point out any mistakes or missing points you notice.</p><h3 id="references"><font color="#7cd175"> REFERENCES </font></h3><ol type="1"><li><font color="#b39ddb"><strong><em>DoveCode</em></strong></font> is based on <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> and uses <a href="https://github.com/blinkfox/hexo-theme-matery/" target="_blank" rel="noopener">hexo-theme-matery</a> as the theme. Currently all the functions and plugins are derived from them, so references like these won't be listed here right now. Please visit their link to see the details.</li><li>The banner picture comes from <strong>Cytus II</strong> (A game produce by <a href="https://www.rayark.com/" target="_blank" rel="noopener">Rayark</a>). <br><div data-align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzdf/Banner.png" width="50%" height="50%" title="Banner Picture"></div></li><li>Sorry it was unable for me to trace the resource of the website's logo (which is also my avatar image on Github). I would be grateful if anyone happens to know and could provide it to me. <br><div data-align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzhf/logo.png" width="10%" title="Logo"></div></li><li>The icon of <strong><em><font color="#b39ddb">DoveCode</font></em></strong> comes from <a href="https://icons8.com/icons" target="_blank" rel="noopener">Icons8</a>. <br><div data-align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzlf/favicon.png" title="Icon"></div></li></ol><h3 id="copyrights"><font color="#7cd175"> COPYRIGHTS </font></h3><ol type="1"><li>All the posts are original on <strong><em><font color="#b39ddb">DoveCode</font></em></strong> and any quotes/references included will be listed in or at the end of the article.</li><li>There are a set of cover images that will be randomly assigned to an article if it doesn't have a specific one. Anyone is welcomed to submit their own photographs. The current set is as follows. <br><div data-align="center"><img src="https://od.lk/s/MzBfMTcxNjM1NTdf/photos.png" title="Photographed by FadingWinds"></div><br><div data-align="center"><img src="https://od.lk/s/MzBfMTc0NzM0OTZf/Em..png" title="Photographed by Emmanuel Li"></div></li><li>The avatar image on the "About" page is drawn by <strong><em>FadingWinds</em></strong>. <br><div data-align="center"><img src="https://od.lk/s/MzBfMTcxMTQwODFf/NeverAnAngel.jpg" width="30%" title="Drawn by FadingWinds"></div></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
