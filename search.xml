<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>The Chain of Blocks</title>
      <link href="/2020/02/17/the-chain-of-blocks/"/>
      <url>/2020/02/17/the-chain-of-blocks/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Natural Language Processing</title>
      <link href="/2020/02/17/natural-language-processing/"/>
      <url>/2020/02/17/natural-language-processing/</url>
      
        <content type="html"><![CDATA[<h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>Three type of models:</p><ul><li>Generative Models</li><li>Discriminative Models, e.g. neural networks</li><li>Graphical Models</li></ul><p>The fundamental goal of NLP is to have <strong>deep understanding of broad language</strong>.</p><p>End systems that we want to build:</p><ul><li><strong>Simple</strong>: spelling correction, text categorization, â€¦</li><li><strong>Complex</strong>: speech recognition, machine translation, information extraction, dialog interfaces, question answering, â€¦</li><li>Unknown: human-level comprehension (probably not just NLP)</li></ul><p><strong>Key problems</strong> in NLP:</p><ol><li><p>Ambiguity</p><ul><li><p>Syntactic ambiguity</p><p>   e.g. <em>Stolen Painting Found by Tree</em> </p><p>   In state-of-the-art ML, can reach ~95% accuracy for many languages when given many training examples</p></li><li><p>Semantic ambiguity </p><p>   e.g. <em>Siri, call me an ambulance</em></p></li></ul></li><li><p>Scale  </p></li><li><p>Sparsity</p><p><strong>Corpus</strong> is a collection of text. Often annotated in some way. </p></li></ol><h3 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h3><p>Examples of classification problems: spam vs. non-spam, text genre, word sense, etc.</p><p><strong>Supervised Learning</strong>:</p><ul><li>NaÃ¯ve Bayes</li><li>Log-linear models (Maximum Entropy Models)</li><li>Weighted linear models and the Perceptron</li><li>Neural networks</li></ul><p>Learning from annotated data problem:</p><ul><li>Annotation requires specific expertise.</li><li>Annotation is expensive.</li><li>Data is private and not accessible.</li><li>Often difficult to define and be consistent.</li></ul><p><em>Always think about the data, and how much of it your model needs (even better: think of the data first, model second).</em></p><p><strong>Training data, development data and held-out data</strong>: an important tool for estimating generalization is train on one set, evaluate during development on another, and use test data only once.</p><p><strong>Main ideas of text classification</strong>:</p><ul><li>Representation as feature vectors</li><li>Scoring by linear functions</li><li>Learning by optimization</li></ul><p><strong>Probabilistic classifiers</strong>: two broad approaches to predict a class $y$</p><p>a) Joint / Generative models (e.g. NaÃ¯ve Bayes)</p><ul><li>Work with a joint probabilistic model of data $P(X,y)$, weights are (often) local conditional probabilities.</li><li>Often assume functional form for $P(X|y), P(y)$; represent p(y,X) as NaÃ¯ve Bayes model, compute $y*=argmax_y p(y,X)= argmax_y p(y)p(X|y)$.</li><li>Estimate <em>probabilities</em> from data.</li><li>Advantages: learning weights is easy and well understood.</li></ul><p>b) Conditional / Discriminative models (e.g. Logistic Regression)</p><ul><li>Work with conditional probability $p(y|X)$. We can then directly compute $y* = argmax_y p(y|X)$ (estimating $p(y|X)$ directly).</li><li>Require numerical optimization methods.</li><li>Estimate <em>parameters</em> from data.</li><li>Advantages: Donâ€™t have to model $p(X)$! Can develop feature rich models for $p(y|X)$.</li><li>Popular in NLP.</li></ul><h4 id="Generative-Approach-Naive-Bayes-Models"><a href="#Generative-Approach-Naive-Bayes-Models" class="headerlink" title="Generative Approach: NaÃ¯ve-Bayes Models"></a>Generative Approach: NaÃ¯ve-Bayes Models</h4><p>The generative story: <strong>pick a topic, then generate a<br>document.</strong></p><p><strong>Assumption: All words (features) are independent given the topic (label).</strong></p><p>Order invariant for tokens.</p><p>Issues: underflow; large number of topics (a lot of computing).</p><p>NB Learning: <strong>Maximum Likelihood Estimate</strong></p><p>In MLE, two parameters to estimate: 1) $ğ‘(ğ‘¦) = \theta_y$ for each topic $ğ‘¦$; 2) $q(x|y) = \theta_{xy}$ for each topic $y$ and word $x$.</p><p><strong>Zero frequency problem</strong> in MLE: if there is a zero in the calculation the whole product becomes zero, no matter how many other values you got which maybe would find another solution.</p><p>Learning by <strong>count</strong>: $\theta_y = C(y)/N$, $\theta_{xy} = C(x,y)/C(y)$. The learning complexity is <strong>O(n)</strong>.</p><p><em>Word sense</em>: bag-of-words classification works ok for nouns, but verbal senses are less topical and more sensitive to structure (argument choice).</p><h4 id="Discriminative-Approach-Linear-Models"><a href="#Discriminative-Approach-Linear-Models" class="headerlink" title="Discriminative Approach: Linear Models"></a>Discriminative Approach: Linear Models</h4><p>Features are indicator functions which count the occurrences of certain patterns in the input. Initially we will have different feature values for every pair of input $X$ and class $y$.</p><p><strong>Block Feature Vectors</strong>: Input has features, which are multiplied by outputs to form the candidates.</p><p>Different candidates will often share features.</p><p>In linear models, each feature gets a weight in $w$. We compute the scores and then according to the prediction rule, we choose the highest (positive) one.</p><details><summary>Example of Linear Model</summary><p>$\Phi(X, SPORTS) = [1 0 1 0 \space 0 0 0 0 \space 0 0 0 0]$</p><p>$\Phi(X, POLITICS) = [0 0 0 0 \space 1 0 1 0 \space 0 0 0 0]$</p><p>$\Phi(X, OTHER) = [0 0 0 0 \space 0 0 0 0 \space 1 0 1 0]$</p><p>$W = [1 \space 1 \space -1 \space 2, 1 \space -1 \space 1 \space -2, -2 \space -1 \space -1 \space 1]$</p><p>Respectively $SCORE= 0;2;-3$, thus $prediction = POLITICS$</p></details><p>(Multinomial) NaÃ¯ve-Bayes is a linear model.</p><p><strong>Picking weights</strong>:</p><ul><li>Goal: choose â€œbestâ€ vector $w$ given training data.</li><li>The best we can ask for are weights that give best training set accuracy, but itâ€™s a hard optimization problem.</li></ul><h4 id="Discriminative-Approach-Maximum-Entropy-Models-Logistic-Regression"><a href="#Discriminative-Approach-Maximum-Entropy-Models-Logistic-Regression" class="headerlink" title="Discriminative Approach: Maximum Entropy Models (=Logistic Regression)"></a>Discriminative Approach: Maximum Entropy Models (=Logistic Regression)</h4><p>Use the scores as probabilities.</p><p>Learning: maximize the (log) conditional likelihood of training data ${(X^{(i)}, y^{(i)})}^N_{i=1}$</p><p>An equation is said to be a <strong>closed-form solution</strong> if it solves a given problem in terms of functions and mathematical operations from a given generally accepted set.</p><p>$L(w) = \Sigma^{N}_{i=1}logP(y^{(i)}|X^{(i)}; w), w^*=argmax_wL(w)$</p><p>â€“ $w^<em>$ doesnâ€™t have a closed-form solution. So the MaxEnt objective is an *unconstrained optimization problem</em>.</p><ul><li>Basic idea: move uphill from current guess.</li><li>Gradient ascent / descent follows the gradient incrementally.</li><li>At <strong>local optimum</strong>, derivative vector is <strong>zero</strong>.</li><li>Will converge <strong>if step sizes are small enough</strong>, but <strong>not efficient</strong>.</li><li>All we need is to be able to evaluate the function and its derivative. Once we have a function $f$, we can find a local optimum by iteratively following the gradient.</li><li>For <strong>convex</strong> (curved or rounded outward) functions, <strong>a local optimum will be global</strong>. Convexity guarantees a single, global maximum value because any higher points are greedily reachable.</li></ul><p>Basic gradient ascent isnâ€™t very efficient, but there are simple enhancements which take into account previous gradients: conjugate gradient, L-BFGS. There are special-purpose optimization techniques for MaxEnt, like<br>iterative scaling, but they arenâ€™t better.</p><p><strong>The optimum parameters are the ones for which each featureâ€™s predicted expectation equals its empirical expectation.</strong></p><p>In logistic regression, instead of worrying about zero count in MLE, we worry about <em>large feature weights</em>. Use regularization (smoothing) for log-linear models (add a L2 regularization term to the likelihood to push weights to zero).</p><p>But even after regularization, MaxEnt still doesnâ€™t have a closed-form solution. We will have to differentiate and use gradient ascent.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning | æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Computer Vision</title>
      <link href="/2020/02/12/notes-for-computer-vision/"/>
      <url>/2020/02/12/notes-for-computer-vision/</url>
      
        <content type="html"><![CDATA[<h3 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h3><p><strong>Image</strong>: a grid(matrix) of intensity values</p><p><strong>Kernel</strong>: the filter/mask.</p><p><strong>Gradient</strong>(of an image): $â–½f = [df/dx, df/dy]$(partial derivative), points in the direction of the most rapid increase in intensity.</p><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>Form a new image whose pixels are a combination of the original ones.</p><p>Use filter to get useful information from images or enhance the image.</p><h4 id="Linear-Filter"><a href="#Linear-Filter" class="headerlink" title="Linear Filter"></a>Linear Filter</h4><p>Replace each pixel by a linear combination (weighted sums) of its neighbors.</p><ol><li><p>Cross-correlation</p><p> Like a dot product - sum of the neighbor matrix multiplied by kernel for every exactly matching position. e.g. (5, 3) * (5, 3)</p></li></ol><ol start="2"><li><p>Convolution</p><p> Like an outer product - compared with cross correlation, the kernel is flipped both horizontally and vertically.</p><p> Convolution is commutative and associative. </p><p> Why convolution: consider the situation between two identical images except one of them is flipped/turned/etc.</p></li><li><p>Blurring / Sharpening</p><p> Mean filter, box filter</p></li></ol><h4 id="Gaussian-filter"><a href="#Gaussian-filter" class="headerlink" title="Gaussian filter"></a>Gaussian filter</h4><p>Apply a Gaussian function. Recall that the weights should always be normalized to sum=1.</p><p>Removes â€œhigh-frequencyâ€ components (low-pass filter).</p><p>Convolution with itself will get another Gaussian filter.</p><h3 id="Edge-Detection"><a href="#Edge-Detection" class="headerlink" title="Edge Detection"></a>Edge Detection</h3><p>Convert a 2D image into a set of curves.</p><p>An edge is a place of rapid change in the image intensity function.</p><p>Edges are caused by a variety of factors:</p><ul><li>surface normal discontinuity</li><li>depth discontinuity</li><li>surface color discontinuity</li><li>illumination discontinuity</li></ul><p>To differentiate a digital image:</p><ul><li>reconstruct a continuous image, then compute the derivative</li><li>take discrete derivative(find difference)</li></ul><p>For noisy input images, smooth them first with Gaussian filter(?).</p><p><strong>Sobel Operator</strong>: Common approximation of derivative of Gaussian (need the 1/8 to get the right gradient magnitude)</p><p>Thresholding edges - 2 thresholds, 3 cases (strong edge)</p><p>Connecting edges - Weak edges are edges if they are connected to strong edges. Look in some neighborhood (usually 8 closest).</p><h4 id="Canny-Edge-Detector"><a href="#Canny-Edge-Detector" class="headerlink" title="Canny Edge Detector"></a>Canny Edge Detector</h4><ol><li>Filter image with derivative of Gaussian</li><li>Find magnitude and orientation of gradient</li><li>Non-maximum suppression</li><li>Linking and thresholding (hysteresis): one low and one high threshold; use the high one to start edge curves and the low threshold to continue them</li></ol><p>Parameters: high threshold, low threshold, sigma(width of the Gaussian blur, large sigma detects large-scale edges)</p><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><h4 id="Down-sampling"><a href="#Down-sampling" class="headerlink" title="Down-sampling"></a>Down-sampling</h4><p>Making smaller images.</p><p>e.g. throw away every other row and column to create a 1/2 size image.</p><p><strong>Aliasing</strong>: Get a wrong image by sub-sampling (this problem occurs especially for synthetic images).</p><p>Aliasing occurs when your sampling rate is not high enough to capture the amount of detail in your image.</p><p>To avoid aliasing: sampling rate â‰¥ 2 * max frequency in the image</p><p><strong>Nyquist Rate</strong>: minimum sampling rate</p><p>Wagon-wheel effect: in video wheel appear to rotate backwards</p><p>Fix aliasing: <strong>Gaussian pre-filtering</strong></p><p><strong>Gaussian pyramid</strong>: Represent N<em>N image as a pyramid of $1</em>1, 2<em>2, 4</em>4, 2^k*2^k$ (assuming $N=2^k$)</p><h4 id="Up-sampling-Image-Interpolation"><a href="#Up-sampling-Image-Interpolation" class="headerlink" title="Up-sampling / Image Interpolation"></a>Up-sampling / Image Interpolation</h4><p>Correspondingly, making bigger images.</p><ol><li>Nearest-neighbor interpolation (use neighbor value to replace)</li><li>Linear interpolation (use a line to fit the gap)</li><li>Gaussian reconstruction</li><li>Bicubic interpolation</li></ol><h3 id="Feature-Detection"><a href="#Feature-Detection" class="headerlink" title="Feature Detection"></a>Feature Detection</h3><p>Can be used to conduct automatic panoramas.</p><p>Combine two images:</p><ol><li>Extract features</li><li>Match features</li><li>Align images</li></ol><p>Find features that are invariant to transformations:</p><ul><li>Geometric invariance: translation, rotation, scale</li><li>Photometric invariance: brightness, exposure, etc.</li></ul><p><strong>Local Features</strong> refer to a pattern or distinct structure found in an image, such as a point, edge, or small image patch.</p><p>Advantages of local features:</p><ul><li>Locality: robust to occlusion (means some sort of blocking of an object[not sure]) and clutter (lots of objects in the image).</li><li>Quantity: hundreds or thousands in a single image</li><li>Distinctiveness: differentiate a large database of objects</li><li>Efficiency: real-time performance achievable</li></ul><p>Good features - look for image regions that are â€œunusualâ€.</p><h4 id="Harris-Corner-Detection"><a href="#Harris-Corner-Detection" class="headerlink" title="Harris Corner Detection"></a>Harris Corner Detection</h4><p>Consider shifting the window $W$ by $(u,v)$, compute the squared differences (SSD). Look for high SSDs. </p><p>$E(u,v) \approx [ u \space v ] \begin{bmatrix}A &amp; B \ B &amp; C\end{bmatrix} \begin{bmatrix} u \ v \end{bmatrix}$</p><p>$A = \Sigma I_x^2(dI/dx), B = \Sigma I_xI_y, C = \Sigma I_y^2(dI/dy)$</p><p>$E(u,v)$ is locally approximated as a quadratic error function.</p><p>Corner detection summary:</p><ol><li>Compute the gradient at each point in the image.</li><li>Compute the â€œABBCâ€ matrix from the entries in the gradient.</li><li>Compute the eigenvalues.</li><li>Find points with large response ($\lambda_{max}&gt;threshold$).</li><li>Choose these points where $\lambda_{min}$ is a local maximum as features.</li></ol><p>In practice, using a simple window $W$ doesnâ€™t work too well. Instead, weâ€™ll weight each derivative value based on its distance from the center pixel.</p><h4 id="Image-Transformations"><a href="#Image-Transformations" class="headerlink" title="Image Transformations"></a>Image Transformations</h4><p>Geometric: Rotation, Scale</p><p>Photometric: Intensity</p><p>We want corner locations to be invariant to photometric transformations and covariant to geometric transformations.</p><p><strong>Invariance</strong>: Image is transformed and corner locations do not change.</p><p><strong>Covariance</strong>:  If we have two transformed versions of the same image, features should be detected in corresponding locations.</p><p>In <em>Harris Detector</em>:</p><ul><li>Corner location is covariant w.r.t. translation.</li><li>Corner location is covariant w.r.t. rotation.</li><li>Partially invariant to affine intensity change.</li><li>Not invariant to scaling.</li></ul><p><strong>Laplacian of Gaussian</strong> (LoG): Use a Gaussian filter first then Laplacian.</p><p><strong>Difference of Gaussian</strong> (DoG): A Gaussian minus a slightly smaller Gaussian.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Machine Learning | æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Behavioral Economics</title>
      <link href="/2020/01/26/notes-for-behavioral-economics/"/>
      <url>/2020/01/26/notes-for-behavioral-economics/</url>
      
        <content type="html"><![CDATA[<h4 id="Making-a-resolution"><a href="#Making-a-resolution" class="headerlink" title="Making a resolution?"></a>Making a resolution?</h4><p>A resolution may be wrong for one of three main reasons:</p><ul><li>Itâ€™s a resolution created based on what someone else (or society) is telling you to change.</li><li>Itâ€™s too vague.</li><li>You donâ€™t have a realistic plan for achieving your resolution.</li></ul><p>Make <strong>SMART</strong> goals - Specific, Measurable, Achievable, Relevant and Time-bound.</p><p>To figure out how to change a habit, try breaking it down into three parts: a cue, a routine and a reward. Itâ€™s important to know why you do (or canâ€™t do) something so you could control it.</p><details><summary>Example here</summary>Bad Habit: I don't get enough sleep at night.<p>Cue: I feel like I need time to myself in the evening.</p><p>Routine: I stay up too late watching TV.</p><p>Reward: Iâ€™m entertained.</p><p>Way to change the behavior: Instead of staying up late to watch TV, carve out special time each day to spend by yourself, even if that may mean asking for help with your children or taking a break from work each day.</p></details><p>A plan should allow some inevitable situations (slip days, or I would like to call â€œfault toleranceâ€) and prepare for them before they actually show up. Some useful technique include:</p><ul><li>Focus on the small one between what is done and what left.</li><li><strong>Donâ€™t be too positive</strong> because who needs a real achievement if already enjoying a fantastic daydream? Try W.O.O.P.: Wish, Outcome, Obstacle, Plan.</li></ul><p>To keep up the resolution, other things might help are:</p><ul><li>Tell someone(or post on public platform even if nobody reads it) about your plan.</li><li>Make yourself lose something if you fail. Money is a good choice.</li><li>Find a group to get some good peer pressure.</li><li><strong>Do a â€œstress testâ€ before it start to know how feasible it is.</strong></li></ul><h4 id="Cognitive-Biases"><a href="#Cognitive-Biases" class="headerlink" title="Cognitive Biases"></a>Cognitive Biases</h4><p>The picture below has done a great work summarizing this topic. </p><br><div align="center"><img src="https://miro.medium.com/max/3072/1*71TzKnr7bzXU_l_pU6DCNA.jpeg" title="Cognitive Bias Codex"></div><br><p>Some thoughts:</p><ul><li>Will the biases cancel each other under certain circumstances?</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Psychological Fun | å¿ƒç† </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Making Aesthetic Interfaces</title>
      <link href="/2019/11/25/making-aesthetic-interfaces/"/>
      <url>/2019/11/25/making-aesthetic-interfaces/</url>
      
        <content type="html"><![CDATA[<p>Expert designers usually do not solve every problem, instead, they <strong>reuse</strong> solutions that worked before, aka. design patterns. </p><p><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> is always a good reference resource.</p><h3 id="Hierarchy"><a href="#Hierarchy" class="headerlink" title="Hierarchy"></a>Hierarchy</h3><p>Decide the content to present, level of importance for each element and organize them.</p><p><strong>Steps</strong> to implement visual hierarchy: collect -&gt; group -&gt; prioritize.</p><p><strong>Aspects</strong>: scale, color, contrast, alignment, proximity.</p><p><strong>Components</strong>: buttons, cards, lists, tabs, menus, etc.</p><h3 id="Typography"><a href="#Typography" class="headerlink" title="Typography"></a>Typography</h3><h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Two easily mixed up terms:</p><ul><li>A <strong>typeface</strong> is a style of type design which includes a complete scope of characters in all sizes and weight. e.g. Noto Sans SC. Usually could be divided into <strong>Sans-serif</strong> and <strong>Serif</strong>.</li><li>A <strong>font</strong> is a graphical representation of text character usually introduced in one particular typeface, size, and weight. e.g. Multiple files of Noto Sans SC with different suffixes when you download it from Google Fonts.</li></ul><h4 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h4><p>Consider the common small notebooks with four lines for each row used in elementary schoolsâ€¦it will help understand some of the concepts below.</p><ul><li><p>Mean line and baseline</p><p>The two lines in the middle.</p></li><li><p>X-height (x)</p><p>The height between the two lines, which equals to the height of a lowercased <em>x</em>.</p></li><li><p>Ascender and Descender</p><p>The part above the mean line is ascender. Correspondingly, descender is the part below the baseline.</p></li><li><p>Alignment</p><p>Besides left, right and center, thereâ€™s a <strong>justified</strong> alignment for English which could be seen used in newspaper. </p></li><li><p>Line length</p><p>A perfect line length is like row numbers when writing codes.</p></li><li><p>Tracking and Kerning</p><p>Space between letters. The former refers to general ones while the latter refers to specific ones.</p></li><li><p>Leading</p><p>The spacing between the baselines of copy.</p></li><li><p>Indent and Line Space</p><p>Paragraph-wise concepts.</p></li></ul><h4 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h4><ul><li>One typeface for headline and one for body text is usually enough.</li><li>Never use both indent and line space.</li><li>In design, the standard leading is 120% the point size of the font.</li></ul><h3 id="Color-Palettes"><a href="#Color-Palettes" class="headerlink" title="Color Palettes"></a>Color Palettes</h3><h4 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h4><ul><li><strong>Additive / RGB</strong>: use for digital mediums</li><li><strong>Subtractive / CMYK</strong>: use for paint and print</li></ul><h4 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h4><ul><li><p>Chroma</p><p>Purity of a color. A hue with high chroma has no black, white or gray added.</p><p>Use hues with chromas that are either exactly the same or a few steps away from each other on the color wheel.</p></li><li><p>Hue</p><p>Just color, like purple, blue, etc.</p></li><li><p>Saturation</p><p>How a hue appears under particular lighting conditions.</p></li><li><p>Value</p><p>How light or dark a color is.</p></li></ul><h4 id="Schemes"><a href="#Schemes" class="headerlink" title="Schemes"></a>Schemes</h4><ul><li><p>Monochromatic</p><p>Different shades of a specific hue.</p><p>Hard to make mistakes.</p><p>Adding a strong neutral (e.g. black or white) could avoid boredom.</p></li><li><p>Analogous</p><p>Use colors located right next to each other on the color wheel.</p></li><li><p>Complementary</p><p>Combine colors from the opposite site of the color wheel.</p><p>Provides high contrast.</p></li><li><p>Triadic</p><p>Based on three separate colors which are equidistant on the color wheel.</p></li></ul><br><br><p><em>Ref:</em></p><p><a href="https://designshack.net/articles/ux-design/google-material-design-everything-you-need-to-know/" target="_blank" rel="noopener">https://designshack.net/articles/ux-design/google-material-design-everything-you-need-to-know/</a></p><p><a href="https://uxplanet.org/typography-in-ui-guide-for-beginners-7ee9bdbc4833" target="_blank" rel="noopener">https://uxplanet.org/typography-in-ui-guide-for-beginners-7ee9bdbc4833</a></p><p><a href="https://uxplanet.org/color-theory-brief-guide-for-designers-76e11c57eaa" target="_blank" rel="noopener">https://uxplanet.org/color-theory-brief-guide-for-designers-76e11c57eaa</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | è®¾è®¡ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> UI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithms and Data Structures</title>
      <link href="/2019/11/11/algorithms-and-data-structures/"/>
      <url>/2019/11/11/algorithms-and-data-structures/</url>
      
        <content type="html"><![CDATA[<h2 id="Theories-and-Methods"><a href="#Theories-and-Methods" class="headerlink" title="Theories and Methods"></a>Theories and Methods</h2><h3 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h3><h4 id="Situation"><a href="#Situation" class="headerlink" title="Situation"></a>Situation</h4><p>A duplicate process, overlapping subproblems</p><p>Recursion in brute force, optimal substructure</p><h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul><li>Memorization</li><li>Table filling</li></ul><h3 id="Other-Frequently-Encountered-Skills"><a href="#Other-Frequently-Encountered-Skills" class="headerlink" title="Other Frequently Encountered Skills"></a>Other Frequently Encountered Skills</h3><h4 id="â€œWalkerâ€-and-â€œrunnerâ€-Solution"><a href="#â€œWalkerâ€-and-â€œrunnerâ€-Solution" class="headerlink" title="â€œWalkerâ€ and â€œrunnerâ€ Solution"></a>â€œWalkerâ€ and â€œrunnerâ€ Solution</h4><p>The former move one step every time while the latter move two. Use to detect cycles.</p><h4 id="Prefix-Sums"><a href="#Prefix-Sums" class="headerlink" title="Prefix Sums"></a>Prefix Sums</h4><p>A new array in which <code>new[i] = sum(old[:i])</code>. Take O(n) to construct, but can make it constant time for the following operations.</p><p>e.g. Find subarrays of $k$ elements that has average value more than $t$.</p><h2 id="Examples-of-Problem-Solution"><a href="#Examples-of-Problem-Solution" class="headerlink" title="Examples of Problem/Solution"></a>Examples of Problem/Solution</h2><ul><li>Find the $N^{th}$ fibonacci number. [Dynamic Programming]</li><li>Given two strings s and t, find the longest subsequence (same order but can skip some letters) common to both strings. [Dynamic Programming]</li><li>Given array $a$ containing integers $[x_1, â€¦, x_n]$, find integers $i, j$ such that $1 â‰¤ i â‰¤ j â‰¤ n$ and $\Sigma^{j}_{i}a$ is maximal. [Dynamic Programming]</li></ul><h2 id="Application-Reflex"><a href="#Application-Reflex" class="headerlink" title="Application Reflex"></a>Application Reflex</h2><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li>Usually recursion takes more space and runs faster, while the â€œforâ€ loop is the opposite.</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding | ç¼–ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prototypes</title>
      <link href="/2019/11/10/prototypes/"/>
      <url>/2019/11/10/prototypes/</url>
      
        <content type="html"><![CDATA[<p>There are several things people can do to test out a product. In this post I combine prototypes together with personas and storytelling, but in fact they are NOT really part of prototypes.</p><h3 id="Designers-Oriented"><a href="#Designers-Oriented" class="headerlink" title="Designers Oriented"></a>Designers Oriented</h3><p>Methods that donâ€™t necessarily involve users.</p><h4 id="Personas"><a href="#Personas" class="headerlink" title="Personas"></a>Personas</h4><p>A fictional character that is meant to represent a group of users that share common goals, attitudes and behaviors when interacting with a particular product or service.</p><p>Steps:</p><ol><li>Collect data of users.</li><li>Segment the users.</li><li>Create personas.</li></ol><p>Personas can help designers have <strong>empathy</strong>.</p><p>Question from myself: how do you decide whether a persona is logical or realistic?</p><p>Itâ€™s easy to find online tools to help you design a persona.</p><h4 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h4><p>Written accounts and narratives of an experience. Imagining a particular situation personas encounter.</p><p>Components: motivation, context, distractions, goal</p><h4 id="Storyboarding"><a href="#Storyboarding" class="headerlink" title="Storyboarding"></a>Storyboarding</h4><p>Illustrations(drawings) that represent a story. Hereâ€™s a storyboard I drew in class:</p><br><div align="center"><img src="https://od.lk/s/MzBfMTgwMDQxMDFf/storyboard.jpg" width="50%" height="50%" title="My Drawing of Storyboard"></div><p>Itâ€™s about a situation where people have issues when using smart home devices.</p><p>Compared with actually â€œtellingâ€ a story, itâ€™s more direct and could convey more information with less content (save paper <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">ğŸ˜‰</span>).</p><p>Some rules:</p><ul><li>Only include necessary details. </li><li>Be proud to be a terrible painter.</li><li>The number of frames should be around six.</li></ul><h3 id="Users-Oriented"><a href="#Users-Oriented" class="headerlink" title="Users Oriented"></a>Users Oriented</h3><p>Requires the engagement of users.</p><h4 id="Prototype"><a href="#Prototype" class="headerlink" title="Prototype"></a>Prototype</h4><p>Simulate the design with low cost. Could be done iteratively during design.</p><p>Two ways of prototyping:</p><ul><li>Vertical: show only part of the interface, but very detailed.</li><li>Horizontal: show many aspects of the interface in a shallow manner.</li></ul><h4 id="Low-Fidelity-Paper-Prototype"><a href="#Low-Fidelity-Paper-Prototype" class="headerlink" title="Low Fidelity: Paper Prototype"></a>Low Fidelity: Paper Prototype</h4><p>Although it sounds more complicated, in fact itâ€™s <strong>much easier and faster</strong> to implement compared with prototyping with fancy tools, even if youâ€™re a terrible painter.</p><p>It helps designers and users <strong>focus on big things</strong>, e.g. interface logics, instead of spending hours trying to choose a perfect font (thatâ€™s what I usually didâ€¦)</p><p>You are more likely to get â€œhonestâ€ feedback with paper prototype because, people will hesitate to say something negative if they realize you have put a large amount of work into it.</p><p><em>*Generally it kind of looks like the expensive and fancy cards that were popular among primary school students.</em></p><h4 id="High-Fidelity-Digital-Prototype"><a href="#High-Fidelity-Digital-Prototype" class="headerlink" title="High Fidelity: Digital Prototype"></a>High Fidelity: Digital Prototype</h4><p>Tools: Figma, Invision, Sketch, and my favorite - <strong>Proto.io</strong>.</p><p>A website that helps with choosing tools: <a href="http://www.prototypr.io/prototyping-tools/" target="_blank" rel="noopener">http://www.prototypr.io/prototyping-tools/</a></p><p>For details in digital prototyping like hierarchy, typography, colors, etc. please refer to another post.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | è®¾è®¡ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> UX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thoughts from Contextual Inquiry</title>
      <link href="/2019/11/09/thoughts-from-contextual-inquiry/"/>
      <url>/2019/11/09/thoughts-from-contextual-inquiry/</url>
      
        <content type="html"><![CDATA[<h4 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h4><p>Contextual inquiry is a qualitative research method. The researcher will give participants a topic to do something, then observe and ask probing questions.</p><h4 id="More-Explanation"><a href="#More-Explanation" class="headerlink" title="More Explanation"></a>More Explanation</h4><ul><li>It takes place in the context of use, e.g. where (home/work)  people will use your app product.</li><li>The purpose is to understand why people do certain things and enhance user experience.</li><li>Itâ€™s a very â€œfreeâ€ research method. </li></ul><h4 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Use â€œparticipantâ€ instead of â€œuserâ€ or â€œsubjectâ€ in the research to connote an active role.</li><li>Introduce the research method to participants and make sure they know that itâ€™s the product that is being tested.</li><li>Explain your conclusions and interpretations to participants through out the interview to avoid any misunderstanding.</li></ul><h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul><li>Start data analysis ASAP.</li><li>Contextual inquiry can be conducted in any stage of design.</li></ul><h4 id="My-Thoughts"><a href="#My-Thoughts" class="headerlink" title="My Thoughts"></a>My Thoughts</h4><p>When you ask the participants why they do something in a contextual inquiry, they might realize things that they never noticed before. So does <strong>psychology</strong>. </p><p>Just have a friend keep asking questions when describing experiences or stating points can be really helpful. When people thinking by themselves, itâ€™s easy for them to â€œfall into routinesâ€ and take something for granted. Even if they try to remind themselves from time to time, itâ€™s still not as good as a different perspective.</p><p>It can also be applied to <strong>science</strong>, at least in the past, when no one had been curious enough about the reasons behind falling apples. In general, â€œwhyâ€ questions can be annoying, but lose the ability to wonderâ€¦it seems way more worse to me.</p><br><br><p><em>Ref:</em></p><p><a href="https://www.interaction-design.org/literature/article/contextual-interviews-and-how-to-handle-them" target="_blank" rel="noopener">https://www.interaction-design.org/literature/article/contextual-interviews-and-how-to-handle-them</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | è®¾è®¡ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>åµŒäºçª—é—´çš„æ¡¥è¾‰</title>
      <link href="/2019/10/30/qian-yu-chuang-jian-de-qiao-hui/"/>
      <url>/2019/10/30/qian-yu-chuang-jian-de-qiao-hui/</url>
      
        <content type="html"><![CDATA[<p>å‡Œæ™¨äº”ç‚¹é’Ÿæ˜¯Queensboroughå¤§æ¡¥æ¡¥æ¶ä¸Šçš„ç¯å…‰å‡†æ—¶ç†„ç­çš„æ—¶é—´ã€‚</p><p>â€”â€”è¿™æ˜¯æˆ‘åœ¨ä»Šå¤©çŸ¥é“çš„æ–°ä¿¡æ¯ã€‚æˆ‘ç”šè‡³è¿˜ä¸ºæ­¤å‘å¸ƒäº†ä¸€æ¡æœ‹å‹åœˆï¼Œä»¿ä½›ä¸€ä¸ªå›°æ‰°è®¸ä¹…çš„è°œé¢˜å¾—åˆ°äº†è§£ç­”ï¼Œè€Œæˆ‘åœ¨æŠŠå®ƒå…¬è¯¸äºä¸–ã€‚</p><p>å®é™…ä¸Šä¹Ÿå¹¶ä¸æ˜¯æ²¡æœ‰è¿™ä¹ˆè¯´çš„é“ç†ã€‚åœ¨è®¸å¤šä¸ªå¤œæ™šï¼Œæˆ‘åœ¨é»‘æš—çš„æˆ¿é—´é‡Œå±¡æ¬¡çå¼€åŒçœ¼ï¼Œæœ‰æ—¶ä¼šçœ‹åˆ°æ˜ åœ¨çª—ä¸Šçš„é‚£æ˜Ÿåº§ä¸€èˆ¬çš„å…‰ç‚¹é›†åˆï¼Œæœ‰æ—¶åˆ™åªæœ‰å·æµä¸æ¯çš„è½¦ã€‚ç„¶åæˆ‘ä¾¿å¸¦ç€å¯¹æ—¶é—´çš„å¥½å¥‡è¾—è½¬åä¾§ç›´åˆ°å†æ¬¡ç¡å»ã€‚</p><p>å‡Œæ™¨äº”ç‚¹é’Ÿã€‚æ‰€è°“çš„â€œæ˜å¤©â€çœŸæ­£åˆ°æ¥çš„æ—¶åˆ»ã€‚</p><p>å¶å°”æˆ‘ä¼šæƒ³è¦å‘åˆ«äººè®²èµ·ç±»ä¼¼è¿™æ ·æ¯«æ— æ„ä¹‰çš„æ•…äº‹ï¼Œå…³äºæˆ‘æˆ¿é—´çš„çª—æˆ·æ˜¯å¤šä¹ˆåƒé‚£ç§å±•è§ˆä¼šå°ºå¯¸å¤§å°çš„ç”»æ¡†ï¼Œè€Œæˆ‘èƒ½çœ‹åˆ°çš„é£æ™¯æ„æˆäº†å¦‚åŒå“ˆåˆ©Â·æ³¢ç‰¹çš„é­”æ³•ä¸–ç•Œé‡Œä¼šåˆŠç™»åœ¨æŠ¥çº¸ä¸Šçš„åŠ¨æ€å›¾ç‰‡ï¼›å…³äºæ¡¥ä¸Šçš„è½¦æµæ˜¯æ€æ ·ä»ä¸åœæ­‡ã€æ¯æ—¶æ¯åˆ»éƒ½æ˜¯é‚£ä¹ˆå¿™ç¢Œï¼Œå³ä½¿æ˜¯åœ¨ç”µè§†èŠ‚ç›®éƒ½ä¼šåœæ’­çš„æ—¶æ®µã€‚ä½†æ˜¯å¤œè‰²åœ¨ä¿æŠ¤ç€æˆ‘çš„åŒæ—¶ä¹Ÿéš”ç»ç€æˆ‘ï¼Œäºæ˜¯æˆ‘é€æ¸å­¦ä¼šé€‚æ—¶åœ°ä¿æŒæ²‰é»˜ã€‚</p><p>å¦‚æœæ˜Ÿè¾°æ˜¯ç¯å¡”â€¦â€¦é‚£è¿™äº›ä¸€å›¢å›¢æš–è‰²çš„å…‰è¾‰ï¼Œä¹Ÿå¯ä»¥æ˜¯å°å°çš„ç¯å¡”å—ï¼Ÿ</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> åªè¨€ç‰‡è¯­ </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Become Pythonic</title>
      <link href="/2019/10/29/notes-for-python-3/"/>
      <url>/2019/10/29/notes-for-python-3/</url>
      
        <content type="html"><![CDATA[<p><em>Note: This post doesnâ€™t include detailed theories. For those, please refer to other posts.</em></p><h3 id="Built-in"><a href="#Built-in" class="headerlink" title="Built-in"></a>Built-in</h3><h4 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h4><ul><li><code>int()</code> returns the lower bound.</li><li>Pay attention to the difference between <code>extend()</code> and <code>append()</code>, e.g. [1,2,4] vs. [[1,2],[4]].</li></ul><h4 id="Easy-Ways"><a href="#Easy-Ways" class="headerlink" title="Easy Ways"></a>Easy Ways</h4><ul><li>Remember to use lambda for simple functions and use <code>[x for x in statement if statement]</code> for iterating.</li><li>Python has a dictionary <code>defaultdict()</code>, it provides default value given by the user. <code>collections.counter()</code> could be a substitute if the default value is $0$. </li></ul><h4 id="Possible-Mistakes"><a href="#Possible-Mistakes" class="headerlink" title="Possible Mistakes"></a>Possible Mistakes</h4><ul><li><code>list.append()</code> will only return <strong>None</strong>, which indicates it has successfully complete the process. Use <code>list</code> to get to the changes.</li><li>Things like a <code>range(0,n)</code> do NOT include n but do include 0.</li><li>If let <code>a = b</code> where a,b are both lists, when you change a it will affect b too. Try <code>copy.deepcopy</code>(needs import) or <code>new = old[:]</code>.</li></ul><h4 id="Other-in-Doc"><a href="#Other-in-Doc" class="headerlink" title="Other in Doc"></a>Other in Doc</h4><ul><li>The time complexity for <code>sort()</code> is O(nlogn).</li></ul><hr><h3 id="Useful-Packages"><a href="#Useful-Packages" class="headerlink" title="Useful Packages"></a>Useful Packages</h3><h4 id="Package-List"><a href="#Package-List" class="headerlink" title="Package List"></a>Package List</h4><ul><li>*â€nltkâ€* is a useful package for natural language processing. The functions include tokenize (divide a sentence into words), strip stop words and punctuations, lemmatize (convert different forms of a word into one, e.g. run, ran, running, runs â†’ run).</li><li>The famous *â€sklearnâ€*.</li></ul><h4 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h4><ul><li>Do not iterate through rows of a <em>pandas</em> dataframe unless really necessary. Usually, just define a function and use <code>.apply(func)</code> instead. No parameter needed when calling.</li><li>If want to make changes to an existing <em>pandas</em> dataframe, remember to write <code>inplace = True</code>.</li><li>Combine <em>pandas</em> dataframes: <code>pandas.concat()</code></li><li><code>pandas.read_csv()</code> is good for loading a data file. The â€œcsvâ€ could be replaced with other type of file.</li><li>A <em>pandas</em> object could be converted to a numpy array using <code>to_numpy()</code>. </li></ul><h4 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h4><ul><li>Use <code>np.zeros()</code> to create a n-d array with zeros as initial values.</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Coding | ç¼–ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> updating </tag>
            
            <tag> notes </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Common Research Methods</title>
      <link href="/2019/10/15/common-research-methods/"/>
      <url>/2019/10/15/common-research-methods/</url>
      
        <content type="html"><![CDATA[<h3 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a><strong>Surveys</strong></h3><p>Sort of quantitative methods.</p><h4 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Identify topics needed to be covered.</li><li>Pretest drafts.</li><li>Cross-sectional: different people in same population at multiple points in time.</li><li>Panel / longitudinal: same people over time.</li><li>Use open-ended questions in pilot tests and then use the answers to form close-ended ones.</li></ul><h4 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h4><ul><li>Use the same question wording and be careful about where the question is asked.</li><li>The number of choices should be a small number â€“ 4 or 5 usually.</li><li>Randomize the order of options to reduce potential bias, except for ordinal ones (e.g. from â€œmostâ€ to â€œleastâ€).</li><li>Ask <strong>clear</strong> and <strong>specific</strong> questions. Use simple and concrete language.</li><li>Only ask <strong>1</strong> question at a time. Donâ€™t indicate two or more things.</li><li>Be careful not to use biased or potentially offensive to certain respondents. (e.g. biological sex)</li><li>Questions should be grouped and asked in a logical order.</li></ul><h4 id="Fun-Facts"><a href="#Fun-Facts" class="headerlink" title="Fun Facts"></a>Fun Facts</h4><ul><li>People tend to choose the options they heard later in a list (recency effect).</li><li>Compared with the better educated and better informed, less educated and less informed respondents have a greater tendency to agree with certain statements.</li><li>People have a natural tendency to want to be accepted and liked, and this may lead people to provide inaccurate answers to questions that deal with sensitive subjects. </li></ul><h3 id="Interviews"><a href="#Interviews" class="headerlink" title="Interviews"></a><strong>Interviews</strong></h3><p>Qualitative research method.</p><h4 id="Strategy-1"><a href="#Strategy-1" class="headerlink" title="Strategy"></a>Strategy</h4><ul><li>Write a general interview guide.</li><li>Modify questions if necessary after each interview.</li><li>Explain the purpose, format, length etc. of the interview. Let the participants ask questions before start the interview.</li><li><strong>Never</strong> count on memories to recall the answers. </li><li>Encourage the participants with nodding, etc.</li><li>Careful with reactions (act surprised or take notes suddenly may discourage the participants), tolerate pauses, show you are listening.</li><li>Stay focused on the topic, donâ€™t lose control of the interview.</li><li>Begin with a warm-up question â€“ simple and easy to answer.</li><li>The last question should provide some closure for the interview.</li></ul><h4 id="Rules-1"><a href="#Rules-1" class="headerlink" title="Rules"></a>Rules</h4><ul><li>When thereâ€™re multiple interviewers, make sure to use the same wordings.</li><li>Be careful to ask â€œwhyâ€ questions. Could be hard to answer or, just, annoying. Ask â€œhowâ€ instead.</li><li>Never answer a question for the respondent.</li><li>Ask questions that could elicit long answers.</li><li>Donâ€™t ask about general things and let the respondent answer on behalf of a group. </li><li>Complete the scripts right after the interview. Donâ€™t wait for a long time.</li></ul><br><br><p><em>Ref:</em></p><p><em><a href="https://www.pewresearch.org/methods/u-s-survey-research/questionnaire-design/" target="_blank" rel="noopener">https://www.pewresearch.org/methods/u-s-survey-research/questionnaire-design/</a></em></p><p><em><a href="http://nixdell.com/classes/HCI-and-Design-Spring-2017/Qualitative-Interview-Design.pdf" target="_blank" rel="noopener">http://nixdell.com/classes/HCI-and-Design-Spring-2017/Qualitative-Interview-Design.pdf</a></em></p><p><em><a href="http://nixdell.com/classes/HCI-and-Design-Spring-2017/interview-strategies.pdf" target="_blank" rel="noopener">http://nixdell.com/classes/HCI-and-Design-Spring-2017/interview-strategies.pdf</a></em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Design | è®¾è®¡ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> notes </tag>
            
            <tag> CT courses </tag>
            
            <tag> research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presentation in Freestyle</title>
      <link href="/2019/09/27/presentation-in-freestyle/"/>
      <url>/2019/09/27/presentation-in-freestyle/</url>
      
        <content type="html"><![CDATA[<p>  Today I took part in a workshop related to presentation skills as a Studio session. Personally, when giving presentations, thereâ€™s one thing that has always been difficult for me. Although I could remember all the content (letâ€™s say I could<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8">ğŸ˜</span>) and present in front of people with some kind of confidence, somehow it feels like Iâ€™m just doing a perfect job in reciting or remembering all the gestures instead of really trying to convey or explain something I know about, even if it was created completely by myself. What I want to be is a presenter in freestyle â€“ well, like dancing to the music without choreography<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8">ğŸ˜‰</span>.</p><p>  Ambitions:</p><ul><li><p>Actually pay attention to the audience. Donâ€™t see them as a group (background image).</p></li><li><p>Walk around and use body language naturally, in contrast to imitating a robot. </p></li><li><p>Humorous, at least 80% of what I will show in daily life.</p></li><li><p>Handle different kinds of situations without too much struggle. Like a comedian.</p><p>So, with these ambitions, and also a bit of sleepiness<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f612.png?v8">ğŸ˜’</span>, I went to the workshop and learned:</p></li><li><p><strong>Eye contact</strong> is very important. Also make sure to look at people that not in the center.</p></li><li><p>Must have a strong and clear <strong>ending</strong> (not the kind of â€˜thatâ€™s allâ€™).</p><p>Ummmâ€¦okay, it was a really good workshop, and I definitely learned more than 2 things. The rest is just all combined with my own thoughts, such as:</p></li><li><p>Personally: keep my hands away from my hair. Just try to hold something, like a chair or a pen. (Otherwise I will need a shave to fix this habit<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8">ğŸ˜</span>)</p></li><li><p>Donâ€™t let the audience lose focus for a long period of time(30 seconds perhaps?). Example: write something on the board, deal with slides.</p></li><li><p>Suppose that you could memorize all the content and you just want to practice your presentation style. An interesting way I came up with: present with another language that your audience donâ€™t understand at all, or just ramble words that make no sense, or remain silence, while <strong>you still have a topic to convey</strong>. Kind of like the game to guess a word. I think this will help you focus on the interaction and connection.</p></li><li><p>Sometimes be completely honest with the audience will help a lot. I mean, suppose you made a mistake during a lecture, if you donâ€™t have the ability to fix it or act as nothing happened immediately yet, just show them how you are speaking to yourself â€œuh-oh.â€ Maybe some non-related explanation: I played video games until 4 this morning so donâ€™t expect me to know what Iâ€™m talking about. But definitely not too much so you will still be on the content.</p><p>Okay, thatâ€™s pretty much it. Iâ€™m running to do my AML homework now.</p><p>Not a bad article for my first postâ€¦I guess?</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Soft Skills | ç»¼åˆèƒ½åŠ› </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CT courses </tag>
            
            <tag> presentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>References and Copyrights</title>
      <link href="/2019/09/24/references-and-copyrights/"/>
      <url>/2019/09/24/references-and-copyrights/</url>
      
        <content type="html"><![CDATA[<h3 id="REGULATIONS"><a href="#REGULATIONS" class="headerlink" title=" REGULATIONS "></a><font color="#7cd175"> REGULATIONS </font></h3><p>Similar to academic integrity, <font color="#b39ddb"><strong><em>DoveCode</em></strong></font> respects and protects all kinds of intellectual property rights. Most content in this website follows <em>Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</em>, and any exception will be stated. </p><p>Reprint/reproduction is welcomed as long as under proper citation. If you would like to â€œcloneâ€ something but couldnâ€™t get the access to it, please contact <a href="https://FadingWinds.me/about" target="_blank" rel="noopener">@FadingWinds</a> and I would love to help. Also, it is very encouraged to point out any mistakes or missing points you notice. </p><h3 id="REFERENCES"><a href="#REFERENCES" class="headerlink" title=" REFERENCES "></a><font color="#7cd175"> REFERENCES </font></h3><ol><li><p><font color="#b39ddb"><strong><em>DoveCode</em></strong></font> is based on <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> and uses <a href="https://github.com/blinkfox/hexo-theme-matery/" target="_blank" rel="noopener">hexo-theme-matery</a> as the theme. Currently all the functions and plugins are derived from them, so references like these wonâ€™t be listed here right now. Please visit their link to see the details.</p></li><li><p>The banner picture comes from <strong>Cytus II</strong> (A game produce by <a href="https://www.rayark.com/" target="_blank" rel="noopener">Rayark</a>). </p> <br> <div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzdf/Banner.png" width="50%" height="50%" title="Banner Picture"></div></li><li><p>Sorry it was unable for me to trace the resource of the websiteâ€™s logo (which is also my avatar image on Github). I would be grateful if anyone happens to know and could provide it to me. </p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzhf/logo.png" width="10%" title="Logo"></div></li><li><p>The icon of <strong>*<font color="#b39ddb">DoveCode</font>*</strong> comes from <a href="https://icons8.com/icons" target="_blank" rel="noopener">Icons8</a>.</p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwNzlf/favicon.png" title="Icon"></div></li></ol><h3 id="COPYRIGHTS"><a href="#COPYRIGHTS" class="headerlink" title=" COPYRIGHTS "></a><font color="#7cd175"> COPYRIGHTS </font></h3><ol><li><p>All the posts are original on <strong>*<font color="#b39ddb">DoveCode</font>*</strong> and any quotes/references included will be listed in or at the end of the article.</p></li><li><p>There are a set of cover images that will be randomly assigned to an article if it doesnâ€™t have a specific one. Anyone is welcomed to submit their own photographs. The current set is as follows.</p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxNjM1NTdf/photos.png" title="Photographed by FadingWinds"></div><br><div align="center"><img src="https://od.lk/s/MzBfMTc0NzM0OTZf/Em..png" title="Photographed by Emmanuel Li"></div></li><li><p>The avatar image on the â€œAboutâ€ page is drawn by <strong><em>FadingWinds</em></strong>. </p><br><div align="center"><img src="https://od.lk/s/MzBfMTcxMTQwODFf/NeverAnAngel.jpg" width="30%" title="Drawn by FadingWinds"></div></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> updating </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
