<!DOCTYPE HTML>
<html lang="default">


<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta name="keywords" content="Natural Language Processing, FadingWinds">
    <meta name="description" content="[TOC]
Overview
Three type of models:

Generative Models
Discriminative Models, e.g. neural networks
Graphical Models

Th">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Natural Language Processing | DoveCode</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DoveCode</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>Index</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>Tags</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>Categories</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>Archives</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>About</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <i class="fa fa-comments"></i>
            
            <span>Contact</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>Friends</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="Search"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DoveCode</div>
        <div class="logo-desc">
            
            Everything could be a story to write, and every story has a whole world inside. UPDATE - FINALLY FIXED A PLUGIN ISSUE AND CAN WORK ON WEBSITE AGAIN!
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                Tags
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                Categories
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                Archives
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                About
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-comments"></i>
                
                Contact
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                Friends
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/fadingwinds" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/fadingwinds" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>


<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('Please enter the passcode | 请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('Sorry, the passcode you entered is incorrect. | 抱歉，您所输入的密码有误。');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        Natural Language Processing
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/notes/" target="_blank">
                                <span class="chip bg-color">notes</span>
                            </a>
                        
                            <a href="/tags/CT-courses/" target="_blank">
                                <span class="chip bg-color">CT courses</span>
                            </a>
                        
                            <a href="/tags/updating/" target="_blank">
                                <span class="chip bg-color">updating</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Data-Science-·-数据科学/" class="post-category" target="_blank">
                                Data Science · 数据科学
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-04-16
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                        4k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                        25 Min
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>[TOC]</p>
<h2 id="overview">Overview</h2>
<p>Three type of models:</p>
<ul>
<li>Generative Models</li>
<li>Discriminative Models, e.g. neural networks</li>
<li>Graphical Models</li>
</ul>
<p>The fundamental goal of NLP is to have <strong>deep understanding of broad language</strong>.</p>
<p>End systems that we want to build:</p>
<ul>
<li><strong>Simple</strong>: spelling correction, text categorization, ...</li>
<li><strong>Complex</strong>: speech recognition, machine translation, information extraction, dialog interfaces, question answering, ...</li>
<li>Unknown: human-level comprehension (probably not just NLP)</li>
</ul>
<p><strong>Key problems</strong> in NLP:</p>
<ol type="1">
<li><p>Ambiguity</p>
<ul>
<li><p>Syntactic ambiguity</p>
<p>e.g. <em>Stolen Painting Found by Tree</em></p>
<p>In state-of-the-art ML, can reach ~95% accuracy for many languages when given many training examples</p></li>
<li><p>Semantic ambiguity</p>
<p>e.g. <em>Siri, call me an ambulance</em></p></li>
</ul></li>
<li><p>Scale</p></li>
<li><p>Sparsity</p>
<p><strong>Corpus</strong> is a collection of text. Often annotated in some way.</p></li>
</ol>
<h3 id="meta-nlp">Meta NLP</h3>
<details>
<p></p><summary> This section is about steps to conduct a NLP research. Unfold to see details. </summary><p></p>
<h4 id="literature-review-should-be-done-early">Literature review: should be done early</h4>
<ul>
<li>Avoid re-invent the wheel</li>
<li>Learn about common tricks, resources, and libraries</li>
</ul>
<ol type="1">
<li>Do a keyword search on <em>Google Scholar</em>, <em>Semantic Scholar</em>, or <em>the ACL Anthology</em>.</li>
<li>Download the papers that seem most relevant.</li>
<li>Skim the abstracts, intros, and previous work sections.</li>
<li>Identify papers that look relevant, appear often, or have lots of citations on Google Scholar, and download them. Then repeat.</li>
</ol>
<p>Places to find the most trustworthy papers:</p>
<ul>
<li><em>NLP</em>: Proceedings of ACL conferences (ACL, NAACL, EACL, EMNLP, CoNLL, LREC), Journal of Computational Linguistics, TACL, COLING, arXiv*</li>
<li><em>Machine Learning/AI</em>: Proceedings of NIPS, ICML, ICLR, AAAI, IJCAI, and arXiv*</li>
<li><em>Computational Linguistics</em>: Journals like Linguistic Inquiry, NLLT, Semantics and Pragmatics</li>
</ul>
<p>What to mention in literature review:</p>
<ul>
<li>General problem/task definition</li>
<li>Relevant methods and results</li>
<li>Comparisons with your work and other related work</li>
<li>Open issues</li>
</ul>
<h4 id="acquiring-datasets">Acquiring Datasets</h4>
<ul>
<li><p>Existing datasets</p>
<p>ACL anthology, Linguistic Data Consortium (LDC), Look for datasheets when available (e.g. QuAC)</p></li>
<li><p>Wild datasets</p>
<p>e.g. Ubuntu Dialogue Corpus, StackOverflow Data (warning: easy to violate copyright and terms of service)</p></li>
<li><p>Build own datasets</p>
<p>Either write detailed guidelines, and work with experts; or write simple guidelines, and crowdsource</p></li>
<li><p>Generate datasets</p>
<p>It's super easy, but artificial data does not reflect the real world.</p></li>
</ul>
<h4 id="quantitative-evaluation">Quantitative Evaluation</h4>
<ol type="1">
<li><p>Follow prior work and use existing metrics</p>
<ul>
<li>If it's a new task, create a metric before you start testing. It must be independent of your model.</li>
</ul></li>
<li><p>Use ablations to study the effectiveness of your choices (and don’t adopt fancy solutions that don’t really help)</p>
<ul>
<li>e.g. MLP sentiment classifier with GloVe embeddings, MLP sentiment classifier with random embeddings, MaxEnt classifier with GloVe embeddings, MaxEnt classifier with random embeddings</li>
</ul></li>
<li><p>Consider controlled human evaluation when standard, and even when less standard</p>
<ul>
<li>e.g. summarization, machine translation, generation</li>
</ul></li>
<li><p>Test for statistical significance when differences are small and models are complex</p></li>
<li><p>Consider extrinsic evaluation on <em>downstream tasks</em> (what the field calls those supervised-learning tasks that utilize a pre-trained model or component)</p></li>
<li><p>Negative results are also important.</p></li>
</ol>
<h4 id="qualitative-evaluation-and-error-analysis">Qualitative Evaluation and Error Analysis</h4>
<p>Goal: convince that your hypothesis is correct.</p>
<p>Interesting hypotheses are often hard to evaluate with standard/intuitive quantitative metrics.</p>
<p>Start with:</p>
<ul>
<li>Look to prior work</li>
<li>Show examples of system output</li>
<li>Identify qualitative categories of system error and count them</li>
<li>Visualize your embedding spaces with tools like t-SNE or PCA</li>
<li>Visualize your hidden states with tools like LSTMVis</li>
<li>Plot how your model performance varies with amount of data</li>
<li>Build an online demo if ambitious (which I'm probably not <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png?v8">🤔</span>)</li>
</ul>
<p><strong>Formative evaluation</strong>: guiding further investigations - Typically: lightweight, automatic, intrinsic - Compare design option A to option B - Tune <em>hyperparameters</em> (parameter whose value is set before the learning process begins): smoothing, weighting, learning rate</p>
<p><strong>Summative evaluation</strong>: reporting results - Compare your approach to previous approaches - Compare different major variants of your approach - Generally only bother with human or extrinsic evaluations here</p>
<p><em>Common mistake: Don’t save all your qualitative evaluation for the summative evaluation.</em></p>
<h4 id="hyperparameter-tuning">Hyperparameter Tuning</h4>
<ul>
<li>Must tune the hyperparameters of your baselines just as thoroughly as you tune them for any new model you propose</li>
<li>Failure to do this invalidates your comparisons</li>
<li>As always, don't tune on test set.</li>
<li>Read the fine print while you’re doing your literature review to get a sense of what hyperparameters to worry about and what values to expect.</li>
<li>If you’re not sure whether to tune a hyperparameter, you probably should.</li>
</ul>
<p>Methods:</p>
<ul>
<li>Grid search: Inefficient (but common)</li>
<li>Bayesian optimization: Optimal, but public packages aren’t great.</li>
<li>Good read: random search - easy, and near-optimal
<ul>
<li>Define distributions over all your hyperparameters.</li>
<li>Sample N times for N experiments.</li>
<li>Look for patterns in your results.</li>
<li>Adjust the distributions and repeat until you run out of resources or performance stops improving.</li>
</ul></li>
</ul>
<h4 id="other">Other</h4>
<p><strong>Biases</strong></p>
<p>Deploying biased models in the wrong places can lead to harms far worse than bad user experiences.</p>
<p>Model de-biasing can be complex, political, and maybe even impossible to do fully, and it may harm performance on reasonable metrics.</p>
<details>
<p></p><summary>Unfold to see a bias example</summary><p></p>
<p>In training data, women appear in cooking scenes 33% more often than men.</p>
In model’s labeling of similar test data, women are detected in cooking scenes 68% more often than men.
</details>
<p><strong>Talk about data</strong></p>
<ul>
<li>What your data looks like, why it was collected, and what kind of information your system learn from it</li>
<li>Who (country, region, gender, native language, etc.) produced the text and labels in your dataset</li>
<li>Any known biases in your dataset (including the obvious ones)</li>
</ul>
</details>
<h3 id="computation-graphs">Computation Graphs</h3>
<p>Computation graphs are the descriptive language of deep learning models.</p>
<p>Functional description of the required computation.</p>
<p>Can be instantiated to do two types of computation: forward and backward computation.</p>
<h4 id="structure">Structure</h4>
<ul>
<li>A <em>node</em> is a {tensor, matrix, vector, scalar} value</li>
<li>An <em>edge</em> represents a function argument (and also data dependency).</li>
<li>A node with an incoming edge is a <em>function</em> of that edge’s tail node.</li>
<li>A node knows how to compute its value and the value of its derivative w.r.t each argument (edge) times a derivative of an arbitrary input <span class="math inline">\frac{dF}{df(u)}</span>.</li>
</ul>
<h4 id="algorithms">Algorithms</h4>
<p><strong>Forward propagation</strong></p>
<ul>
<li>Loop over nodes in topological order</li>
<li>Compute the value of the node given its inputs</li>
<li>Given my inputs, make a prediction (or compute an "error" with respect to a "target output")</li>
</ul>
<p><strong>Backward propagation</strong></p>
<ul>
<li>Loop over the nodes in reverse topological order starting with a final goal node</li>
<li>Compute derivatives of final goal node value with respect to each edge’s tail node</li>
<li>How does the output change if I make a small change to the inputs?</li>
</ul>
<p><strong>Static declaration</strong></p>
<ul>
<li>Phase 1: define an architecture (maybe with some primitive flow control like loops and conditionals)</li>
<li>Phase 2: run a bunch of data through it to train the model and/or make predictions</li>
</ul>
<p><strong>Dynamic declaration</strong></p>
<ul>
<li>Graph is defined implicitly (e.g., using operator overloading) as the forward computation is executed</li>
</ul>
<p><strong>Batching</strong></p>
<ul>
<li><p>Packing a few examples together has significant computational benefits</p></li>
<li><p>CPU is helpful, but with GPU you get to use all the GPU cores, which is world changing.</p></li>
<li><p>Easy with simple networks, but gets harder as the architecture becomes more complex</p>
<ul>
<li>Complex networks may include different parts with varying length</li>
<li>It is very hard to batch complete examples this way</li>
<li>But you can still batch sub-parts across examples, so you alternate between batched and non-batched computations</li>
</ul></li>
</ul>
<h2 id="text-classification">Text Classification</h2>
<p>Examples of classification problems: spam vs. non-spam, text genre, word sense, etc.</p>
<p><strong>Supervised Learning</strong>:</p>
<ul>
<li>Naïve Bayes</li>
<li>Log-linear models (Maximum Entropy Models)</li>
<li>Weighted linear models and the Perceptron</li>
<li>Neural networks</li>
</ul>
<p>Learning from annotated data problem:</p>
<ul>
<li>Annotation requires specific expertise.</li>
<li>Annotation is expensive.</li>
<li>Data is private and not accessible.</li>
<li>Often difficult to define and be consistent.</li>
</ul>
<p><em>Always think about the data, and how much of it your model needs (even better: think of the data first, model second).</em></p>
<p><strong>Training data, development data and held-out data</strong>: an important tool for estimating generalization is train on one set, evaluate during development on another, and use test data only once.</p>
<p>* My understanding of dev vs. test: dev data is the one that you have correct labels to, while test data is like real new data coming in.</p>
<p><strong>Main ideas of text classification</strong>:</p>
<ul>
<li>Representation as feature vectors</li>
<li>Scoring by linear functions</li>
<li>Learning by optimization</li>
</ul>
<p><strong>Probabilistic classifiers</strong>: two broad approaches to predict a class <span class="math inline">y</span></p>
<ol type="a">
<li>Joint / Generative models (e.g. Naïve Bayes)</li>
</ol>
<ul>
<li>Work with a joint probabilistic model of data <span class="math inline">P(X,y)</span>, weights are (often) local conditional probabilities.</li>
<li>Often assume functional form for <span class="math inline">P(X|y), P(y)</span>; represent p(y,X) as Naïve Bayes model, compute <span class="math inline">y*=argmax_y p(y,X)= argmax_y p(y)p(X|y)</span>.</li>
<li>Estimate <em>probabilities</em> from data.</li>
<li>Advantages: learning weights is easy and well understood.</li>
</ul>
<ol start="2" type="a">
<li>Conditional / Discriminative models (e.g. Logistic Regression)</li>
</ol>
<ul>
<li>Work with conditional probability <span class="math inline">p(y|X)</span>. We can then directly compute <span class="math inline">y* = argmax_y p(y|X)</span> (estimating <span class="math inline">p(y|X)</span> directly).</li>
<li>Require numerical optimization methods.</li>
<li>Estimate <em>parameters</em> from data.</li>
<li>Advantages: Don’t have to model <span class="math inline">p(X)</span>! Can develop feature rich models for <span class="math inline">p(y|X)</span>.</li>
<li>Popular in NLP.</li>
</ul>
<h3 id="generative-approach-naïve-bayes-models">Generative Approach: Naïve-Bayes Models</h3>
<p>The generative story: <strong>pick a topic, then generate a document.</strong></p>
<p><strong>Assumption: All words (features) are independent given the topic (label).</strong></p>
<p>Order invariant for tokens.</p>
<p>Issues: underflow; large number of topics (a lot of computing).</p>
<p>NB Learning: <strong>Maximum Likelihood Estimate</strong></p>
<p>In MLE, two parameters to estimate: 1) <span class="math inline">𝑞(𝑦) = \theta_y</span> for each topic <span class="math inline">𝑦</span>; 2) <span class="math inline">q(x|y) = \theta_{xy}</span> for each topic <span class="math inline">y</span> and word <span class="math inline">x</span>.</p>
<p><strong>Zero frequency problem</strong> in MLE: if there is a zero in the calculation the whole product becomes zero, no matter how many other values you got which maybe would find another solution.</p>
<p>Learning by <strong>count</strong>: <span class="math inline">\theta_y = C(y)/N</span>, <span class="math inline">\theta_{xy} = C(x,y)/C(y)</span>. The learning complexity is <strong>O(n)</strong>.</p>
<p><em>Word sense</em>: bag-of-words classification works ok for nouns, but verbal senses are less topical and more sensitive to structure (argument choice).</p>
<h3 id="discriminative-approach-linear-models">Discriminative Approach: Linear Models</h3>
<p>Features are indicator functions which count the occurrences of certain patterns in the input. Initially we will have different feature values for every pair of input <span class="math inline">X</span> and class <span class="math inline">y</span>.</p>
<p><strong>Block Feature Vectors</strong>: Input has features, which are multiplied by outputs to form the candidates.</p>
<p>Different candidates will often share features.</p>
<p>In linear models, each feature gets a weight in <span class="math inline">w</span>. We compute the scores and then according to the prediction rule, we choose the highest (positive) one.</p>
<details>
<p></p><summary>Example of Linear Model</summary><p></p>
<p><span class="math inline">\Phi(X, SPORTS) = [1 0 1 0 \space 0 0 0 0 \space 0 0 0 0]</span></p>
<p><span class="math inline">\Phi(X, POLITICS) = [0 0 0 0 \space 1 0 1 0 \space 0 0 0 0]</span></p>
<p><span class="math inline">\Phi(X, OTHER) = [0 0 0 0 \space 0 0 0 0 \space 1 0 1 0]</span></p>
<p><span class="math inline">W = [1 \space 1 \space -1 \space 2, 1 \space -1 \space 1 \space -2, -2 \space -1 \space -1 \space 1]</span></p>
Respectively <span class="math inline">SCORE= 0;2;-3</span>, thus <span class="math inline">prediction = POLITICS</span>
</details>
<p>(Multinomial) Naïve-Bayes is a linear model.</p>
<p><strong>Picking weights</strong>:</p>
<ul>
<li>Goal: choose "best" vector <span class="math inline">w</span> given training data.</li>
<li>The best we can ask for are weights that give best training set accuracy, but it's a hard optimization problem.</li>
</ul>
<h3 id="discriminative-approach-maximum-entropy-models-logistic-regression">Discriminative Approach: Maximum Entropy Models (=Logistic Regression)</h3>
<p>Use the scores as probabilities.</p>
<p>Learning: maximize the (log) conditional likelihood of training data <span class="math inline">\{(X^{(i)}, y^{(i)})\}^N_{i=1}</span></p>
<p>An equation is said to be a <strong>closed-form solution</strong> if it solves a given problem in terms of functions and mathematical operations from a given generally accepted set.</p>
<p><span class="math inline">L(w) = \Sigma^{N}_{i=1}logP(y^{(i)}|X^{(i)}; w), w^*=argmax_wL(w)</span></p>
<p>-- <span class="math inline">w^*</span> doesn't have a closed-form solution. So the MaxEnt objective is an <em>unconstrained optimization problem</em>.</p>
<ul>
<li>Basic idea: move uphill from current guess.</li>
<li>Gradient ascent / descent follows the gradient incrementally.</li>
<li>At <strong>local optimum</strong>, derivative vector is <strong>zero</strong>.</li>
<li>Will converge <strong>if step sizes are small enough</strong>, but <strong>not efficient</strong>.</li>
<li>All we need is to be able to evaluate the function and its derivative. Once we have a function <span class="math inline">f</span>, we can find a local optimum by iteratively following the gradient.</li>
<li>For <strong>convex</strong> (curved or rounded outward) functions, <strong>a local optimum will be global</strong>. Convexity guarantees a single, global maximum value because any higher points are greedily reachable.</li>
</ul>
<p>Basic gradient ascent isn’t very efficient, but there are simple enhancements which take into account previous gradients: conjugate gradient, L-BFGS. There are special-purpose optimization techniques for MaxEnt, like iterative scaling, but they aren’t better.</p>
<p><strong>The optimum parameters are the ones for which each feature’s predicted expectation equals its empirical expectation.</strong></p>
<p>In logistic regression, instead of worrying about zero count in MLE, we worry about <em>large feature weights</em>. Use regularization (smoothing) for log-linear models (add a L2 regularization term to the likelihood to push weights to zero).</p>
<p>But even after regularization, MaxEnt still doesn't have a closed-form solution. We will have to differentiate and use gradient ascent.</p>
<h3 id="perceptron-algorithm">Perceptron Algorithm</h3>
<p>Iteratively processes the training set, reacting to training errors. Can be thought of as trying to drive down training error.</p>
<ul>
<li>Online (or batch)</li>
<li>Error driven</li>
<li>Simple, additive updates</li>
</ul>
<h4 id="binary-class">Binary Class</h4>
<p><strong>Steps</strong> for the online (binary <span class="math inline">y = \plusmn 1</span>) perceptron algorithm:</p>
<ol type="1">
<li>Start with <strong>zero</strong> weights</li>
<li>Visit training instances <span class="math inline">(X^{(i)}, y^{(i)})</span> one by one, until all correct
<ul>
<li>Make a prediction: <span class="math inline">y^* = sign(w ·\Phi(X^{(i)}))</span> (sign means whether it &gt;(=) 0)</li>
<li>If correct (<span class="math inline">y^*==y^{(i)}</span>): no change, go to next example!</li>
<li>If wrong: adjust weights <span class="math inline">w = w - y^* \Phi(X^{(i)})</span></li>
</ul></li>
</ol>
<p>The perceptron finds a separating hyperplane.</p>
<p>If add bias, algorithm stays the same.</p>
<h4 id="multiclass">Multiclass</h4>
<p>For multiclass situation, we will have:</p>
<ul>
<li>A weight vector for each class</li>
<li>Score (activation) of a class <span class="math inline">y</span>: <span class="math inline">w_y·\Phi(x)</span></li>
<li>Compare all possible outputs, prediction highest score wins</li>
</ul>
<p><strong>Perceptron learning traits</strong>:</p>
<ul>
<li>No counting or computing probabilities on training set</li>
<li>Separability: some parameters get the training set perfectly correct</li>
<li>Convergence: if the training is separable (e.g. could divide into two classes with 100% accuracy for binary case), perceptron will eventually converge</li>
<li>Mistake Bound: the maximum number of mistakes (binary case) related to the margin or degree of separability</li>
</ul>
<p><strong>Perceptron problems</strong>:</p>
<ul>
<li>Noise: if the data isn’t separable, weights might thrash
<ul>
<li>Averaging weight vectors over time can help (averaged perceptron)</li>
</ul></li>
<li>Mediocre generalization: finds a “barely” separating solution</li>
<li>Overtraining: test / held-out accuracy usually rises, then falls (overtraining is a kind of overfitting)</li>
</ul>
<h3 id="a-note-for-features-tfidf">A Note for Features: TF/IDF</h3>
<ul>
<li>More frequent terms in a document are more important.</li>
<li>May want to normalize term frequency (TF) by dividing by the frequency of the most common term in the document.</li>
<li>Terms that appear in many different documents are less indicative (thus inverse document frequency)</li>
</ul>
<h3 id="neural-networks">Neural Networks</h3>
<h4 id="two-types-of-machine-learning">Two types of machine learning</h4>
<p><strong><em>Representation Learning</em></strong> attempts to automatically learn good features and representations.</p>
<p><strong><em>Deep Learning</em></strong> attempts to learn multiple levels of representation of increasing complexity/abstraction.</p>
<h4 id="parameters">Parameters</h4>
<ul>
<li>Weights: <span class="math inline">w</span> and <span class="math inline">b</span></li>
<li>Activation function (If dropped and single neuron, becomes perceptron)</li>
<li>Hidden layer numbers and neuron numbers</li>
</ul>
<h4 id="process">Process</h4>
<p>Neural net -&gt; several MaxEnt models</p>
<p>Hidden layer figures out what to do by <em>learning</em>.</p>
<p>Training:</p>
<ul>
<li>Without hidden layer: supervised, just like MaxEnt</li>
<li>With hidden layer: latent units -&gt; not convex; back propagate the gradient, about the same but no guarantee</li>
</ul>
<h4 id="probabilistic-output-from-neural-nets">Probabilistic Output from Neural Nets</h4>
<p>Normalize the output activations with <strong><em>softmax</em></strong>:</p>
<p><span class="math display">
y = softmax(o) \\
softmax(o_i) = \frac{exp(o_i)}{\Sigma^k_{j=1}exp(o_j)}
</span></p>
<p>* <span class="math inline">o</span> is the output layer.</p>
<p>Usually no non-linearity before softmax.</p>
<h4 id="word-embeddings">Word Embeddings</h4>
<p>Representing words with <strong>one-hot</strong> vectors.</p>
<details>
<p></p><summary>Dimensionality (unfold)</summary><p></p>
<ul>
<li>Size of vocabulary</li>
<li>20K for speech</li>
<li>500K for broad-converage domains</li>
<li>13M for Google corpora</li>
</ul>
</details>
<p>The word embeddings should represent each word with a <em>dense low-dimensional vector</em>.</p>
<p>Low-dimensional &lt;&lt; vocabulary size</p>
<p>If trained well, similar words will have similar vectors.</p>
<p>Word embeddings as features, e.g. sentiment classification.</p>
<p>Feature based models: bag of words</p>
<details>
<summary>Practical Tips (unfold)</summary> - Select network structure appropriate for the problem, e.g. window vs. recurrent vs. recursive - Gradient checks to identify bugs - Parameter initialization - If model isn't powerful enough, make it larger; else regularize to avoid overfitting - Know your non-linearity function and its gradient
</details>
<p><strong>Debugging</strong></p>
<ul>
<li><p>Verify value of initial loss when using softmax.</p></li>
<li><p>Perfectly fit a single mini-batch.</p></li>
<li><p>If learning fails completely, maybe gradients stuck.</p>
<ul>
<li>Check learning rate</li>
<li>Verify parameter initialization</li>
<li>Change non-linearity functions</li>
</ul></li>
</ul>
<p><strong>Avoid overfitting</strong></p>
<ul>
<li>Reduce model size a little</li>
<li>L1 and L2 regularization</li>
<li>Early stopping</li>
<li>Dropout</li>
</ul>
<p>Word embeddings vs. sparse vectors:</p>
<ul>
<li>Count vectors: sparse and large</li>
<li>Embedded vectors: small dense</li>
<li>More contested advantage other than dimensionality: better generalization</li>
</ul>
<h2 id="lexical-semantics">Lexical Semantics</h2>
<h3 id="word-sense-disambiguation-wsd">Word Sense Disambiguation (WSD)</h3>
<p><strong>Lemma (citation form)</strong>: Basic part of the word, same stem, rough semantics. One lemma can have many meanings.</p>
<p><strong>Wordform</strong>: The "inflected" word as it appears in text.</p>
<details>
<p></p><summary>Examples (click to unfold)</summary><p></p>
<p>Wordform: banks, sung</p>
<p>Lemma: bank, sing</p>
</details>
<p><strong>Sense (word sense)</strong>: A discrete representation of an aspect of a word’s meaning.</p>
<p><strong>Homonyms</strong>: Words that share a form but have unrelated, distinct meanings.</p>
<ul>
<li>Homographs: bank/bank</li>
<li>Homophones: write/right</li>
</ul>
<p>Homonyms in NLP: Information retrieval, machine translation, text-to-speech</p>
<p><strong>Synonyms</strong>: Different words that have same meanings in some or all contexts.</p>
<ul>
<li>Very few examples for perfect synonyms</li>
</ul>
<p><strong>Antonyms</strong>: Senses that are opposites with respect to one feature of meaning.</p>
<p><strong>Hyponymy and Hypernymy</strong>: One sense is a hyponym/subordinate of another if the first sense is more specific, denoting a subclass of the other; On the contrary, the other will be hypernym/superordinate.</p>
<details>
<summary>Examples (click to unfold)</summary> - Car is a hyponym of vehicle - Fruit is a hypernym of mango
</details>
<h4 id="wordnet">Wordnet</h4>
<p>A hierarchically organized lexical database.</p>
<ul>
<li>Each word in WordNet has at least one sense</li>
<li>Each sense has a gloss (textual description)</li>
<li>The synset (synonym set), the set of near-synonyms, is a set of senses with a shared gloss</li>
</ul>
<details>
<p></p><summary>Example (click to unfold)</summary><p></p>
<p>Chump as a noun with the gloss: "a person who is gullible and easy to take advantage of"</p>
<p>This sense of "chump" is shared with 9 words: chump1, fool2, gull1, mark9, patsy1, fall guy1, sucker1, soft touch1, mug2</p>
All these senses have the same gloss -&gt; they form a synset
</details>
<h4 id="supervised-wsd">Supervised WSD</h4>
<p>Given a lexicon (e.g., WordNet) and a word in a sentence, the goal is to classify the sense of the word.</p>
<p>Linear model:</p>
<p><span class="math display">
p(sense | word, context) \propto e^{\theta *\phi(sense, word, context)}\\
p(sense | word, context) \frac{e^{\theta *\phi(sense, word, context)}}{\Sigma_{s'}e^{\theta *\phi(s', word, context)}} \\
</span></p>
<h4 id="unsupervised-wsd">Unsupervised WSD</h4>
<p>Goal: induce the senses of each word and classify in context</p>
<ol type="1">
<li>For each word in context, compute some features</li>
<li>Cluster each instance using a clustering algorithm</li>
<li>Cluster labels are word senses</li>
</ol>
<h3 id="semantic-role-labeling-srl">Semantic role labeling (SRL)</h3>
<ul>
<li>Some word senses (a.k.a. predicates) represent events</li>
<li>Events have participants that have specific roles (as arguments)</li>
<li>Predicate-argument structure at the type level can be stored in a lexicon</li>
</ul>
<h4 id="propbank">PropBank</h4>
<p>A semantic role lexicon.</p>
<p>run.01 (operate) ---- Frame</p>
<ul>
<li>ARG0 (operator)</li>
<li>ARG1 (machine/operation)</li>
<li>ARG2 (employer)</li>
<li>ARG3 (co-worker)</li>
<li>ARG4 (instrument) ----Semantic roles</li>
</ul>
<p>*FrameNet, an alternative role lexicon</p>
<p>Task for semantic role labeling: given a sentence, disambiguate predicate frames and annotate semantic roles</p>
<h4 id="role-identification">Role Identification</h4>
<p>Classification models similar to WSD</p>
<p>Sentence spans -&gt; potential roles</p>
<p>Score can come from any classifier (linear, SVM, NN)</p>
<h4 id="word-similarity">Word Similarity</h4>
<p>Given two words, predict how similar they are.</p>
<p>The Distributional Hypothesis: You shall know a word by the company it keeps.</p>
<p>Given a vocabulary of <span class="math inline">n</span> words, represent a word <span class="math inline">w</span> as: <span class="math inline">w =(f_1, f_2, f_3, ..., f_n)</span></p>
<p><span class="math inline">f_i</span>: Binary (or count) features indicating the presence of the <span class="math inline">i^{th}</span> word in the vocabulary in the word’s context</p>
<p>Similarity can be measured using vector distance metrics. e.g. cosine similarity, gives values between -1 (completely different), 0 (orthogonal), and 1 (the same).</p>
<p><strong>Vector-space Models</strong></p>
<ul>
<li>Words represented by vectors</li>
<li>In contrast to the discrete class representation of word senses</li>
<li>Common methods (and packages): Word2Vec, GloVe</li>
</ul>
<h4 id="word2vec">Word2Vec</h4>
<ul>
<li>Method (and open-source package) for learning word vectors from raw text</li>
<li>Widely used across academia/industry</li>
<li>Goal: good word embeddings, aka. embeddings are vectors in a low dimensional space; similar words should be close to one another</li>
<li>Two models: skip-gram, CBOW</li>
</ul>
<h4 id="skip-gram-model">Skip-Gram Model</h4>
<p>Given: corpus <span class="math inline">D</span> of pairs <span class="math inline">(w, c)</span> where <span class="math inline">w</span> is a word and <span class="math inline">c</span> is context.</p>
<p>Context may be a single neighboring word (in window of size <span class="math inline">k</span>). Consider the parameterized probability <span class="math inline">p(c|w;\theta)</span>.</p>
<p>Goal: maximize the corpus probability:</p>
<p><span class="math inline">argmax_{\theta} \Pi_{(w,c)\in D}p(c|w;\theta)</span></p>
<p>The important thing is <strong>how we parametrize the probability distribution</strong>.</p>
<p>If <span class="math inline">d</span> is the dimensionality of the vectors, we have <span class="math inline">d \times |V| + d \times |C|</span> parameters.</p>
<p>The log of the objective sums over all context is <strong>not tractable in practice</strong>. It can be approximated with <strong><em>negative sampling</em></strong>.</p>
<p>Negative sampling for skip-gram:</p>
<ul>
<li>Efficient way of deriving word embeddings</li>
<li>Consider a word-context pair <span class="math inline">(w,c)</span>, the probability that it was not observed is <span class="math inline">1-p(D=1|w,c)</span>.</li>
<li>Parameterization: <span class="math inline">p(D=1|w,c)=\frac{1}{1+e^{-v_cv_w}}</span></li>
<li>New learning objective： <span class="math inline">argmax_\theta\Pi_{(w,c)\in D}p(D=1|w,c)\Pi_{(w,c)\in D'}p(D=0|w,c)</span></li>
<li>For a given <span class="math inline">k</span>, the size of <span class="math inline">D'</span> is <span class="math inline">k</span> times bigger than <span class="math inline">D</span></li>
<li>Each context c is a word</li>
<li>For each observed word-context pair, <span class="math inline">k</span> samples are generated based on unigram distribution</li>
</ul>
<p>Intuition for skip-gram model: <strong>words that share many contexts will be similar</strong>.</p>
<h2 id="language-model">Language Model</h2>
<h3 id="problem-overview">Problem Overview</h3>
<p><strong>Setup</strong>: assume a (finite) vocabulary of words <span class="math inline">V=\{ the, a, man, telescope, Beckham, two, Madrid,...\}</span>, We can construct an (infinite) set of strings</p>
<p><strong>Data</strong>: given a training set of example sentences</p>
<p><strong>Problem</strong>: estimate a probability distribution over sentences</p>
<p><em>Why would we ever want to do this?</em></p>
<p>Answer: (Automatic) Speech Recognition (ASR): audio in, text out</p>
<details>
<p></p><summary>Click to unfold and see some funny examples</summary> Wreck a nice beach? -- Recognize speech<p></p>
Eye eight uh Jerry? -- I ate a cherry
</details>
<h4 id="learning-language-models">Learning language models</h4>
<p><strong>Goal</strong>: Assign useful probabilities <span class="math inline">P(X)</span> to sentences <span class="math inline">X</span></p>
<p><em>Input</em>: many observations of training sentences <span class="math inline">X</span></p>
<p><em>Output</em>: system that can compute <span class="math inline">P(X)</span></p>
<p>Probabilities should broadly indicate plausibility of sentences, e.g. P("I saw a van") &gt;&gt; P("eyes awe of an")</p>
<p>This is not only about grammar, e.g. P("infected curly chair") =(almost) 0 (yes, I made this one up)</p>
<h3 id="models">Models</h3>
<p>Empirical distribution over training sentences...</p>
<ul>
<li>Doesn't generalize at all</li>
<li>Need to assign non-zero probability to previously unseen sentences</li>
</ul>
<p>Decompose Probability...</p>
<ul>
<li>Assume word choice depends on previous words only</li>
<li>Not really better because last word still represents complete event</li>
</ul>
<p>Markov assumption</p>
<ul>
<li>P(english | this is written in)</li>
<li>P(english | is written in)</li>
<li>P(english | written in)</li>
<li>P(english | in)</li>
<li>P(english)</li>
</ul>
<h4 id="the-noisy-channel-model">The Noisy Channel Model</h4>
<p>Goal: predict sentence given acoustics</p>
<p><span class="math display">
w^* = argmax_XP(X|a)
= argmax_XP(a|X)P(X)
</span></p>
<ul>
<li>Language model: Distributions over sequences of words (sentences) -- <span class="math inline">P(X)</span></li>
<li>Acoustic model: Distributions over acoustic waves given a sentence -- <span class="math inline">P(a|X)</span></li>
</ul>
<p>ASR Noisy Channel System:</p>
<p>Language Model: source <span class="math inline">P(X)</span> -&gt; <span class="math inline">X</span> -&gt; Acoustic Model: channel <span class="math inline">P(a|X)</span> -&gt; <span class="math inline">a</span></p>
<p>observed <span class="math inline">a</span> -&gt; decoder (LM&amp;AM) -&gt; best <span class="math inline">X</span></p>
<p>Other systems with similar structure: <em>MT Noisy Channel System (translation)</em>, <em>Caption Generation Noisy Channel System</em></p>
<h4 id="n-gram-models">N-gram Models</h4>
<p><strong>Unigram</strong></p>
<ul>
<li>Generative process: pick a word, pick a word, repeat...until you pick STOP</li>
<li>Big <strong>problem</strong>: P(the the the the) &gt;&gt; P(a real sentence)</li>
</ul>
<p><strong>Bigram (and more)</strong></p>
<ul>
<li>Generative process: pick start, pick a word conditioned on previous one, repeat until to pick STOP</li>
<li>k-gram: conditioning on k-1 previous words</li>
<li>Learning: estimate the distribution</li>
</ul>
<p><strong><em>Well-defined Distributions - proof for unigrams</em></strong></p>
<p>For all string <span class="math inline">X</span> (of any length), <span class="math inline">P(X) \geq 0</span>.</p>
<p>Claim: the sum over string of all length is 1: <span class="math inline">\Sigma_XP(X)=1</span></p>
<p>RNN language models are surprisingly not necessarily well defined distributions.</p>
<p><strong><em>Parameters for n-gram models</em></strong></p>
<p>Maximum likelihood estimate - relative frequency</p>
<p><span class="math display">
q_{ML}(w) = \frac{c(w)}{c()}, q_{ML}(w|v) = \frac{c(v,w)}{c(v)}, q_{ML}(w|u,v) = \frac{c(u,v,w)}{c(u,v)}, ...
</span></p>
<p>where <span class="math inline">c</span> is the empirical counts on a training set.</p>
<p>General approach:</p>
<ul>
<li>Take a training set <span class="math inline">D</span></li>
</ul>
<h3 id="continuous-representations">Continuous representations</h3>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone, qq, weibo, douban"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>Reprint policy</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
                    <img alt="Creative Commons License"
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"/></a>
            </div>
            <br/><span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                       property="dct:title" rel="dct:type">《Natural Language Processing》</span> by
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2020/04/16/data-science-shu-ju-ke-xue/natural-language-processing/"
               property="cc:attributionName" rel="cc:attributionURL">FadingWinds
            </a> is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">
                Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License
            </a> 
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    
        <div class="disqus-card card" data-aos="fade-up">
    <div id="disqus_thread" class="card-content">
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
    </div>
</div>

<script type="text/javascript">
    disqus_config = function () {
        this.page.url = 'https://FadingWinds.github.io/2020/04/16/data-science-shu-ju-ke-xue/natural-language-processing/';
        this.page.identifier = '/2020/04/16/data-science-shu-ju-ke-xue/natural-language-processing/';
        this.page.title = 'Natural Language Processing';
    };
    let disqus_shortname = 'dovecode';

    (function () { // DON'T EDIT BELOW THIS LINE
        let d = document, s = d.createElement('script');
        // 如：s.src = 'https://blinkfox.disqus.com/embed.js';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/08/17/blockchain-qu-kuai-lian/the-chain-of-blocks/">
                    <div class="card-image">
                        
                        <img src="https://images.cointelegraph.com/images/1480_aHR0cHM6Ly9zMy5jb2ludGVsZWdyYXBoLmNvbS9zdG9yYWdlL3VwbG9hZHMvdmlldy8wNmY3YTk1MDNiOWZiODg1ZGNlMjQ5ZDU3ZjA4OWIwMy5qcGc=.jpg" class="responsive-img" alt="The Chain of Blocks">
                        
                        <span class="card-title">The Chain of Blocks</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            The story among Alice, Bob and Charlie(Carol). Sometimes more.
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2020-08-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Blockchain-·-区块链/" class="post-category" target="_blank">
                                    Blockchain · 区块链
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/notes/" target="_blank">
                        <span class="chip bg-color">notes</span>
                    </a>
                    
                    <a href="/tags/CT-courses/" target="_blank">
                        <span class="chip bg-color">CT courses</span>
                    </a>
                    
                    <a href="/tags/updating/" target="_blank">
                        <span class="chip bg-color">updating</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/02/27/bugs-in-the-wild/anaconda-vs-code-duo-ban-ben-python-unable-to-import-wen-ti/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/12.jpg" class="responsive-img" alt="Anaconda+VS Code+多版本Python的"Unable to Import"问题解决方案">
                        
                        <span class="card-title">Anaconda+VS Code+多版本Python的"Unable to Import"问题解决方案</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            如题。自行研究（Google）后得到的解决方案。
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2020-02-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Bugs-In-The-Wild/" class="post-category" target="_blank">
                                    Bugs In The Wild
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Python/" target="_blank">
                        <span class="chip bg-color">Python</span>
                    </a>
                    
                    <a href="/tags/VS-Code/" target="_blank">
                        <span class="chip bg-color">VS Code</span>
                    </a>
                    
                    <a href="/tags/Anaconda/" target="_blank">
                        <span class="chip bg-color">Anaconda</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('50')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: DoveCode<br />'
            + 'Author: FadingWinds<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + 'Please do follow the reprint regulations of DoveCode.';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            This website is based on &nbsp;&copy;<a href="https://hexo.io/" target="_blank">Hexo</a>&nbsp;and 
            uses the theme 
            <a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>&nbsp;with some personalized changes.
            <br>
            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;Total words:&nbsp;<span
                class="white-color">12.8k</span>&nbsp;
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="fa fa-eye"></i>&nbsp;Total visits:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fa fa-users"></i>&nbsp;Total visitors:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;
            </span>
            <br>
            <span id="timeDate">Loading days...</span><span id="times">Loading times...</span>
            <script>
                var now = new Date();

                function createtime() {
                    var grt = new Date("09/17/2019 00:00:00");
                    now.setTime(now.getTime() + 250);
                    days = (now - grt) / 1000 / 60 / 60 / 24;
                    dnum = Math.floor(days);
                    hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum);
                    hnum = Math.floor(hours);
                    if (String(hnum).length == 1) {
                        hnum = "0" + hnum;
                    }
                    minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum);
                    mnum = Math.floor(minutes);
                    if (String(mnum).length == 1) {
                        mnum = "0" + mnum;
                    }
                    seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
                    snum = Math.round(seconds);
                    if (String(snum).length == 1) {
                        snum = "0" + snum;
                    }
                    document.getElementById("timeDate").innerHTML = "This website has been running for " + dnum + " days, ";
                    document.getElementById("times").innerHTML = hnum + " hours, " + mnum + " minutes, and " + snum + " seconds.";
                }
                setInterval("createtime()", 250);
            </script>
            
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/fadingwinds" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:wq.tang26@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=AceWinds#9853" class="tooltipped" data-tooltip="Add me on Discord: AceWinds#9853" data-position="top" data-delay="50">
        <i class="fa fa-discord"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>